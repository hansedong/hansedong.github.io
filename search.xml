<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[K8S 如何使用具有配额限制的 ceph 共享存储]]></title>
    <url>%2F2018%2F12%2F18%2F11%2F</url>
    <content type="text"><![CDATA[前言 首先，你得有个 Ceph 集群，而这个集群具体怎么搭建，这里不展开说，有很多文章讲怎么搭建，至于其中方便点的部署方式，可以尝试使用 ceph-deploy。这里主要说的是，K8S 怎么使用 Ceph 存储，但经过一番探索，发现，K8S 支持 Ceph 共享存储这个事儿虽然不是很复杂，但问题是，想让 K8S 支持带配额能力的 Ceph 还是有那么一点麻烦的。 K8S 在使用共享存储时，其a支持很多 backend，比如 GlusterFS、Ceph，这2个比较典型。之所以选择用 Ceph ，主要有几个方面： ①：Ceph 目前更主流，更多主流大厂在使用，市场更好一些。②：整体上看，性能上可能 Ceph 更好一些（这个可能仁者见仁智者见智，性能调优之后也不好说，而且他俩的适用场景本身就有一些区别，比如 GlusterFS 更适合存储大文件，但也有 GluterFS 对小文件存储的优化方式） 其实具体用哪个存储，还是要考后边的团队，对什么存储比较熟，更有掌控力。 另外，有些文章可能提到 CephFS（Ceph 文件存储）还不能用到生产环境，而且文章还是18年中旬，但很多英文文章都在17年提及，CephFS 早就 Production Ready 了。在生产环境使用应该问题不大，不过具体能不能在生产环境使用，可能还需要摸索一番。 K8S 如何使用共享存储 那么，K8S 在对分布式存储的支持上，主要有2种方式来做： ①：静态方式：PVC + PV②：动态方式：PVC + StorageClass 静态方式：PV + PVCPV 其实就是持久化Volume，它是资源的定义，比如说，存储空间多大、存储类型、存储后端等，它侧重于资源本身。 PVC 是一个声明，声明自己需要多少资源，声明需要多少资源，以及访问模式等，它侧重于“需求”。 在 K8S 中，我们可以为 POD 设置 ResourceRequest，比如 CPU、内存等，然后 K8S 为其匹配合适的 Node 节点，调度过去。同理，K8S 的 PVC 也是 ResourceRequest，只是 K8S 为其匹配的目标是 PV 而已。有一点不同，K8S 为 PVC 匹配得到 PV 后，会做一个绑定，这就意味着，一个 PVC，绑定一个 PV 后，PV 的状态就会变为 Bound（束缚），两者绑定后，不会再与其他资源匹配。 使用 PV 和 PVC 模式，需要注意的是 PV。PVC很简单，但 PV 有点麻烦，其中涉及很多内容，以一个 PV 示例来说： 1234567891011121314151617181920212223242526272829303132apiVersion: v1kind: PersistentVolumemetadata: finalizers: - kubernetes.io/pv-protection name: pvc-21f06e44-fdfe-11e8-be0a-801844e392e8spec: accessModes: - ReadWriteMany capacity: storage: 500Mi cephfs: monitors: - 172.18.12.235:6789 - 172.18.12.236:6789 - 172.18.12.237:6789 path: /volumes/kubernetes/kubernetes/kubernetes-dynamic-pvc-21f5369d-fdfe-11e8-91a2-82b342c34790 secretRef: name: ceph-kubernetes-dynamic-user-21f536d3-fdfe-11e8-91a2-82b342c34790-secret namespace: cephfs user: kubernetes-dynamic-user-21f536d3-fdfe-11e8-91a2-82b342c34790 claimRef: apiVersion: v1 kind: PersistentVolumeClaim name: pvc-1 namespace: cephfs resourceVersion: "19338037" uid: 21f06e44-fdfe-11e8-be0a-801844e392e8 mountOptions: - --client-quota persistentVolumeReclaimPolicy: Delete storageClassName: cephfs PVC 示例如下： 1234567891011121314151617181920apiVersion: v1kind: PersistentVolumeClaimmetadata: finalizers: - kubernetes.io/pvc-protection name: pvc-1 namespace: cephfsspec: accessModes: - ReadWriteMany resources: requests: storage: 500Mi volumeName: pvc-21f06e44-fdfe-11e8-be0a-801844e392e8status: accessModes: - ReadWriteMany capacity: storage: 500Mi phase: Bound 其中的 PV 里的 path 内容，需要我们提前去分布式存储系统上创建好。包括配额，也需要先做好才行。 这就有点麻烦了，我们每次创建一个 PV 之前，需要先在分布式存储系统，创建好path。但频繁这么去做，成本是有点大。所以，K8S 也有一个动态方式。 注意：PVC 有命名空间、PV 没有命名空间。 动态方式：PVC + StorageClass动态的方式，需要借助 PVC + StorageClass ，PVC 不用说了，主要说一下 StorageClass。先来看一个 StorageClass 示例： 1234567891011121314apiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: cephfsmountOptions:- --client-quotaparameters: adminId: admin adminSecretName: ceph-secret-admin adminSecretNamespace: cephfs claimRoot: /volumes/kubernetes monitors: 172.18.12.235:6789,172.18.12.236:6789,172.18.12.237:6789provisioner: ceph.com/cephfsreclaimPolicy: Delete StorageClass 定义了后端存储系统的：类型、 servers、根目录、回收策略等。 我们在创建了 PVC（持久化Volume资源声明）后，K8S 会根据 PVC 和 和 StorageClass，动态创建 PV，然后动态自行去分布式存储系统进行创建目录操作，然后进行挂载，最终映射到 POD 内。用动态方式的好处是，我们只关心资源需求 PVC 就可以了，非常方便。 但是：目前 K8S所有版本（截止到 v1.12.3），对动态方式的支持，还不全面。仅支持一些分布式存储，比如： Volume Plugin Internal Provisioner Config Example CephFS - - Glusterfs ✔️ Glusterfs 注意：Ceph 和 CephFS 不一样，Ceph 是整个服务，而 CephFS 仅仅是 Ceph 的文件存储而已。Ceph 除了文件存储 CephFS 外，还支持 对象存储、块存储。 从上面来看，K8S 根本不支持 CephFS 的 动态管理文件存储支持，所以，要么，我们需要寻找一种使其支持的方式，要么，转投 GlusterFS。我们要做的，就是寻找方式，动态管理 CephFS 的文件存储。 让K8S支持动态管理CephFS（并支持配额） 这里要提一下 K8S 能够支持动态存储管理的简单原理了。 要做到动态存储管理，就意味着2点： ①：用户只需要创建 PVC 资源。②：K8S 需要监听 PVC 资源的变化，动态创建 PV。③：用户需要在 POD 中使用 PVC 资源。④：K8S 需要能够在宿主机创建 Volume。 安装 cephfs-provisioner其实，在 Kubernetes incubator （K8S 孵化器组）中，专门有一个外部存储支持的项目：https://github.com/kubernetes-incubator/external-storage/ 。这个项目中，有关于 Ceph 动态存储管理的方式，换句话说，这不是官方原装的，所以，我们需要安装它。 在 K8S 中，安装这个项目比较简单，这个项目中，也给出了 deploy 示例。总的来说，需要单独创建 ServiceAccount、Role、ClusterRole、RoleBinding、ClusterRoleBinding 等资源。 其中 ClusterRole 主要用来获取 K8S 监听和操作 PVC、PV 资源的权限、Role 是获取 Secret 资源的权限，因为 K8S 要操作 Ceph 集群，就需要认证信息，而认证信息就是放到 Secret 资源中的。下面是具体的创建内容： ①：首先是 Secret 资源，这个 CephFS 集群的访问权限（文件名如：ceph-secret-admin.yaml ），这个数据，需要你先去 CephFS 集群，通过下面方式获取： 1ceph auth get-key client.admin | base64 然后，将上面的结果，放到下面的 key 的内容中： 12345678apiVersion: v1kind: Secretmetadata: name: ceph-secret-admin namespace: cephfstype: "kubernetes.io/rbd" data: key: QVFDbmhBNWNJSmUvSUxxs32323WC9LZUJJR3EzdnpkOTkrVmc9PQ== ②：创建 ClusterRole 资源，clusterrolebinding.yaml 123456789101112kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: cephfs-provisionersubjects: - kind: ServiceAccount name: cephfs-provisioner namespace: cephfsroleRef: kind: ClusterRole name: cephfs-provisioner apiGroup: rbac.authorization.k8s.io ③：创建 ClusterRolebinding 资源，clusterrolebinding.yaml 123456789101112kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: cephfs-provisionersubjects: - kind: ServiceAccount name: cephfs-provisioner namespace: cephfsroleRef: kind: ClusterRole name: cephfs-provisioner apiGroup: rbac.authorization.k8s.io ④：创建 Role 资源，role.yaml 123456789101112apiVersion: rbac.authorization.k8s.io/v1kind: Rolemetadata: name: cephfs-provisioner namespace: cephfsrules: - apiGroups: [""] resources: ["secrets"] verbs: ["create", "get", "delete"] - apiGroups: [""] resources: ["endpoints"] verbs: ["get", "list", "watch", "create", "update", "patch"] ⑤：创建 Rolebinding 资源，rolebinding.yaml 123456789101112apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata: name: cephfs-provisioner namespace: cephfsroleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: cephfs-provisionersubjects:- kind: ServiceAccount name: cephfs-provisioner ⑥：创建 ServiceAccount 资源，serviceaccount.yaml 123456789101112131415161718apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata: name: cephfs-provisioner namespace: cephfsroleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: cephfs-provisionersubjects:- kind: ServiceAccount name: cephfs-provisioner[root@node008037 dynimcpv]# cat serviceaccount.yamlapiVersion: v1kind: ServiceAccountmetadata: name: cephfs-provisioner namespace: cephfs ⑦：创建 StorageClass 资源，storageClass.yaml 1234567891011121314kind: StorageClassapiVersion: storage.k8s.io/v1metadata: name: cephfs namespace: cephfsprovisioner: ceph.com/cephfsmountOptions:- "--client-quota"parameters: monitors: 172.18.12.235:6789,172.18.12.236:6789,172.18.12.237:6789 adminId: admin adminSecretName: ceph-secret-admin adminSecretNamespace: cephfs claimRoot: /volumes/kubernetes ⑧：创建 cephfs-provisioner 的 Deployment 资源，cephfs-provisioner-deployment.yaml 1234567891011121314151617181920212223242526apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: cephfs-provisioner namespace: cephfsspec: replicas: 1 strategy: type: Recreate template: metadata: labels: app: cephfs-provisioner spec: containers: - name: cephfs-provisioner image: "quay.io/external_storage/cephfs-provisioner:latest" env: - name: PROVISIONER_NAME value: ceph.com/cephfs command: - "/usr/local/bin/cephfs-provisioner" args: - "-id=cephfs-provisioner-1" - "-enable-quota=true" serviceAccount: cephfs-provisioner 好了，执行下面的命令，进行资源创建： 12345// 创建 Namespacekubectl create ns cephfs// 依次执行下面命令，创建如上资源kubectl apply -f 文件名 如果如上操作后，cephfs-provisioner 就已经安装好了。上面的文件比较多，其中有几个需要注意的地方： cephfs-provisioner Deployment 的 args 参数中，有一个 -enable-quota=true ，这个不写的话，是无法做到磁盘配额的。 创建完成后，在 cephfs 命令空间下，会有 cephfs-provisioner 的实例： 123[root@nodex test]# kubectl get pods -n cephfsNAME READY STATUS RESTARTS AGEcephfs-provisioner-6f77cd58b4-75hll 1/1 Running 0 16h 测试CephFS动态管理上面准备工作 cephfs-provisioner 已经安装好，下面就需要测试了，我们测试，其实仅需要创建2个资源，一个是 PVC、一个是 Deployment 的 POD 资源。 PVC 资源：pvc.yaml 12345678910111213kind: PersistentVolumeClaimapiVersion: v1metadata: name: pvc-1 namespace: cephfs annotations: volume.beta.kubernetes.io/storage-class: "cephfs"spec: accessModes: - ReadWriteMany resources: requests: storage: 500Mi Deployment 资源：deploy.yaml 1234567891011121314151617181920212223apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nginx-use-rbd namespace: cephfsspec: replicas: 1 template: metadata: labels: app: nginx spec: containers: - image: nginx:1.13 imagePullPolicy: IfNotPresent name: nginx-use-rbd volumeMounts: - mountPath: /test-pvc name: cephfs-pvc volumes: - name: cephfs-pvc persistentVolumeClaim: claimName: pvc-1 然后，分别用 kubectl apply -f 文件名 创建即可。 问题处理 创建后，我们需要观察在 cephfs 命名空间下，有没有自动创建出来 pv。 1kubectl get pv -n cephfs -w 如果自动创建，且后期自动完成了匹配，且 nginx POD 创建后变为 Running ，则表示一切正常。但往往事实并不如人意。我们可能遇到下面几个问题： ①：PV 没有自动创建出来。②：我们使用了共享存储的 nginx 示例 POD 一直是 ContainerCreating 状态。 PV 无法自动创建问题如果是 PV 没自动创建出来 ，说明 cephfs-provisioner 有问题，它没能创建好相关的资源，我们主要看它的日志信息 1kubectl logs -n cephfs cephfs-provisioner-6f77cd58b4-75hll 通常来说，PV 没创建出来最大的可能就是出在 cephfs-provisioner-6f77cd58b4-75hll 这个 POD 上，其实这个 POD 做的工作主要有几个： ①：连接 CephFS 集群②：创建 Volume③：设置配额属性，setxattr④：创建 PV 而这个 cephfs-provisioner 本身操作 CephFS 集群的方式，其实是通过 Golang，调用的 Python 脚本来做的这个事情。如果观察其 log ，出错在 setxattr 部分，则最大的可能性，就是版本问题。 cephfs-provisioner 处理方式如下： 官方的镜像，在我们之前的 cephfs-provisioner-deployment.yaml 中，其实它使用的 Ceph 的版本，默认是 mimic ，可以从这里看：https://github.com/kubernetes-incubator/external-storage/blob/master/ceph/cephfs/Dockerfile ，而我们的 Ceph 版本，是 hammer，而官方并没有提供 hammer 版本的 cephfs-provisioner，所以，我们只能自己来构建。构建步骤如下： 克隆 https://github.com/kubernetes-incubator/external-storage 项目。 进入 ceph/cephfs 目录下 修改 Dockerfile ，将 CEPH_VERSION 里的 mimic ，改为 hammer 。 修改 Makefile，将 REGISTRY 里的地址，改为私有镜像仓库地址，目的是推送镜像使用。 执行 make &amp;&amp; make push 。 完成后，更改 deploy.yaml 里 image 为新镜像地址，然后执行 kubectl apply -f deploy.yaml 完成后，观测 cephfs-provisioner POD 正常启动，同时看到 PV 自动被创建出来了。 POD持续ContainerCreating状态处理这种问题，很有可能是 POD 使用了 PVC 形式的 Volume，而这个 Volume 却迟迟无法创建出来，所以，这时候，可以通过 kubectl describe 来看具体原因 1kubectl describe pod -n cephfs nginx-use-rbd-dcfb58dc6-8f7gj 123[root@node008037 glusterfs]# kubectl get pods -n test -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODEnginx-rbd-tttttt-77bc5d46b7-5w294 0/1 ContainerCreating 0 9m &lt;none&gt; node012044 &lt;none&gt; 如果通过结果能看到相关 Volume 信息，且在不断输出重试内容，则大概能说明是 Volume 无法创建，进而无法挂载到容器内部导致的 1234567Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 9m default-scheduler Successfully assigned test/nginx-rbd-tttttt-77bc5d46b7-5w294 to node012044 Normal SuccessfulAttachVolume 9m attachdetach-controller AttachVolume.Attach succeeded for volume "pvc-c7c9dcce-0298-11e9-be0a-801844e392e8" Warning FailedMount 1m (x11 over 7m) kubelet, node012044 MountVolume.WaitForAttach failed for volume "pvc-c7c9dcce-0298-11e9-be0a-801844e392e8" : fail to check rbd image status with: (executable file not found in $PATH), rbd output: () Warning FailedMount 34s (x4 over 7m) kubelet, node012044 Unable to mount volumes for pod "nginx-rbd-tttttt-77bc5d46b7-5w294_test(4c970495-0299-11e9-be0a-801844e392e8)": timeout expired waiting for volumes to attach or mount for pod "test"/"nginx-rbd-tttttt-77bc5d46b7-5w294". list of unmounted volumes=[cephfs-pvc]. list of unattached volumes=[cephfs-pvc default-token-89ltk] 这种情况其实是宿主机缺少 ceph 的 client 工具，可以通过下面的方式解决： 123// centos 系统yum -y install ceph-commonyum -y install ceph-fuse 安装结束后，k8s 会自动尝试重新挂载。 参考 https://kubernetes.io/docs/concepts/storage/persistent-volumes/ https://blog.csdn.net/xudawenfighting/article/details/80125389 https://www.cnblogs.com/hukey/p/8323853.html https://www.cnblogs.com/yangxiaoyi/p/7795274.html http://dockone.io/article/558 https://www.cnblogs.com/yswenli/p/7234579.html https://www.jianshu.com/p/a4e1ba361cc9 https://www.cnblogs.com/ltxdzh/p/9173706.html https://baijiahao.baidu.com/s?id=1612194635156434300&amp;wfr=spider&amp;for=pc http://blog.chinaunix.net/uid-22166872-id-4959819.html https://www.gluster.org/glusterfs-vs-ceph/ http://www.sysnote.org/]]></content>
      <categories>
        <category>cephr</category>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CoreDNS系列2：KubeDNS 架构组成及实现原理]]></title>
    <url>%2F2018%2F11%2F22%2F10%2F</url>
    <content type="text"><![CDATA[KubeDNS简述KubeDNS，是K8S官方推荐的DNS解析组件之一，从 K8S 1.11 开始，K8S 已经使用 CoreDNS，替换 KubeDNS 来充当其 DNS 解析的重任。 KubeDNS的前身，是 skyDNS，这个组件，会把 DNS 的解析记录等数据，存储到 K8S 所使用的 etcd 集群中，我们这里不讨论这个，仅讨论现今的 KubeDNS，KubeDNS，并不把 DNS 的解析规则存储到 etcd，而是放到进程的内存中，当 KubeDNS 的服务 POD 重启后，会重建一遍 DNS 规则到内存。 在前篇，我们已经了解了 K8S 中，DNS 解析的原理，本篇，我们侧重 KubeDNS 本身。包含下面几个点： KubeDNS 服务，包含了哪些组件，职责是什么？ KubeDNS 是如何区分 K8S 内部域名还是外部域名的？ KubeDNS 如何配置上游DNS服务器？ KubeDNS 如何配置自定义域名解析？ KubeDNS 解析 K8S 内部域名的的实现原理是什么？ KubeDNS 如何做弹性扩缩容？ KubeDNS组件构成在 K8S 中，KubeDNS 的实例是 POD，配置一个 KubeDNS 的 Service，对 KubeDNS 的 POD 进行匹配。在 K8S 的 其他 POD 中，使用这个 Service 的 IP 地址，作为 /etc/resolv.conf 里 nameserver 的地址，从而达到 POD 里使用 KubeDNS 的目的。这是 K8S 的默认行为，我们不需要手动干预。 其实严格来说，是 Service 匹配 Endpoint，因为 POD 创建之后可能会有IP，但此IP可能是一个 POD 非 完全Ready 状态下的 IP，理论上，这种 IP 是无无法提供服务的，所以，说 Service 匹配 Endpoint 更合适一些。 然而，KubeDNS POD 是由 Deployment 控制启动的，POD 中，并非只有一个容器。KubeDNS 的配置，是使用的 ConfigMap，总的来看，有2个主要资源： 一个标准 KubeDNS 的 Deployment 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189apiVersion: extensions/v1beta1kind: Deploymentmetadata: labels: addonmanager.kubernetes.io/mode: Reconcile k8s-app: kube-dns kubernetes.io/cluster-service: &quot;true&quot; name: kube-dns namespace: kube-systemspec: progressDeadlineSeconds: 600 replicas: 2 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: kube-dns strategy: rollingUpdate: maxSurge: 10% maxUnavailable: 0 type: RollingUpdate template: metadata: annotations: scheduler.alpha.kubernetes.io/critical-pod: &quot;&quot; creationTimestamp: null labels: k8s-app: kube-dns spec: affinity: nodeAffinity: preferredDuringSchedulingIgnoredDuringExecution: - preference: matchExpressions: - key: node-role.kubernetes.io/master operator: In values: - &quot;true&quot; weight: 100 podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchLabels: k8s-app: kube-dns topologyKey: kubernetes.io/hostname containers: - args: - --domain=cluster.local. - --dns-port=10053 - --config-dir=/kube-dns-config - --v=2 env: - name: PROMETHEUS_PORT value: &quot;10055&quot; image: gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.10 imagePullPolicy: IfNotPresent livenessProbe: failureThreshold: 5 httpGet: path: /healthcheck/kubedns port: 10054 scheme: HTTP initialDelaySeconds: 60 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 5 name: kubedns ports: - containerPort: 10053 name: dns-local protocol: UDP - containerPort: 10053 name: dns-tcp-local protocol: TCP - containerPort: 10055 name: metrics protocol: TCP readinessProbe: failureThreshold: 3 httpGet: path: /readiness port: 8081 scheme: HTTP initialDelaySeconds: 3 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 5 resources: limits: memory: 170Mi requests: cpu: 100m memory: 70Mi terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /kube-dns-config name: kube-dns-config - args: - -v=2 - -logtostderr - -configDir=/etc/k8s/dns/dnsmasq-nanny - -restartDnsmasq=true - -- - -k - --cache-size=1000 - --dns-loop-detect - --log-facility=- - --server=/cluster.local/127.0.0.1#10053 - --server=/in-addr.arpa/127.0.0.1#10053 - --server=/ip6.arpa/127.0.0.1#10053 image: gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.10 imagePullPolicy: IfNotPresent livenessProbe: failureThreshold: 5 httpGet: path: /healthcheck/dnsmasq port: 10054 scheme: HTTP initialDelaySeconds: 60 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 5 name: dnsmasq ports: - containerPort: 53 name: dns protocol: UDP - containerPort: 53 name: dns-tcp protocol: TCP resources: requests: cpu: 150m memory: 20Mi terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /etc/k8s/dns/dnsmasq-nanny name: kube-dns-config - args: - --v=2 - --logtostderr - --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,A - --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,A image: gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.10 imagePullPolicy: IfNotPresent livenessProbe: failureThreshold: 5 httpGet: path: /metrics port: 10054 scheme: HTTP initialDelaySeconds: 60 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 5 name: sidecar ports: - containerPort: 10054 name: metrics protocol: TCP resources: requests: cpu: 10m memory: 20Mi terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: Default nodeSelector: beta.kubernetes.io/os: linux restartPolicy: Always schedulerName: default-scheduler securityContext: &#123;&#125; serviceAccount: kube-dns serviceAccountName: kube-dns terminationGracePeriodSeconds: 30 tolerations: - key: CriticalAddonsOnly operator: Exists - effect: NoSchedule key: node-role.kubernetes.io/master operator: Equal volumes: - configMap: defaultMode: 420 name: kube-dns optional: true name: kube-dns-config 从这个 Deployment 可以看到2个重点： KubeDNS 使用 名称为 kube-dns 的 ConfigMap 作为其配置（ConfigMap没有，POD也可以运行）。 KubeDNS 使用了3个容器，组合提供DNS服务。 KubeDNS组件构成及实现KubeDNS，使用3个容器组合服务，分别是：dnsmasq、kube-dns、sidecar，这3者，职责不同，整个DNS架构组成如下： POD的DNS请求打到 dnsmasq 容器的53端口，dnsmasq 决定此请求时自己处理，还是转到 kubedns 容器处理。各组件具体职责： dnsmasq 容器dnsmasq 容器：负责整个 KubeDNS 的请求入口，53端口，就是它开放的，因此，在 K8S 中，内部域名和外部域名请求处理的区分，也是 dnsmasq 来做的。它充当 DNS 的请求入口，有几个作用： 充当 DNS 请求入口。 区别 K8S 内部和外部域名走不通的策略。 DNS 缓存，进行过DNS请求后的域名会进行缓存，提高DNS请求效率。 在此容器的启动参数包含下面部分： 123- --server=/cluster.local/127.0.0.1#10053- --server=/in-addr.arpa/127.0.0.1#10053- --server=/ip6.arpa/127.0.0.1#10053 这个参数说明，cluster.local 结尾的域名（这种是 K8S 的内部域名，不理解为什么这种是 K8S 内部域名的，可以翻看之前的文章），dnsmasq 进程会把此 DNS 请求，转发到 127.0.0.1:10053 端口上。而 10053 端口，是 kube-dns容器进程，正是监听的 10053 端口。所以说， dnsmasq 通过 - –server=/cluster.local/127.0.0.1#10053 这个配置，来决策 K8S 内部的域名的 DNS 请求，往 kube-dns 容器转发。 dnsmasq 先解析本地 /etc/hosts 文件再解析 /etc/dnsmasq.d/*.conf 文件然后解析 /etc/dnsmasq.conf最后解析自定义上游DNS的部分，也就是 /etc/dnsmasq.conf 中 resolv-file 的字段部分，一般我们将 resolv-file 字段配置为 /etc/resolv.conf。 dnsmasq为KubeDNS提供缓存加速能力KubeDNS 缓存，其实，利用的便是 dnsmasq 具备的缓存能力。另外，缓存，在 KubeDNS 中非常重要，能最大程度的发挥 DNS 响应效率，我们通过一个实际例子测试一下： 第一次请求一个不存在的域名时，第一次 DNS 请求： 123456789101112131415161718[root@test8-5646b97977-p6z8p /]# dig jjjjjjjj1212121212.com; &lt;&lt;&gt;&gt; DiG 9.9.4-RedHat-9.9.4-61.el7_5.1 &lt;&lt;&gt;&gt; jjjjjjjj1212121212.com;; global options: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NXDOMAIN, id: 61155;; flags: qr rd ra; QUERY: 1, ANSWER: 0, AUTHORITY: 1, ADDITIONAL: 0;; QUESTION SECTION:;jjjjjjjj1212121212.com. IN A;; AUTHORITY SECTION:com. 900 IN SOA a.gtld-servers.net. nstld.verisign-grs.com. 1542785400 1800 900 604800 86400;; Query time: 864 msec;; SERVER: 10.233.0.3#53(10.233.0.3);; WHEN: Wed Nov 21 07:30:18 UTC 2018;; MSG SIZE rcvd: 113 请求耗时 864ms，耗时相当长。再次执行 DNS 请求： 1234567891011121314151617[root@test8-5646b97977-p6z8p /]# dig jjjjjjjj1212121212.com; &lt;&lt;&gt;&gt; DiG 9.9.4-RedHat-9.9.4-61.el7_5.1 &lt;&lt;&gt;&gt; jjjjjjjj1212121212.com;; global options: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NXDOMAIN, id: 60767;; flags: qr rd ra; QUERY: 1, ANSWER: 0, AUTHORITY: 0, ADDITIONAL: 1;; OPT PSEUDOSECTION:; EDNS: version: 0, flags:; udp: 4096;; QUESTION SECTION:;jjjjjjjj1212121212.com. IN A;; Query time: 1 msec;; SERVER: 10.233.0.3#53(10.233.0.3);; WHEN: Wed Nov 21 07:41:35 UTC 2018;; MSG SIZE rcvd: 51 请求耗时仅仅1毫秒。加速效果相当明显。 dnsmasq 容器内的进程组成及职责dnamasq 容器内，其实有2个进程，一个是 dnsmasq-nanny，一个是 dnsmamsq。其实，我们上面看到的 KubeDNS 的 deployment 里关于 kube-dns 容器的配置里的相关参数，并不是直接对 dnsmasq 进程生效的，而是对 dnsmasq-nanny 进程生效的。 dnsmasq-nanny 进程，是 dnsmasq 容器进程的1号进程，是保姆进程，而 dnsmasq 进程，就是由 dnsmasq-nanny 进程 fork 出来的，dnsmasq-nanny 具备 fork、restart 及传递配置参数给 dnsmasq 进程的能力。 我们可以通过进程树来看 dnsmasq-nanny 和 dnsmasq 的关系。 12345678910// 查看进程树/ # pstree -pdnsmasq-nanny(1)---dnsmasq(30)// dnsmasq-nanny 是 dnsmasq 的父进程。// 通过 ps 命令也可以看出来，dnsmasq 的参数，是 dnsmasq-nanny 传过去的/ # cat sss PID USER TIME COMMAND 1 root 12:38 /dnsmasq-nanny -v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053 30 root 268:54 /usr/sbin/dnsmasq -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053 既然 dnsmasq-nanny 是保姆进程，其有几个重要参数： 12345// dnsmasq-nanny 进程的配置目录，这个目录，是需要挂载 configmap 类型 Volume 到此目录的-configDir=/etc/k8s/dns/dnsmasq-nanny// 开启配置检测，当配置发生变更时，重启 dnsmasq 进程-restartDnsmasq=true dnsmasq 容器负责让KubeDNS更新配置生效更新配置很简单，只需要更新名称为 kube-dns 的 ConfigMap 的内容，即可，但这里我们主要从源码角度，讲一下原理是什么。首先再次明确一遍，KubeDNS由多个组件构成，但真正负责处理配置更新的，只有 dnsmasq 容器的 dnsmasq-nanny 进程。 我们通过部分源码，也可以看出来，dnsmasq-nanny 是 如何 重启 dnsmasq 进程的： 1234567891011121314151617181920212223242526272829303132333435363738394041// dns/pkg/dnsmasq/nanny.go#169// RunNanny runs the nanny and handles configuration updates.func RunNanny(sync config.Sync, opts RunNannyOpts) &#123; defer glog.Flush() currentConfig, err := sync.Once() if err != nil &#123; glog.Errorf(&quot;Error getting initial config, using default: %v&quot;, err) currentConfig = config.NewDefaultConfig() &#125; nanny := &amp;Nanny&#123;Exec: opts.DnsmasqExec&#125; nanny.Configure(opts.DnsmasqArgs, currentConfig) if err := nanny.Start(); err != nil &#123; glog.Fatalf(&quot;Could not start dnsmasq with initial configuration: %v&quot;, err) &#125; configChan := sync.Periodic() for &#123; select &#123; case status := &lt;-nanny.ExitChannel: glog.Flush() glog.Fatalf(&quot;dnsmasq exited: %v&quot;, status) break case currentConfig = &lt;-configChan: // 如果接受到新的配置的数据变化，先杀掉dnsmasq，再使用最新配置，启动一个新的dnsmasq进程 if opts.RestartOnChange &#123; glog.V(0).Infof(&quot;Restarting dnsmasq with new configuration&quot;) nanny.Kill() nanny = &amp;Nanny&#123;Exec: opts.DnsmasqExec&#125; nanny.Configure(opts.DnsmasqArgs, currentConfig) nanny.Start() &#125; else &#123; glog.V(2).Infof(&quot;Not restarting dnsmasq (--restartDnsmasq=false)&quot;) &#125; break &#125; &#125;&#125; 那么，配置是什么时候产生变化的呢？ dnsmasq-nanny 进程处理配置变化的方式比较粗暴，如果设置了 -configDir 配置目录的话，此进程会间隔 10s ，进行一次配置检测，如果发生变化，就 重启 dnsmasq。 123456789101112// dns/cmd/dnsmasq-nanny/main.go#72// 进程启动入口部分func main() &#123; parseFlags() glog.V(0).Infof(&quot;opts: %v&quot;, opts) // 运行 dnsmasq-nanny 之前，做配置变化的检测 sync := config.NewFileSync(opts.configDir, opts.syncInterval) dnsmasq.RunNanny(sync, opts.RunNannyOpts)&#125; 123456789101112131415161718192021222324252627// dns/pkg/dns/config/sync.go#81// 计划任务，检测配置目录里的文件，是否发生变化，发生了，则返回func (sync *kubeSync) Periodic() &lt;-chan *Config &#123; // 检测配置变更的动作，用协程开启，异步进行 go func() &#123; // 开启一个间隔为 10s 的计划任务，每次获取最新的配置版本和数据 // resultChan 是一个结构体，核心，是一个版本，以及这个版本对应的所有文件名及文件数据 // 它只是获取数据的版本和具体数据，数据是否变了，sync.syncSource.Periodic 并不管 resultChan := sync.syncSource.Periodic() for &#123; syncResult := &lt;-resultChan // 这里是判断是否数据发生了变化，如果是，则将最新的数据返回 // 后边，我们会看，这个操作是怎么判断数据产生了变化的 config, changed, err := sync.processUpdate(syncResult, false) if err != nil &#123; continue &#125; if !changed &#123; continue &#125; // 返回最新数据（如果走到这一步，说明配置绝对发送了变化，否则会在上面的步骤继续走循环逻辑） sync.channel &lt;- config &#125; &#125;() return sync.channel&#125; 我们看看，是如何判断文件发生变化的 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253// dns/pkg/dns/config/sync.go#99//判断数据是否产生了变化func (sync *kubeSync) processUpdate(result syncResult, buildUnchangedConfig bool) (config *Config, changed bool, err error) &#123; glog.V(4).Infof(&quot;processUpdate %+v&quot;, result) // 这里是判断核心，如果 sync 对象中保存的版本，与 result 对象中保存的版本不同，则认为数据发生了变化 if result.Version != sync.latestVersion &#123; glog.V(3).Infof(&quot;Updating config to version %v (was %v)&quot;, result.Version, sync.latestVersion) changed = true sync.latestVersion = result.Version &#125; else &#123; glog.V(4).Infof(&quot;Config was unchanged (version %v)&quot;, sync.latestVersion) // short-circuit if we haven&apos;t been asked to build an unchanged config object if !buildUnchangedConfig &#123; return &#125; &#125; if result.Version == &quot;&quot; &amp;&amp; len(result.Data) == 0 &#123; config = NewDefaultConfig() return &#125; // 下面都是解析配置的过程了，无需细看 config = &amp;Config&#123;&#125; for key, updateFn := range map[string]fieldUpdateFn&#123; &quot;federations&quot;: updateFederations, &quot;stubDomains&quot;: updateStubDomains, &quot;upstreamNameservers&quot;: updateUpstreamNameservers, &#125; &#123; value, ok := result.Data[key] if !ok &#123; glog.V(3).Infof(&quot;No %v present&quot;, key) continue &#125; if err = updateFn(key, value, config); err != nil &#123; glog.Errorf(&quot;Invalid configuration for %v, ignoring update: %v&quot;, key, err) return &#125; &#125; if err = config.Validate(); err != nil &#123; glog.Errorf(&quot;Invalid configuration: %v (value was %+v), ignoring update&quot;, err, config) config = nil return &#125; return&#125; 综上：dnsmasq-nanny 保姆进程，会一直检测自己参数 -configDir 配置的目录里的文件是否发生了变化，判断的逻辑是：每隔10秒钟，读取一次这个目录下的所有【配置文件数据】（一个Map，key为文件名，值为文件内容），用 sha256 计算一次摘要，作为这个目录下所有数据的版本。然后记录下来，下次的时候执行同样的逻辑，如果发现版本不同，则认为配置文件发送了变化。然后将得到的【配置文件数据】重新解析为 dnsmasq-nanny 的配置数据，最后，杀死 dnsmasq 进程，启动一个系的 dnsmasq 进程。但是，这里边，有几个细节点： 第一：重启 dnsmasq 的方式，先杀后起，方式台粗暴，很可能导致这个时间点的大量DNS请求失败。不优雅。 第二：dnsmasq-nanny 检测数据变化的方式，这种方式就有2个值得注意的问题： ①：官方是，每次遍历目录下的所有文件，然后，使用 ioutil.ReadFile 读取文件内容。如果目录下文件多，可能导致，你遍历的同时，配置文件也在变化，你遍历的速度和文件更新速度不一致，导致，读取的配置，并不一定是最新的，可能你遍历完，某个配置文件才更新完。那么此时，你读取的一部分文件数据并不是和当前目录下文件数据完全一致，本次会重启 dnsmasq。进而，下次检测，还认为有文件变化，到时候，又重启一次 dnsmasq。这种方式不优雅，但问题不大。 ②：文件的检测，直接使用 ioutil.ReadFile 读取文件内容，也存在问题。如果文件变化，和文件读取同时发生，很可能你读取完，文件的更新都没完成，那么你读取的并非一个完整的文件，而是坏的文件，这种文件，dnsmasq-nanny 无法做解析，不过官方代码中有数据校验，解析失败也问题不大，大不了下个周期的时候，再取到完整数据，再解析一次。 kube-dns服务容器及实现原理讲完 KubeDNS 服务的 dnsmasq 容器，现在开始 kube-dns 容器。kube-dns 容器，最主要的职责，就是负责解析 K8S 的内部域名记录，这个解析，它监听了 10053 端口，本质上，kube-dns 是接受 dnsmasq 请求的（ dnsmasq 容器负责处理所有 DNS 请求，对于 K8S 的内部域名请求，转发给 kube-dns 来处理）。 kube-dns 的进程职责是，监视Kubernetes master上 Service 和 Endpoint 的改变，并在内存中维护 lookup 结构用于服务DNS请求。 此容器启动参数为： 1234- --domain=cluster.local.- --dns-port=10053- --config-dir=/kube-dns-config- --v=2 –domain：表示在哪个domain下创建域名记录。–dns-port：这个是启动端口。–config-dir：这个是使用的配置，通常来说，我们的 KubeDNS POD，会使用 ConfigMap，将配置挂载到容器内的 –config-dir 指定的目录上。 这里打算源码层面，追踪一下具体实现，但不打算从头追到尾，这样篇幅太大，只罗列一部分核心点。核心点内容包括： kube-dns 容器，都是监听的哪些 K8S 资源数据，作为域名记录的解析依据？ kube-dns 使用什么技术或数据结构实现内存级数据查询的？ 如果 K8S 有 Service 资源，但没有对应的 POD 资源，域名解析是否还能成功？ kube-dns容器监听哪些K8S资源？我们知道，K8S 内部域名的 DNS 解析，得到的是 Service 的 IP 地址。kube-dns 肯定监听了 Service 资源了，为的是，处理内部域名映射到 Service 的 IP 地址。从源码看，其实 kube-dns 容器，其实监听了2个资源，分别是： Service 资源 Endpoints 资源 至于为何还需要监听 Endpoints 资源，我们一步步解开谜底 123456789101112131415161718192021222324// 开启一个 KubeDNS 处理实例func NewKubeDNS(client clientset.Interface, clusterDomain string, timeout time.Duration, configSync config.Sync) *KubeDNS &#123; kd := &amp;KubeDNS&#123; kubeClient: client, domain: clusterDomain, cache: treecache.NewTreeCache(), cacheLock: sync.RWMutex&#123;&#125;, nodesStore: kcache.NewStore(kcache.MetaNamespaceKeyFunc), reverseRecordMap: make(map[string]*skymsg.Service), clusterIPServiceMap: make(map[string]*v1.Service), domainPath: util.ReverseArray(strings.Split(strings.TrimRight(clusterDomain, &quot;.&quot;), &quot;.&quot;)), initialSyncTimeout: timeout, configLock: sync.RWMutex&#123;&#125;, configSync: configSync, &#125; // 监听并处理 Endpoints 资源 kd.setEndpointsStore() // 监听并处理 Services 资源 kd.setServicesStore() return kd&#125; 123456789101112131415161718192021222324252627282930313233343536373839// setServicesStore 负责处理 Service 资源func (kd *KubeDNS) setServicesStore() &#123; // Returns a cache.ListWatch that gets all changes to services. kd.servicesStore, kd.serviceController = kcache.NewInformer( kcache.NewListWatchFromClient( kd.kubeClient.Core().RESTClient(), &quot;services&quot;, v1.NamespaceAll, fields.Everything()), &amp;v1.Service&#123;&#125;, resyncPeriod, // 这里也可以看出来，它其实主要针对 Service 的 增删改做 handle 处理 kcache.ResourceEventHandlerFuncs&#123; AddFunc: kd.newService, DeleteFunc: kd.removeService, UpdateFunc: kd.updateService, &#125;, )&#125;// 同上func (kd *KubeDNS) setEndpointsStore() &#123; // Returns a cache.ListWatch that gets all changes to endpoints. kd.endpointsStore, kd.endpointsController = kcache.NewInformer( kcache.NewListWatchFromClient( kd.kubeClient.Core().RESTClient(), &quot;endpoints&quot;, v1.NamespaceAll, fields.Everything()), &amp;v1.Endpoints&#123;&#125;, resyncPeriod, kcache.ResourceEventHandlerFuncs&#123; AddFunc: kd.handleEndpointAdd, UpdateFunc: kd.handleEndpointUpdate, // If Service is named headless need to remove the reverse dns entries. DeleteFunc: kd.handleEndpointDelete, &#125;, )&#125; 我们看一下，如果 K8S 有 Service 资源创建出来，kube-dns 容器都做些什么 12345678910111213141516171819202122232425262728293031323334// 针对 K8S 中 Service 资源的创建，做处理func (kd *KubeDNS) newService(obj interface&#123;&#125;) &#123; if service, ok := assertIsService(obj); ok &#123; glog.V(3).Infof(&quot;New service: %v&quot;, service.Name) glog.V(4).Infof(&quot;Service details: %v&quot;, service) // 1、如果 Service 是 ExternaName 类型，则创建 cname 记录 // 稍微提一下，K8S 中的 Service，有几种类型：ClusterIP、NodePort、LoadBalancer、ExternaName // ExternalName services are a special kind that return CNAME records if service.Spec.Type == v1.ServiceTypeExternalName &#123; kd.newExternalNameService(service) return &#125; // 2、如果是无头服务，则处理无头服务方式的DNS记录（A记录），无头服务的域名记录有些不同，所以这里是单独进行处理的 // 需要说明的是，我们刚刚提到的 Endpoints 资源，其实主要就是用在无头服务上的。 // if ClusterIP is not set, a DNS entry should not be created if !v1.IsServiceIPSet(service) &#123; if err := kd.newHeadlessService(service); err != nil &#123; glog.Errorf(&quot;Could not create new headless service %v: %v&quot;, service.Name, err) &#125; return &#125; if len(service.Spec.Ports) == 0 &#123; glog.Warningf(&quot;Service with no ports, this should not have happened: %v&quot;, service) &#125; // 3、创建正常的 Service DNS 记录 kd.newPortalService(service) &#125;&#125; 上面提到，Service 的创建操作，会涉及到 newHeadlessService 无头服务域名记录的操作，具体如下： 123456789101112131415161718192021222324252627282930// 创建无头服务Service的域名记录// Generates skydns records for a headless service.func (kd *KubeDNS) newHeadlessService(service *v1.Service) error &#123; // Create an A record for every pod in the service. // This record must be periodically updated. // Format is as follows: // For a service x, with pods a and b create DNS records, // a.x.ns.domain. and, b.x.ns.domain. key, err := kcache.MetaNamespaceKeyFunc(service) if err != nil &#123; return err &#125; // 根据 Service 名称获取key，在根据 Key，获取这个 Service 下的 Endpoints，用来生成特殊的无头服务的域名记录 e, exists, err := kd.endpointsStore.GetByKey(key) if err != nil &#123; return fmt.Errorf(&quot;failed to get endpoints object from endpoints store - %v&quot;, err) &#125; // 如果这个 Service 下没有 Endpoints，将不生成域名记录，一旦有endpoints之后，就会生成。 if !exists &#123; glog.V(1).Infof(&quot;Could not find endpoints for service %q in namespace %q. DNS records will be created once endpoints show up.&quot;, service.Name, service.Namespace) return nil &#125; if e, ok := e.(*v1.Endpoints); ok &#123; return kd.generateRecordsForHeadlessService(e, service) &#125; return nil&#125; 如果 Service 下没有Endpoints，不处理，一旦有 Endpoints 产生，立即生成域名记录： 123456789101112131415// 根据 Endpoints，创建无头服务记录func (kd *KubeDNS) addDNSUsingEndpoints(e *v1.Endpoints) error &#123; // 先根据 Endpoints 超出其属于哪个 Service svc, err := kd.getServiceFromEndpoints(e) if err != nil &#123; return err &#125; // 判断这个 Service 是不是无头服务的 Service，如果不是，直接返回 if svc == nil || v1.IsServiceIPSet(svc) || svc.Spec.Type == v1.ServiceTypeExternalName &#123; // No headless service found corresponding to endpoints object. return nil &#125; // 如果是无头服务的 Service，生成域名记录 return kd.generateRecordsForHeadlessService(e, svc)&#125; kube-dns使用何种数据结构实现DNS检索？参看后面附录 dns-sidecar容器dns-sidecar容器职责简述这是其实是一个健康监测容器，检查 dnsmasq 和 kube-dns 2个容器的监控状态。先回顾一下 sidecar 容器的参数 123456- args: - --v=2 - --logtostderr - --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,A - --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,A image: gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.10 这个容器，主要职责是探测 dnsmasq 以及 kubedns 服务的状态。但从上面 deployment 的参数，看不出太多东西，从源码角度可以看到更多： 12345678910111213141516// sidecar 容器的默认启动参数// NewOptions creates a new options struct with default values.func NewOptions() *Options &#123; return &amp;Options&#123; DnsMasqAddr: &quot;127.0.0.1&quot;, DnsMasqPort: 53, DnsMasqPollIntervalMs: 5000, // sidecar 容器，开放了一个 10054 端口访问，这样一来，我们可以通过 Prometheus 来收集 sidecar 的数据。 PrometheusAddr: &quot;0.0.0.0&quot;, PrometheusPort: 10054, PrometheusPath: &quot;/metrics&quot;, PrometheusNamespace: &quot;kubedns&quot;, &#125;&#125; sidecar 容器，开放了一个 10054 端口访问，这样一来，我们可以通过 Prometheus 来收集 sidecar 的数据，另外需要说明的是，其实 sidecar 容器开放的 metrics 接口，暴露出来的数据，是 sidecar 探测 dnsmasq 以及 kube-dns 两个目标后，汇总的数据，数据包括： 基本的 Go 应用性能数据（协程数量、CPU使用、打开的最大文件描述符数、内存使用等） dnsmasq 容器发生的错误数 dnsmasq 容器已经发生的 DNS 缓存驱逐次数 dnsmasq 容器已经发生的 DNS 缓存插入次数 dnsmasq 容器缓存未命中次数 dnsmasq 域名解析失败次数 探测到的 dnsmasq 延迟时间 dns-sidecar 是如何做检测的sidecar 的探测周期，默认为 5s 一次。–probe 指定了探测参数，比如： 1- --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,A 表示：探测的服务“标签”为 kubedns，使用域名 kubernetes.default.svc.cluster.local ，将 A 记录类型的DNS解析请求，打到 127.0.0.1:10053 上。 注意：这里的“标签”，是为某个DNS服务目标，打的一个标识而已，后边，这个“标签”会用到。主要是为了方便 通过 dns-sidecar 的 metrics 接口，访问到具体的某个“标签”的DNS服务的健康指标。 指定多个 –probe 参数，则都会探测多个目标服务 123456789101112131415// dns/pkg/sidecar/server.go#43func (s *server) Run(options *Options) &#123; s.options = options glog.Infof(&quot;Starting server (options %+v)&quot;, *s.options) // 循环遍历 probes 目标，然后开启探测 for _, probeOption := range options.Probes &#123; probe := &amp;dnsProbe&#123;DNSProbeOption: probeOption&#125; s.probes = append(s.probes, probe) probe.Start(options) &#125; s.runMetrics(options)&#125; 追一下是如何进行具体的DNS探测的： 1234567891011121314151617181920// dns/pkg/sidecar/dnsprobe.go#74func (p *dnsProbe) Start(options *Options) &#123; glog.V(2).Infof(&quot;Starting dnsProbe %+v&quot;, p.DNSProbeOption) p.lastError = fmt.Errorf(&quot;waiting for first probe&quot;) // 为探测目标，定制抓们的 metrics URL，这样就可以通过 http://x.x.x.x:port/healthcheck/具体标签 的方式，来访问具体某个标签的DNS探测状态了。 // 这里的“标签”，其实就是之前我们提到的所用之处。 http.HandleFunc(&quot;/healthcheck/&quot;+p.Label, p.httpHandler) p.registerMetrics(options) if p.delayer == nil &#123; glog.V(4).Infof(&quot;Using defaultLoopDelayer&quot;) p.delayer = &amp;defaultLoopDelayer&#123;&#125; &#125; // 异步探测动作 go p.loop()&#125; 具体的 DNS 探测动作： 123456789101112131415161718192021222324// dns/okg/sidecar/dnsprobe.go#111func (p *dnsProbe) loop() &#123; glog.V(4).Infof(&quot;Starting loop&quot;) p.delayer.Start(p.Interval) // 初始化一个dns客户端 dnsClient := &amp;dns.Client&#123;&#125; // 循环检测，用不退出 for &#123; glog.V(4).Infof(&quot;Sending DNS request @%v %v&quot;, p.Server, p.Name) // 发送一个 DNS 请求 msg, latency, err := dnsClient.Exchange(p.msg(), p.Server) glog.V(4).Infof(&quot;Got response, err=%v after %v&quot;, err, latency) if err == nil &amp;&amp; len(msg.Answer) == 0 &#123; err = fmt.Errorf(&quot;no RRs for domain %q&quot;, p.Name) &#125; // 更新 DNS请求的metrics指标（延迟）数据 p.update(err, latency) p.delayer.Sleep(latency) &#125;&#125; KubeDNS 服务弹性水平伸缩官方推荐方案：cluster-proportional-autoscaler关于 KubeDNS 服务弹性伸缩，官方已经给出了一套比较简单的弹性水平扩缩容解决方案，下面是一套推荐的配置，用它来部署 KubeDNS 的 autoscaler： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374apiVersion: extensions/v1beta1kind: Deploymentmetadata: labels: addonmanager.kubernetes.io/mode: Reconcile k8s-app: kubedns-autoscaler kubernetes.io/cluster-service: &quot;true&quot; name: kubedns-autoscaler namespace: kube-systemspec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: kubedns-autoscaler strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 1 type: RollingUpdate template: metadata: creationTimestamp: null labels: k8s-app: kubedns-autoscaler spec: affinity: nodeAffinity: preferredDuringSchedulingIgnoredDuringExecution: - preference: matchExpressions: - key: node-role.kubernetes.io/master operator: In values: - &quot;true&quot; weight: 100 podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchLabels: k8s-app: kubedns-autoscaler topologyKey: kubernetes.io/hostname containers: - command: - /cluster-proportional-autoscaler - --namespace=kube-system - --configmap=kubedns-autoscaler - --target=Deployment/kube-dns - --default-params=&#123;&quot;linear&quot;:&#123;&quot;nodesPerReplica&quot;:10,&quot;min&quot;:2&#125;&#125; - --logtostderr=true - --v=2 image: gcr.io/google_containers/cluster-proportional-autoscaler-amd64:1.1.2 imagePullPolicy: IfNotPresent name: autoscaler resources: requests: cpu: 20m memory: 10Mi terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst nodeSelector: beta.kubernetes.io/os: linux restartPolicy: Always schedulerName: default-scheduler securityContext: &#123;&#125; serviceAccount: cluster-proportional-autoscaler serviceAccountName: cluster-proportional-autoscaler terminationGracePeriodSeconds: 30 tolerations: - effect: NoSchedule key: node-role.kubernetes.io/master operator: Equal 我们看几个关键参数：123456- --namespace=kube-system // autoscaler 配置所在的命名空间- --configmap=kubedns-autoscaler // autoscaler 的 configmap 配置名称- --target=Deployment/kube-dns // 表示要弹性伸缩的目标- --default-params=&#123;&quot;linear&quot;:&#123;&quot;nodesPerReplica&quot;:10,&quot;min&quot;:2&#125;&#125; // 默认配置，这个配置可以由 configmap 覆盖- --logtostderr=true- --v=2 我们可以定义 configmap 来配置 kubedns-autoscaler 的具体参数，configmap 中可以配置的内容如下： cluster-proportional-autoscaler 线性配置方式123456789data: linear: |- &#123; &quot;coresPerReplica&quot;: 2, &quot;nodesPerReplica&quot;: 1, &quot;min&quot;: 1, &quot;max&quot;: 100, &quot;preventSinglePointFailure&quot;: true &#125; 123min：表示目标最小实例数，也就是 KubeDNS 最少的实例数量。coresPerReplica：当集群中有很多核（不是可用，而是总量）时，它决定 KubeDNS 的实例数量。nodesPerReplica：当集群中核数（不是可用，而是总量）少时，nodesPerReplica 来控制实例数量。 整体来看，由下面这个公式，的出来最大的DNS实例数： 123replicas = max( ceil( cores * 1/coresPerReplica ) , ceil( nodes * 1/nodesPerReplica ) )replicas = min(replicas, max)replicas = max(replicas, min) cluster-proportional-autoscaler 梯度配置方式省略，也可以查看官方文档。 cluster-proportional-autoscaler 水平扩容是如何实现的 独立部署 cluster-proportional-autoscaler autoscaler 从 APIServer（也就是K8S Master）拉取集群的核数和节点数，并根据这2者，确定一个 POD 最大实例数。 可以通过 configmap 配置 autoscaler 的参数，而不需要 重启 autoscaler 实例。 autoscaler 提供了【线性】及【梯度】2种扩容方式。 这里边有一个小问题：通常，我们使用 ConfigMap 都是讲 ConfigMap 作为 volume ，Mount 到容器上，而 autoscaler 并没有使用这种方式，而是监听了K8S资源（通过 –configmap 及 –namespace 配置）从中获取配置，这种方式非常快而且高效，且不需要重启 autoscaler 实例。 扩容项目为：https://github.com/kubernetes-incubator/cluster-proportional-autoscaler cluster-proportional-autoscaler 不足之处基本上从上面可以知道，这个 扩容器，比较简单，最多就是根据集群的核数，以及节点数来配置最大实例数，其实这个扩缩容，并没有完整的实现出来，官方的意思是，Kubernetes 设想的 Horizontal Pod Autoscaler（https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/） 是一个顶级的资源，需要根据集群中容器的CPU指标来衡量一个合理的值，但是当前这个 cluster-proportional-autoscaler ，仅仅是一个 DIY 实现，所谓的 DIY 实现，意思就是，你得自己手动来控制（比如通过更改 configmap 的方式），它并没有收集集群中容器的CPU利用率等数据，仅仅是一个比较粗略的水平调度实现而已。 Questions到底dnsmasq和kube-dns，谁提供的DNS上游能力？DNS 上游查询能力，也即是我们访问 非 K8S 内的域名，比如 youku.com，是 dnsmasq 向上游 DNS 服务器查询的，还是 kube-dns 来做的？ 首先，dnsmasq 是 K8S 内 DNS 服务的入口，它决定内部的域名往 kube-dns 转发，其他的域名，往上游 DNS 服务器转发，然后将结果根据域名的TTL做缓存，加速DNS查询。 从上面文章的分析看，应该就是 dnsmasq 来上游查询处理的，但是，从 kube-dns 源码上看，其实 kube-dns 本身，也是具备提供 DNS 上游查询能力的。我们需要具体来看，这个上游查询，是谁来做的。 在 K8S 中，我们这样设置 DNS 上游，需要在 名为 kube-dns 的 ConfigMap 中来做。比如： 12345678910apiVersion: v1kind: ConfigMapmetadata: name: kube-dns namespace: kube-systemdata: stubDomains: | &#123;&quot;acme.local&quot;: [&quot;1.2.3.4&quot;]&#125; upstreamNameservers: | [&quot;8.8.8.8&quot;, &quot;8.8.4.4&quot;] 上游DNS服务器字段：upstreamNameservers。 既然是需要配置 ConfigMap 来使配置生效，那么就看一下，是谁 使用了这个 ConfigMap 就行了。所以，我们回顾文章最开始部分的 deployment，看看谁将 ConfigMap 作为 Voloume 挂载到自己容器里使用就行了。 我们能够看到，使用 ConfigMap 的，有2个容器，分别是： ①：dnsmasq 容器②：kube-dns 容器 kube-dns 容器使用这个 ConfigMap，肯定是可以根据其配置，使用配置中的 upstreamNameservers 中的地址来决定域名解析请求往何处转发的。但问题在于，它有这个能力，但不代表上游解析，真的由它来做。 这里边需要注意的是，dnsmasq 容器也使用了这个配置，但并不是 dnsmasq 容器的 dnsmasq 进程直接使用的，而是 dnsmasq 容器中的 dnsmasq-nanny 进程来使用的，这个进程本身就是保姆进程，当 ConfigMap 变化后，dnsmasq-nanny 进程，便会解析配置，将配置中的 upstreamNameservers、stubDomains 内容，转换为 dnsmasq 进程能够识别的参数，然后杀死 原来的 dnsmasq 进程，启动一个新的。 所以，在 KubeDNS 服务中，提供上游DNS解析能力的，是 dnsmasq 容器，而不是 kube-dns 容器。 附录1：kube-dns使用何种数据结构实现DNS检索 后续有时间再补充。]]></content>
      <categories>
        <category>kubernetes</category>
        <category>kubedns</category>
        <category>coredns</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>kubedns</tag>
        <tag>coredns</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CoreDNS系列1：Kubernetes内部域名解析原理、弊端及优化方式]]></title>
    <url>%2F2018%2F11%2F20%2F9%2F</url>
    <content type="text"><![CDATA[Kubernetes 中的 DNS本篇，是 CoreDNS 的前篇之一，后续会着重介绍 CoreDNS，但是步步深入讲 CoreDNS，讲一下 Kubernetes，以及 kubedns 有一定的必要，所以，就有了 CoreDNS 系列，本篇主要尽可能详尽的说明 Kubernetes 的DNS解析原理，以及 Kubernetes 集群中 DNS 解析目前存在的弊端和优化方式。 在 Kubernetes 中，服务发现有几种方式：①：基于环境变量的方式②：基于内部域名的方式 基本上，使用环境变量的方式很少，主要还是使用内部域名这种服务发现的方式。 其中，基于内部域名的方式，涉及到 Kubernetes 内部域名的解析，而 kubedns，是 Kubernetes 官方的 DNS 解析组件。从 1.11 版本开始，kubeadm 已经使用第三方的 CoreDNS 替换官方的 kubedns 作为 Kubernetes 集群的内部域名解析组件，我们的重点，是 CoreDNS，但是在开始 CoreDNS 之前，需要先了解下 kubedns，后续，会对这2个 DNS 组件做对比，分析它们的优劣势。 Kubernetes 中的域名是如何解析的在 Kubernetes 中，比如服务 a 访问服务 b，对于同一个 Namespace下，可以直接在 pod 中，通过 curl b 来访问。对于跨 Namespace 的情况，服务名后边对应 Namespace即可。比如 curl b.default。那么，使用者这里边会有几个问题： ①：服务名是什么？②：为什么同一个 Namespace 下，直接访问服务名即可？不同 Namespace 下，需要带上 Namespace 才行？③：为什么内部的域名可以做解析，原理是什么？ DNS 如何解析，依赖容器内 resolv 文件的配置 1234cat /etc/resolv.confnameserver 10.233.0.3search default.svc.cluster.local svc.cluster.local cluster.local 这个文件中，配置的 DNS Server，一般就是 K8S 中，kubedns 的 Service 的 ClusterIP，这个IP是虚拟IP，无法ping，但可以访问。 1234[root@node4 user1]# kubectl get svc -n kube-systemNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkube-dns ClusterIP 10.233.0.3 &lt;none&gt; 53/UDP,53/TCP 270dkubernetes-dashboard ClusterIP 10.233.22.223 &lt;none&gt; 443/TCP 124d 所以，所有域名的解析，其实都要经过 kubedns 的虚拟IP 10.233.0.3 进行解析，不论是 Kubernetes 内部域名还是外部的域名。 Kubernetes 中，域名的全称，必须是 service-name.namespace.svc.cluster.local 这种模式，服务名，就是Kubernetes中 Service 的名称，所以，当我们执行下面的命令时： 1curl b 必须得有一个 Service 名称为 b，这是前提。在容器内，会根据 /etc/resolve.conf 进行解析流程。选择 nameserver 10.233.0.3 进行解析，然后，用字符串 “b”，依次带入 /etc/resolve.conf 中的 search 域，进行DNS查找，分别是： 12// search 内容类似如下（不同的pod，第一个域会有所不同）search default.svc.cluster.local svc.cluster.local cluster.local b.default.svc.cluster.local -&gt; b.svc.cluster.local -&gt; b.cluster.local ，直到找到为止。所以，我们执行 curl b，或者执行 curl b.default，都可以完成DNS请求，这2个不同的操作，会分别进行不同的DNS查找步骤： 1234567// curl b，可以一次性找到（b +default.svc.cluster.local）b.default.svc.cluster.local// curl b.default，第一次找不到（ b.default + default.svc.cluster.local）b.default.default.svc.cluster.local// 第二次查找（ b.default + svc.cluster.local），可以找到b.default.svc.cluster.local So Questionscurl b，要比 curl b.default 效率高？答案是肯定的，因为 curl b.default，多经过了一次 DNS 查询。当执行 curl b.default，也就使用了带有命名空间的内部域名时，容器的第一个 DNS 请求是 12// b.default + default.svc.cluster.localb.default.default.svc.cluster.local 当请求不到 DNS 结果时，使用 12// b.default + svc.cluster.localb.default.svc.cluster.local 进行请求，此时才可以得到正确的DNS解析。 访问外部域名走 search 域吗这个答案，不能说肯定也不能说否定，看情况，可以说，大部分情况要走 search 域。 我们以请求 youku.com 为例，通过抓包的方式，看一看在某个容器中访问 youku.com，进行的DNS查找的过程，都产生了什么样的数据包。注意：我们要抓DNS容器的包，就得先进入到DNS容器的网络中（而不是发起DNS请求的那个容器）。 由于DNS容器往往不具备bash，所以无法通过 docker exec 的方式进入容器内抓包，我们采用其他的方式： 123456// 1、找到容器ID，并打印它的NS IDdocker inspect --format &quot;&#123;&#123;.State.Pid&#125;&#125;&quot; 16938de418ac// 2、进入此容器的网络Namespacensenter -n -t 54438// 3、抓DNS包tcpdump -i eth0 udp dst port 53|grep youku.com 在其他的容器中，进行 youku.com 域名查找 1nslookup youku.com 172.22.121.65 注意：nslookup命令的最后指定DNS服务容器的IP，是因为，如果不指定，且DNS服务的容器存在多个的话，那么DNS请求，可能会均分到所有DNS服务的容器上，我们如果只抓某单个DNS服务容器抓到的包，可能就不全了，指定IP后，DNS的请求，就必然只会打到单个的DNS容器。抓包的数据才完整。 可以看到类似如下结果： 1234517:01:28.732260 IP 172.20.92.100.36326 &gt; nodexxxx.domain: 4394+ A? youku.com.default.svc.cluster.local. (50)17:01:28.733158 IP 172.20.92.100.49846 &gt; nodexxxx.domain: 60286+ A? youku.com.svc.cluster.local. (45)17:01:28.733888 IP 172.20.92.100.51933 &gt; nodexxxx.domain: 63077+ A? youku.com.cluster.local. (41)17:01:28.734588 IP 172.20.92.100.33401 &gt; nodexxxx.domain: 27896+ A? youku.com. (27)17:01:28.734758 IP nodexxxx.34138 &gt; 192.168.x.x.domain: 27896+ A? youku.com. (27) 我们可以看到，在真正解析 youku.com 之前，经历了 youku.com.default.svc.cluster.local. -&gt; youku.com.svc.cluster.local. -&gt; youku.com.cluster.local. -&gt; youku.com. 这也就意味着有3次DNS请求，是浪费的无意义的请求。 为何会出现DNS请求浪费的情况这是因为，在 Kubernetes 中，其实 /etc/resolv.conf 这个文件，并不止包含 nameserver 和 search 域，还包含了非常重要的一项：ndots。我们之前没有提及这个项，也是希望再次能引起读者重视。 1234[root@xxxx-67f54c6dff-h4zxq /]# cat /etc/resolv.conf nameserver 10.233.0.3search cicd.svc.cluster.local svc.cluster.local cluster.localoptions ndots:5 ndots:5，表示：如果查询的域名包含的点“.”，不到5个，那么进行DNS查找，将使用非完全限定名称（或者叫绝对域名），如果你查询的域名包含点数大于等于5，那么DNS查询，默认会使用绝对域名进行查询。举例来说： 如果我们请求的域名是，a.b.c.d.e，这个域名中有4个点，那么容器中进行DNS请求时，会使用非绝对域名进行查找，使用非绝对域名，会按照 /etc/resolv.conf 中的 search 域，走一遍追加匹配： a.b.c.d.e.cicd.svc.cluster.local. -&gt;a.b.c.d.e.svc.cluster.local. -&gt;a.b.c.d.e.cluster.local. 直到找到为止。如果走完了search域还找不到，则使用 a.b.c.d.e. ，作为绝对域名进行DNS查找。 我们通过抓包分析一个具体案例： 域名中点数少于5个的情况： 12345678910111213141516// 对域名 a.b.c.d.ccccc 进行DNS解析请求 [root@xxxxx-67f54c6dff-h4zxq /]# nslookup a.b.c.d.ccccc 172.22.121.65 Server: 172.22.121.65Address: 172.22.121.65#53** server can&apos;t find a.b.c.d.ccccc: NXDOMAIN// 抓包数据如下：18:08:11.013497 IP 172.20.92.100.33387 &gt; node011094.domain: 28844+ A? a.b.c.d.ccccc.cicd.svc.cluster.local. (54)18:08:11.014337 IP 172.20.92.100.33952 &gt; node011094.domain: 57782+ A? a.b.c.d.ccccc.svc.cluster.local. (49)18:08:11.015079 IP 172.20.92.100.45984 &gt; node011094.domain: 55144+ A? a.b.c.d.ccccc.cluster.local. (45)18:08:11.015747 IP 172.20.92.100.54589 &gt; node011094.domain: 22860+ A? a.b.c.d.ccccc. (31)18:08:11.015970 IP node011094.36383 &gt; 192.168.x.x.domain: 22860+ A? a.b.c.d.ccccc. (31)// 结论：// 点数少于5个，先走search域，最后将其视为绝对域名进行查询 域名中点数&gt;=5个的情况： 12345678910111213141516// 对域名 a.b.c.d.e.ccccc 进行DNS解析请求[root@xxxxx-67f54c6dff-h4zxq /]# nslookup a.b.c.d.e.ccccc 172.22.121.65 Server: 172.22.121.65Address: 172.22.121.65#53** server can&apos;t find a.b.c.d.e.ccccc: NXDOMAIN// 抓包数据如下：18:10:14.514595 IP 172.20.92.100.34423 &gt; node011094.domain: 61170+ A? a.b.c.d.e.ccccc. (33)18:10:14.514856 IP node011094.58522 &gt; 192.168.x.x.domain: 61170+ A? a.b.c.d.e.ccccc. (33)18:10:14.515880 IP 172.20.92.100.49328 &gt; node011094.domain: 267+ A? a.b.c.d.e.ccccc.cicd.svc.cluster.local. (56)18:10:14.516678 IP 172.20.92.100.35651 &gt; node011094.domain: 54181+ A? a.b.c.d.e.ccccc.svc.cluster.local. (51)18:10:14.517356 IP 172.20.92.100.33259 &gt; node011094.domain: 53022+ A? a.b.c.d.e.ccccc.cluster.local. (47)// 结论：// 点数&gt;=5个，直接视为绝对域名进行查找，只有当查询不到的时候，才继续走 search 域。 如何优化 DNS 请求浪费的情况优化方式1：使用全限定域名其实最直接，最有效的优化方式，就是使用 “fully qualified name”，简单来说，使用“完全限定域名”（也叫绝对域名），你访问的域名，必须要以 “.” 为后缀，这样就会避免走 search 域进行匹配，我们抓包再试一次： 12// 注意：youku.com 后边有一个点 .nslookup youku.com. 172.22.121.65 在DNS服务容器上抓到的包如下： 1216:57:07.628112 IP 172.20.92.100.36772 &gt; nodexxxx.domain: 46851+ [1au] A? youku.com. (38)16:57:07.628339 IP nodexxxx.47350 &gt; 192.168.x.x.domain: 46851+ [1au] A? youku.com. (38) 并没有多余的DNS请求。 优化方式2：具体应用配置特定的 ndots其实，往往我们还真不太好用这种绝对域名的方式，有谁请求youku.com的时候，还写成 youku.com. 呢？ 在 Kubernetes 中，默认设置了 ndots 值为5，是因为，Kubernetes 认为，内部域名，最长为5，要保证内部域名的请求，优先走集群内部的DNS，而不是将内部域名的DNS解析请求，有打到外网的机会，Kubernetes 设置 ndots 为5是一个比较合理的行为。 如果你需要定制这个长度，最好是为自己的业务，单独配置 ndots 即可（Pod为例，其实配置deployment最好）。 12345678910111213apiVersion: v1kind: Podmetadata: namespace: default name: dns-examplespec: containers: - name: test image: nginx dnsConfig: options: - name: ndots value: &quot;1&quot; Kubernetes DNS 策略在Kubernetes 中，有4种 DNS 策略，从 Kubernetes 源码中看： 1234567891011121314151617181920const ( // DNSClusterFirstWithHostNet indicates that the pod should use cluster DNS // first, if it is available, then fall back on the default // (as determined by kubelet) DNS settings. DNSClusterFirstWithHostNet DNSPolicy = &quot;ClusterFirstWithHostNet&quot; // DNSClusterFirst indicates that the pod should use cluster DNS // first unless hostNetwork is true, if it is available, then // fall back on the default (as determined by kubelet) DNS settings. DNSClusterFirst DNSPolicy = &quot;ClusterFirst&quot; // DNSDefault indicates that the pod should use the default (as // determined by kubelet) DNS settings. DNSDefault DNSPolicy = &quot;Default&quot; // DNSNone indicates that the pod should use empty DNS settings. DNS // parameters such as nameservers and search paths should be defined via // DNSConfig. DNSNone DNSPolicy = &quot;None&quot;) 这几种DNS策略，需要在Pod，或者Deployment、RC等资源中，设置 dnsPolicy 即可，以 Pod 为例： 123456789101112131415161718192021222324252627apiVersion: v1kind: Podmetadata: labels: name: cadvisor-nodexxxx hostip: 192.168.x.x name: cadvisor-nodexxxx namespace: monitoringspec: containers: - args: - --profiling - --housekeeping_interval=10s - --storage_duration=1m0s image: google/cadvisor:latest name: cadvisor-nodexxxx ports: - containerPort: 8080 name: http protocol: TCP resources: &#123;&#125; securityContext: privileged: true terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst nodeName: nodexxxx 具体来说： None 表示空的DNS设置这种方式一般用于想要自定义 DNS 配置的场景，而且，往往需要和 dnsConfig 配合一起使用达到自定义 DNS 的目的。 Default 有人说 Default 的方式，是使用宿主机的方式，这种说法并不准确。这种方式，其实是，让 kubelet 来决定使用何种 DNS 策略。而 kubelet 默认的方式，就是使用宿主机的 /etc/resolv.conf（可能这就是有人说使用宿主机的DNS策略的方式吧），但是，kubelet 是可以灵活来配置使用什么文件来进行DNS策略的，我们完全可以使用 kubelet 的参数：–resolv-conf=/etc/resolv.conf 来决定你的DNS解析文件地址。 ClusterFirst 这种方式，表示 POD 内的 DNS 使用集群中配置的 DNS 服务，简单来说，就是使用 Kubernetes 中 kubedns 或 coredns 服务进行域名解析。如果解析不成功，才会使用宿主机的 DNS 配置进行解析。 ClusterFirstWithHostNet 在某些场景下，我们的 POD 是用 HOST 模式启动的（HOST模式，是共享宿主机网络的），一旦用 HOST 模式，表示这个 POD 中的所有容器，都要使用宿主机的 /etc/resolv.conf 配置进行DNS查询，但如果你想使用了 HOST 模式，还继续使用 Kubernetes 的DNS服务，那就将 dnsPolicy 设置为 ClusterFirstWithHostNet。 下一篇，我们将主要讲 kubedns 的架构组成。]]></content>
      <categories>
        <category>kubernetes</category>
        <category>kubedns</category>
        <category>coredns</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>kubedns</tag>
        <tag>coredns</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一次对进程大量积压 ESTABLISHED 链接的排查记录]]></title>
    <url>%2F2018%2F11%2F16%2F8%2F</url>
    <content type="text"><![CDATA[背景我们都知道，基于Kubernetes的微服务，大行其道，传统部署模式一直都在跟着变化，但其实，在原有业务向服务化方向过度过程中，有些场景可能会变得复杂。 比如说：将Kubernetes的模式应用到开发环节上，这个环节需要频繁的变更代码，微服务的方式，可能就需要不断的： 改代码-&gt;构建镜像-&gt;镜像推送-&gt;部署-&gt;拉去镜像-&gt;生成容器 尤其是PHP的业务，不需要构建二进制，仅需要发布代码，因此，如果按照上面的部署方式，就需要频繁改代码，走构建镜像这个流程，最后再做发布，这在开发环节就显得过于麻烦了，换而言之，有没有办法，能让开发直接将代码上传到容器中呢？ 其实是有的，就是设计一个FTP中间件代理，让用户本地改完代码，通过FTP客户端（很多IDE是支持FTP的）直接上传到容器内部，甚至于用户保存一下代码就上传到容器内。 因此，这就引出了今天的主角，是我基于FTP协议+gRPC协议自研的FTP代理工具。 这个工具上线后，服务全公司所有研发，经过一段时间运行和修补，相对稳定，也做了一些关于内存方面的优化，直到又一次，在维护这个FTP代理的时候，发现一个奇怪的问题： FTP代理进程，监听的是 192.168.88.32 的 21 端口，所以，这个端口对应了多少连接，就表示有多少个客户端存在，通过： 1netstat -apn |grep &quot;192.168.88.32:21&quot; 发现，有将近1000个链接，且都是 ESTABLISHED，ESTABLISHED 状态表示一个连接的状态是“已连接”，但我们研发团队，并没有那么多人，直觉上看，事出反常必有妖。 初步分析可能性感觉可能有一种情况，就是每个人开了多个FTP客户端，实际场景下，研发同学组可能会使用3种类型的FTP客户端 PHPStorm：这个客户端（SFTP插件）自己会维护一个FTP长连接。Sublime + VsCode，这2个客户端不会维护链接，数据交互完成（比如传输任务），就主动发送 QUIT 指令到FTP代理端，然后所有链接关闭。很干净。 另外，使用PHPStorm的话，也存在开多个IDE创建，就使用多个FTP客户端连接的情况。为了继续排查，我把所有对 192.168.88.32:21 的链接，做了分组统计，看看哪个IP的连接数最多 12# 注：61604 是 ftp代理的进程IDnetstat -apn|grep &quot;61604/server&quot;|grep &apos;192.168.88.32:21&apos;|awk -F &apos;:&apos; &apos;&#123;print$2&#125;&apos;|awk &apos;&#123;print$2&#125;&apos;|sort|uniq -c |sort 上面的统计，是看哪个IP，对 192.168.88.32:21 连接数最多（18个）。 统计发现，很多IP，都存在多个链接的情况，难道每个人都用了多个IDE且可能还多IDE窗口使用吗？于是，挑了一个最多的，找到公司中使用这个IP的人，沟通发现，他确实使用了IDE多窗口，但是远远没有使用18个客户端那么多，仅仅PHPStorm开了3个窗口而已。 初步排查结论：应该是FTP代理所在服务器的问题，和用户开多个客户端没有关系。 进一步排查这次排查，是怀疑，这将近1000个的 ESTABLISHED 客户端链接中，有大量假的 ESTABLISHED 链接存在，之前的统计发现，实际上，对 192.168.88.32:21 的客户端链接进行筛选，得到的IP，一共才200个客户端IP而已，平均下来，每个人都有5个FTP客户端链接FTP代理，想象觉得不太可能。那么，如何排查 ESTABLISHED 假链接呢？ 在 TCP 四次挥手过程中，首先需要有一端，发起 FIN 包，接收方接受到 FIN 包之后，便开启四次挥手的过程，这也是连接断开的过程。 从之前的排查看，有人的IP，发起了多达18个FTP连接，那么，要排查是不是在 FTP 代理服务器上，存在假的 ESTABLISHED 连接的话，就首先需要去 开发同学的机器上看，客户端连接的端口，是不是仍在使用。比如： 1tcp ESTAB 0 0 192.168.88.32:21 192.168.67.38:58038 这个表明，有一个研发的同学 IP是 192.168.67.38，使用了端口 58038，连接 192.168.88.32 上的 FTP 代理服务的 21 端口。所以，先要去看，到底研发同学的电脑上，这个端口存在不存在。 后来经过与研发同学沟通确认，研发电脑上并没有 58038 端口使用，这说明，对FTP代理服务的的客户端链接中显示的端口，也就是实际用户的客户端端口，存在大量不存在的情况。 结论：FTP代理服务器上，存在的近1000个客户端连接中（ESTABLISHED状态），有大量的假连接存在。也就是说，实际上这个连接早就断开不存在了，但服务端却还显示存在。 排查假 ESTABLISHED 连接首先，如果出现假的 ESTABLISHED 连接，表示连接的客户端已经不存在了，客户端一方，要么发起了 TCP FIN 请求服务端没有收到，比如因为网络的各种原因（比如断网了）之后，FTP客户端无法发送FIN到服务端。要么服务端服务器接受到了 FIN，但是在后续过程中，丢包了等等。 为了验证上面的问题，我本机进行了一次模拟，连接FTP服务端后，本机直接断网，断网后，杀死FTP客户端进程，等待5分钟（为什么等待5分钟后面说）后，重新联网。然后再 FTP 服务端，查看服务器上与 FTP代理进行连接的所有IP，然后发现我本机的IP和端口依然在列，然后再我本机，通过 1lsof -i :端口号 却没有任何记录，直接说明：服务端确实保持了假 ESTABLISHED 链接，一直不释放。 上面提到，我等待5分钟，是因为，服务端的 keepalive，是这样的配置： 1234[root@xx xx]# sysctl -a |grep keepalivenet.ipv4.tcp_keepalive_intvl = 75net.ipv4.tcp_keepalive_probes = 9net.ipv4.tcp_keepalive_time = 300 服务器默认设置的 tcp keepalive 检测是300秒后进行检测，也就是5分钟，当检测失败后，一共进行9次重试，每次时间间隔是75秒。那么，问题就来了，服务器设置了 keepalive，如果 300 + 9*75 秒后，依然连接不上，就应该主动关闭假 ESTABLISHED 连接才对。为何还会积压呢？ 猜想1：大量的积压的 ESTABLISHED 连接，实际上都还没有到释放时间为了验证这个问题，我们就需要具体的看某个连接，什么时候创建的。所以，我找到其中一个我确定是假的 ESTABLISHED的链接（那个IP的用户，把所有FTP客户端都关了，进程也杀死了），看此连接的创建时间，过程如下： 先确定 FTP 代理进程的ID，为 61604 然后，看看这个进程的所有连接，找到某个端口的（55360，就是一个客户端所使用的端口） 12[root@xxx xxx]# lsof -p 61604|grep 55360server 61604 root 6u IPv4 336087732 0t0 TCP node088032:ftp-&gt;192.168.70.16:55360 (ESTABLISHED) 我们看到一个 “6u”，这个就是进程使用的这个连接的socket文件，Linux中，一切皆文件。我们看看这个文件的创建时间，就是这个连接的创建时间了 123ll /proc/61604/fd/6//输出：lrwx------. 1 root root 64 Nov 1 14:03 /proc/61604/fd/6 -&gt; socket:[336087732] 这个连接是11月1号创建的，现在已经11月8号，这个时间，早已经超出了 keepalive 探测 TCP连接是否存活的时间。这说明2个点： 1、可能 Linux 的 KeepAlive 压根没生效。2、可能我的 FTP 代理进程，压根没有使用 TCP KeepAlive 猜想2： FTP 代理进程，压根没有使用 TCP KeepAlive要验证这个结论，就得先知道，怎么看一个连接，到底具不具备 KeepAlive 功效？ netstat 命令不好使（也可能我没找到方法），我们使用 ss 命令，查看 FTP进程下所有连接21端口的链接 1ss -aoen|grep 192.168.12.32:21|grep ESTAB 从众多结果中，随便筛选2个结果： 12tcp ESTAB 0 0 192.168.12.32:21 192.168.20.63:63677 ino:336879672 sk:65bb &lt;-&gt;tcp ESTAB 0 0 192.168.12.32:21 192.168.49.21:51896 ino:336960511 sk:67f7 &lt;-&gt; 我们再对比一下，所有连接服务器sshd进程的 1234tcp ESTAB 0 0 192.168.12.32:333 192.168.53.207:63269 timer:(keepalive,59sec,0) ino:336462258 sk:6435 &lt;-&gt;tcp ESTAB 0 0 192.168.12.32:333 192.168.55.185:64892 timer:(keepalive,3min59sec,0) ino:336461969 sk:62d1 &lt;-&gt;tcp ESTAB 0 0 192.168.12.32:333 192.168.53.207:63220 timer:(keepalive,28sec,0) ino:336486442 sk:6329 &lt;-&gt;tcp ESTAB 0 0 192.168.12.32:333 192.168.53.207:63771 timer:(keepalive,12sec,0) ino:336896561 sk:65de &lt;-&gt; 对比很容易发现，连接 21端口的所有连接，多没有 timer 项。这说明，FTP代理 进程监听 21 端口时，所有进来的链接，全都没有使用keepalive。 找了一些文章，大多只是说，怎么配置Linux 的 Keep Alive，以及不配置的，会造成 ESTABLISHED 不释放问题，没有说进程需要额外设置啊？难道 Linux KeepAlive 配置，不是对所有连接直接就生效的？ 所以，我们有必要验证 Linux keepalive，必须要进程自己额外开启才能生效 验证 Linux keepalive，必须要进程自己额外开启才能生效在开始这个验证之前，先摘取一段FTP中间件代理关于监听 21 端口的部分代码： 1234567891011121314151617181920212223242526272829func (ftpServer *FTPServer) ListenAndServe() error &#123; laddr, err := net.ResolveTCPAddr(&quot;tcp4&quot;, ftpServer.listenTo) if err != nil &#123; return err &#125; listener, err := net.ListenTCP(&quot;tcp4&quot;, laddr) if err != nil &#123; return err &#125; for &#123; clientConn, err := listener.AcceptTCP() if err != nil || clientConn == nil &#123; ftpServer.logger.Print(&quot;listening error&quot;) break &#125; //以闭包的方式整理处理driver和ftpBridge，协程结束整体由GC做资源释放 go func(c *net.TCPConn) &#123; driver, err := ftpServer.driverFactory.NewDriver(ftpServer.FTPDriverType) if err != nil &#123; ftpServer.logger.Print(&quot;Error creating driver, aborting client connection：&quot; + err.Error()) &#125; else &#123; ftpBridge := NewftpBridge(c, driver) ftpBridge.Serve() &#125; c = nil &#125;(clientConn) &#125; return nil&#125; 足够明显，整个函数，net.ListenTCP 附近都没有任何设置KeepAlive的相关操作。我们查看 相关函数，找到了设置 KeepAlive的地方，进行一下设置： 123456if err != nil || clientConn == nil &#123; ftpServer.logger.Print(&quot;listening error&quot;) break&#125;// 此处，设置 keepaliveclientConn.SetKeepAlive(true) 重新构建部署之后，可以看到，所有对21端口的连接，全部都带了 timer 1ss -aoen|grep 192.168.12.32:21|grep ESTAB 输出如下： 1234tcp ESTAB 0 0 192.168.12.32:21 192.168.70.76:54888 timer:(keepalive,1min19sec,0) ino:397279721 sk:6b49 &lt;-&gt;tcp ESTAB 0 0 192.168.12.32:21 192.168.37.125:49648 timer:(keepalive,1min11sec,0) ino:398533882 sk:6b4a &lt;-&gt;tcp ESTAB 0 0 192.168.12.32:21 192.168.33.196:64471 timer:(keepalive,7.957ms,0) ino:397757143 sk:6b4c &lt;-&gt;tcp ESTAB 0 0 192.168.12.32:21 192.168.21.159:56630 timer:(keepalive,36sec,0) ino:396741646 sk:6b4d &lt;-&gt; 可以很明显看到，所有的连接，全部具备了 timer 功效，说明：想要使用 Linux 的 KeepAlive，需要程序单独做设置进行开启才行。 最后：ss 命令结果中 keepalive 的说明首先，看一下 Linux 中的配置，我的机器如下： 1234[root@xx xx]# sysctl -a |grep keepalivenet.ipv4.tcp_keepalive_intvl = 75net.ipv4.tcp_keepalive_probes = 9net.ipv4.tcp_keepalive_time = 300 tcp_keepalive_time：表示多长时间后，开始检测TCP链接是否有效。tcp_keepalive_probes：表示如果检测失败，会一直探测 9 次。tcp_keepalive_intvl：承上，探测9次的时间间隔为 75 秒。 然后，我们看一下 ss 命令的结果： 1ss -aoen|grep 192.168.12.32:21|grep ESTAB 1tcp ESTAB 0 0 192.168.12.32:21 192.168.70.76:54888 timer:(keepalive,1min19sec,0) ino:397279721 sk:6b49 &lt;-&gt; 摘取这部分：timer:(keepalive,1min19sec,0) ，其中： keepalive：表示此链接具备 keepalive 功效。1min19sec：表示剩余探测时间，这个时间每次看都会边，是一个递减的值，第一次探测，需要 net.ipv4.tcp_keepalive_time 这个时间倒计时，如果探测失败继续探测，后边会按照 net.ipv4.tcp_keepalive_intvl 这个时间值进行探测。直到探测成功。0：这个值是探测时，检测到这是一个无效的TCP链接的话已经进行了的探测次数。]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes是如何对资源（CPU、内存等）做限制的]]></title>
    <url>%2F2018%2F10%2F31%2F7%2F</url>
    <content type="text"><![CDATA[Kubernetes对资源的限制在Kubernetes中，对资源（CPU、内存等）的限制，需要定义在yaml中，以Deployment举例： 123456789101112131415161718apiVersion: v1kind: Podmetadata: name: cpu-overload namespace: testspec: containers: - name: cpu-overload image: stress/stress:latest resources: limits: cpu: &quot;2&quot; memory: 1000Mi requests: cpu: &quot;1&quot; memory: 500Mi command: [&quot;stress&quot;] args: [&quot;-c&quot;, &quot;2&quot;] 其中，CPU 有2个限制： requests：相对限制，是容器的最低申请资源，这个限制是相对的，无法做到绝对严格。 limits：绝对限制，这个是限制的绝对的，不可能超越。 本例中，对容器 cpu-overload 的 CPU 的限制，是，申请1个核的运算资源，最多可以使用2个核。 这里需要特别说明一点，所谓的最多2个核，其实是均摊的，如果这个容器真的触发了计算瓶颈，在docker中看，CPU使用率是200%，但在宿主机去看，其实并非是将2个核占满了，而是将压力分摊到了多个CPU的核上。 对Kubernetes来说，只能做到限制容器资源，无法对pod资源做限制，Kubernetes官方认为，要计算一个pod的资源限制，将pod中各个容器的资源做加和就行了。这里不展示详细说，具体怎么为Kubernetes限制内存和CPU，可以直接参看官方文档：Managing Compute Resources for Containers ，这篇文章写的够详细了。 资源限制的传递Kubernetes其实可以认为是一系列组件包装起来的一个大型工具。关于资源限制，其实Kubernetes自己做不了这些，而是将对资源限制，通过yaml中的定义，传递到Docker容器中。比如，之前我们在Deployment中容器的CPU，限制为最多使用2个核，这个限制，Kubernetes会传递给Docker来做，所以本质上，Kubernetes资源的限制能力，来源于Docker，而Docker能做到什么程度的限制，又取决于Linux的cgroups，所以在很早之前的Docker是不支持在Windows平台运行的，归根结底，还是因为cgroups是Linux内核支持的产物。 说了这么多，我们可以通过一个实例来说明这个传递性。在开始前，简单说一下步骤: 在Kubernetes中启动一个单独的pod，资源限制为最多4个CPU核。 找到这个pod对应的容器，看一下容器的运行配置，是不是限制了4个核。 找到这个容器对应的Cgroups配置，看是否对容器限制了4个核。 实验首先，创建一个限制了1个核的pod12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576apiVersion: v1kind: Podmetadata: labels: testsss: cadvisor-test name: cadvisor-test namespace: testspec: containers: - args: - -allow_dynamic_housekeeping=true - -global_housekeeping_interval=1m0s - -housekeeping_interval=5s - -disable_metrics=udp,tcp,percpu,sched,disk,network - -storage_duration=15s - -profiling=true - -enable_load_reader=true - -port=30008 - -max_procs=1 image: google/cadvisor imagePullPolicy: IfNotPresent name: cadvisor-test ports: - containerPort: 30008 hostPort: 30008 name: http protocol: TCP resources: limits: cpu: &quot;1&quot; memory: 2000Mi requests: cpu: &quot;0.1&quot; memory: 100Mi terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /rootfs name: rootfs readOnly: true - mountPath: /var/run name: var-run readOnly: true - mountPath: /sys name: sys readOnly: true - mountPath: /var/lib/docker name: docker readOnly: true - mountPath: /dev/disk name: disk readOnly: true dnsPolicy: ClusterFirst hostNetwork: true nodeName: nodexxx restartPolicy: Always volumes: - hostPath: path: / type: &quot;&quot; name: rootfs - hostPath: path: /var/run type: &quot;&quot; name: var-run - hostPath: path: /sys type: &quot;&quot; name: sys - hostPath: path: /DATA/docker type: &quot;&quot; name: docker - hostPath: path: /dev/disk name: disk 在这个yaml中，我们队cAdvisor容器，限制为1核2G内存。我们通过 1kubectl apply -f cadvisor-pod.yaml 将pod运行起来。 查看容器的运行时限制运行为容器后，查看此pod所在节点，进入到节点，找到这个容器，通过下面指令查看此容器的运行时配置 1docker inspect 6f9ecad83132 然后，从一大堆输出中，找到下面的重点部分： 1234567891011121314151617181920212223242526272829&quot;Isolation&quot;: &quot;&quot;,&quot;CpuShares&quot;: 102,&quot;Memory&quot;: 2097152000,&quot;NanoCpus&quot;: 0,&quot;CgroupParent&quot;: &quot;/kubepods/burstable/pod1fb8b728-dbed-11e8-a89e-801844e1f8c8&quot;,&quot;BlkioWeight&quot;: 0,&quot;BlkioWeightDevice&quot;: null,&quot;BlkioDeviceReadBps&quot;: null,&quot;BlkioDeviceWriteBps&quot;: null,&quot;BlkioDeviceReadIOps&quot;: null,&quot;BlkioDeviceWriteIOps&quot;: null,&quot;CpuPeriod&quot;: 100000,&quot;CpuQuota&quot;: 100000,&quot;CpuRealtimePeriod&quot;: 0,&quot;CpuRealtimeRuntime&quot;: 0,&quot;CpusetCpus&quot;: &quot;&quot;,&quot;CpusetMems&quot;: &quot;&quot;,&quot;Devices&quot;: [],&quot;DeviceCgroupRules&quot;: null,&quot;DiskQuota&quot;: 0,&quot;KernelMemory&quot;: 0,&quot;MemoryReservation&quot;: 0,&quot;MemorySwap&quot;: 4194304000,&quot;MemorySwappiness&quot;: -1,&quot;OomKillDisable&quot;: false,&quot;PidsLimit&quot;: 0,&quot;Ulimits&quot;: null,&quot;CpuCount&quot;: 0,&quot;CpuPercent&quot;: 0, 其中： Memory：限制内存资源，单位为byte，2097152000 = 2G CpuShares：CPU使用的相对权重，一个核为1024，我们设置的request cpu为 0.1 ，所以就是 102 CpuPeriod：一个CPU为100000，也就是100个milicpu，这个一般不需要改。 CpuQuota：CPU资源的绝对限制，一般和CpuPeriod结合在一起，CpuQuota/CpuPeriod，就是能够使用的核数，100000/100000=1，表示我们能最多使用1个CPU核心。 CpusetCpus：这个值表示当前容器运行时，绑定到哪几个CPU编号上，注意：这个不是CPU个数，而是绑定到哪几个CPU上，多个CPU编号用逗号分割。 从上面的docker运行时限制看，和Kubernetes的Pod的定义完全吻合。下面再看Cgroups的限制，这才是核心。 根据容器，查Cgroups的限制内容首先，我们看一下pod的名称： 12[root@nodexx test]# kubectl get pod -n test|grep cadvisorcadvisor-test 1/1 Running 0 14h 然后，在pod所在的宿主机，找到这个pod对应的容器Id 123456docker ps|grep cadvisor//找到结果[root@nodexx docker]# docker ps|grep cadvisor 6f9ecad83132 google/cadvisor &quot;/usr/bin/cadvisor...&quot; 15 hours ago Up 15 hours k8s_cadvisor-test_cadvisor-test_test_1fb8b728-dbed-11e8-a89e-801844e1f8c8_0139259f9a21c gcr.io/google_containers/pause-amd64:3.0 &quot;/pause&quot; 15 hours ago Up 15 hours 我们可以注意到，一个匹配出来2个容器，一个是 cadvisor 容器，一个是pause容器，pause容器，是 Kubernetes pod 的基础容器。我们只需要 cadivsor容器的Id：6f9ecad83132，我们要通过它拿到这个容器的Cgroup信息 1234# docker inspect 6f9ecad83132|grep Cgroup&quot;Cgroup&quot;: &quot;&quot;,&quot;CgroupParent&quot;: &quot;/kubepods/burstable/pod1fb8b728-dbed-11e8-a89e-801844e1f8c8&quot;,&quot;DeviceCgroupRules&quot;: null, 好了，完事具备，我们直接进入Cgroup配置目录： 1cd /sys/fs/cgroup/cpu/kubepods/burstable/pod1fb8b728-dbed-11e8-a89e-801844e1f8c8 注意一下这个目录，是 /sys/fs/cgroup/cpu 与 /kubepods/burstable/pod1fb8b728-dbed-11e8-a89e-801844e1f8c8 拼接起来的一个整体路径。在这个目录下，有很多文件： 1234567891011121314151617181920212223ll //total 0drwxr-xr-x 2 root root 0 Oct 30 18:14 139259f9a21cc26fb8b40b85b345566489fc3bedc1b36766b032abc60b93e702drwxr-xr-x 2 root root 0 Oct 30 18:14 6f9ecad83132b77c7e0ce3bfe8e56b471c8c41b920970b3fdd6da4d158fb4b1e-rw-r--r-- 1 root root 0 Oct 30 18:14 cgroup.clone_children-rw-r--r-- 1 root root 0 Oct 30 18:14 cgroup.procs-r--r--r-- 1 root root 0 Oct 30 18:14 cpuacct.stat-rw-r--r-- 1 root root 0 Oct 30 18:14 cpuacct.usage-r--r--r-- 1 root root 0 Oct 30 18:14 cpuacct.usage_all-r--r--r-- 1 root root 0 Oct 30 18:14 cpuacct.usage_percpu-r--r--r-- 1 root root 0 Oct 30 18:14 cpuacct.usage_percpu_sys-r--r--r-- 1 root root 0 Oct 30 18:14 cpuacct.usage_percpu_user-r--r--r-- 1 root root 0 Oct 30 18:14 cpuacct.usage_sys-r--r--r-- 1 root root 0 Oct 30 18:14 cpuacct.usage_user-rw-r--r-- 1 root root 0 Oct 30 10:40 cpu.cfs_period_us-rw-r--r-- 1 root root 0 Oct 30 10:40 cpu.cfs_quota_us-rw-r--r-- 1 root root 0 Oct 30 18:14 cpu.rt_period_us-rw-r--r-- 1 root root 0 Oct 30 18:14 cpu.rt_runtime_us-rw-r--r-- 1 root root 0 Oct 30 10:40 cpu.shares-r--r--r-- 1 root root 0 Oct 30 18:14 cpu.stat-rw-r--r-- 1 root root 0 Oct 30 18:14 notify_on_release-rw-r--r-- 1 root root 0 Oct 30 18:14 tasks 其中，当前目录下有很多Cgroup内容，而有2个子目录： 12139259f9a21cc26fb8b40b85b345566489fc3bedc1b36766b032abc60b93e7026f9ecad83132b77c7e0ce3bfe8e56b471c8c41b920970b3fdd6da4d158fb4b1e 这2个目录，其实就是 pod 中的2个容器（pause容器，cadvisor-test容器），我们进入 cadvisor-test容器的目录下 1cd 6f9ecad83132b77c7e0ce3bfe8e56b471c8c41b920970b3fdd6da4d158fb4b1e 为什么可以判定2个目录中，6f9ecad83132b77c7e0ce3bfe8e56b471c8c41b920970b3fdd6da4d158fb4b1e就是cadvisor-test容器目录呢？因为容器的ID，就是6f9ecad83132呀！ 好了，查看下目录下的内容： 123456789101112131415161718-rw-r--r-- 1 root root 0 Oct 30 18:15 cgroup.clone_children-rw-r--r-- 1 root root 0 Oct 30 10:40 cgroup.procs-r--r--r-- 1 root root 0 Oct 30 18:15 cpuacct.stat-rw-r--r-- 1 root root 0 Oct 30 18:15 cpuacct.usage-r--r--r-- 1 root root 0 Oct 30 18:15 cpuacct.usage_all-r--r--r-- 1 root root 0 Oct 30 18:15 cpuacct.usage_percpu-r--r--r-- 1 root root 0 Oct 30 18:15 cpuacct.usage_percpu_sys-r--r--r-- 1 root root 0 Oct 30 18:15 cpuacct.usage_percpu_user-r--r--r-- 1 root root 0 Oct 30 18:15 cpuacct.usage_sys-r--r--r-- 1 root root 0 Oct 30 18:15 cpuacct.usage_user-rw-r--r-- 1 root root 0 Oct 30 10:40 cpu.cfs_period_us-rw-r--r-- 1 root root 0 Oct 30 10:40 cpu.cfs_quota_us-rw-r--r-- 1 root root 0 Oct 30 18:15 cpu.rt_period_us-rw-r--r-- 1 root root 0 Oct 30 18:15 cpu.rt_runtime_us-rw-r--r-- 1 root root 0 Oct 30 10:40 cpu.shares-r--r--r-- 1 root root 0 Oct 30 18:15 cpu.stat-rw-r--r-- 1 root root 0 Oct 30 18:15 notify_on_release-rw-r--r-- 1 root root 0 Oct 30 18:15 tasks 其中，能够看到好几个熟悉的词，比如 cpu.cfs_quota_us，这个正是对CPU资源做限制的。我们查看一下其内容： 12[root@nodexxx # cat cpu.cfs_quota_us100000 没错，这个值正是100000，也就是1个CPU。 Linux Cgroup 是如何与Dock而关联的？上面的方式，已经层层找到了对CPU、内存等限制，是如何通过Kubernets的Deployment，一步步追查到Cgroup的。那么，Linux Cgroup，怎么与容器关联起来的呢？ 我们看一个cgroup目录中的tasks 12[root@nodexxx]# cat tasks 21694 这个值，就是进程ID，所以，Cgroup对资源的限制，就是对进程ID来限制的。我们看一下这个进程ID 12[root@nodexxx]# ps -ef|grep 21694root 21694 21663 7 Oct30 ? 01:05:05 /usr/bin/cadvisor -logtostderr -allow_dynamic_housekeeping=true -global_housekeeping_interval=1m0s -housekeeping_interval=5s -disable_metrics=udp,tcp,percpu,sched,disk,network -storage_duration=15s -profiling=true -enable_load_reader=true -port=30008 -max_procs=1 此进程ID，正是cadvisor的进程ID，其实这个进程，是容器内的进程，换句话说，其父进程，肯定是一个容器进程： 123[root@nodexxx]# ps -ef|grep 21663root 21663 2567 0 Oct30 ? 00:00:00 docker-containerd-shim 6f9ecad83132b77c7e0ce3bfe8e56b471c8c41b920970b3fdd6da4d158fb4b1e /var/run/docker/libcontainerd/6f9ecad83132b77c7e0ce3bfe8e56b471c8c41b920970b3fdd6da4d158fb4b1e docker-runcroot 21694 21663 7 Oct30 ? 01:05:16 /usr/bin/cadvisor -logtostderr -allow_dynamic_housekeeping=true -global_housekeeping_interval=1m0s -housekeeping_interval=5s -disable_metrics=udp,tcp,percpu,sched,disk,network -storage_duration=15s -profiling=true -enable_load_reader=true -port=30008 -max_procs=1 总结 Kubernete对资源的限制，靠的是Docker，Docker对资源的限制，靠的是Linux Cgroup 。 Linux Cgroup 限制资源，是限制进程，只需要在Cgroup配置目录的tasks文件中，添加进程ID，限制立即生效。 Linux Cgroup 不仅仅可以限制CPU，内存，还可以限制磁盘IO等。]]></content>
      <categories>
        <category>docker</category>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker ENTRYPOINT/FROM 对 docker stop 的影响]]></title>
    <url>%2F2018%2F10%2F28%2F6%2F</url>
    <content type="text"><![CDATA[背景之前有同事和我提过，在 Kubernetes 中，删除一个“应用”，有些应用删除比较慢。我们的“应用”，可以理解为一个 Deployment，删除应用，就是删除 Deployment，然后等待 Pod 全部退出。 当时其实没有太在意这个事情，因为 Kubernetes 的删除，从最细的粒度来看，就是删除 Pod，而删除 Pod，其实本质上就是停止容器，停止容器，本身其实会执行的是 docker stop 的过程，超时后，执行 docker kill 逻辑。 docker stop 是 docker daemon 进程，向 docker 容器进程，发送 kill -USR2 信号，而 docker kill，其实是一个 Kill -9 信号。换句话说，先让容器自行退出，一定时间内没有成功，再强制杀死。 当时觉得，有些容器停止慢，应该是容器业务比较繁重造成的，进程自行退出花费时间比较多而已。但今天看偶尔看 ENTRYPOINT 的的东西的时候，发现，也许这个问题，并没有那么简单。 ENTRYPOINT 的用法说明我们知道，在构建镜像的时候，可以指定程序入口。可以设置 ENTRYPOINT 和 CMD。这2个可以配合使用，也可以完全独立使用。我们本次，不关心CMD，只讨论 ENTRYPOINT 的不同写法，对容器的影响。 Dockerfile 中 ENTRYPOINT 的2个用法12345// 用法1ENTRYPOINT [&quot;your executable program&quot;, &quot;param1&quot;, &quot;param2&quot; ...]// 用法2ENTRYPOINT command param1 param2 ... 第一种写法，你可以自行定义某个二进制程序以及参数，作为容器的1号进程的相关启动内容。这种写法，也就是数组写法。 第二种写法，会将所设定的程序，限定在 /bin/sh -c 下执行。换句话说，这种方式，容器内会有2个进程，一个是 /bin/sh 进程，一个是真正的你的二进制程序的进程，它的进程ID，不是1。 注意1：Docker 官方，推荐第一种写法。 注意2：第二种写法，限定你的进程在 /bin/sh 下，是很多文章提到的，但这个说法，其实并不准确，后边我会测试说明。现在，我们先默认这句话是正确的。 需要特别说明的是： 如果你的进程不是1号进程，/bin/sh 是1号进程的话，会存在一个问题：/bin/sh 进程，不会处理 Linux 信号。这就导致，用第二种 ENTRYPOINT 的写法，就可能出现 docker stop 无法正常停止容器（当时等待 docker stop 超时后，docker daemon还是会发送 kill 信号的，这个可以保证容器停止并退出）。 好了，我们要用第二种写法，测试 docker stop 无法正常停止容器这个过程。 在开始之前，我们明确几个事情 我们要测试 Dockerfile 中 ENTRYPOINT 写法不同，对容器中进程的影响 我们的可执行程序，就用 top 我们要看这个影响，是否会间接影响到 docker stop 最后，我们看一下，如果基础镜像不同，是否测试结果也会不同，我们先用 ubuntu:latest 做基础镜像测试。最后再用 centos:7.5.1804 作为基础镜像测试。 开始测试使用 ubuntu 作为基础镜像做测试Dockerfile 内容如下: 12FROM ubuntu:latestENTRYPOINT top -b 执行下面命令，创建测试镜像，并运行为容器： 123docker build -t test-centos2 -f Dockerfile .$ docker run -it test-centos2 可以直接看到如下结果: 123456789top - 11:40:05 up 1 day, 19:34, 0 users, load average: 0.08, 0.05, 0.01Tasks: 2 total, 1 running, 1 sleeping, 0 stopped, 0 zombie%Cpu(s): 0.2 us, 0.3 sy, 0.0 ni, 99.6 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 stKiB Mem : 2046932 total, 333164 free, 234772 used, 1478996 buff/cacheKiB Swap: 1048572 total, 1048080 free, 492 used. 1607492 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 4500 696 628 S 0.0 0.0 0:00.17 sh 6 root 20 0 36528 3004 2644 R 0.0 0.1 0:00.01 top 这个结果说明： 容器内确实起了2个进程 1号进程就是 /bin/sh 我们的可执行程序，不是1号进程，而是 /bin/sh 进程的子进程，进程ID为6 那么，我们停止此容器。停止之前，先说一下，docker stop 会给容器发送SIG信号，让进程自行退出，如果进程不处理此信号，docker stop 会超时，然后 发送 kill 信号。默认超时时间是 10s ，我们执行如下命令： 1234time docker stop -t 30 e58dc2887ef8//输出：e58dc2887ef8docker stop -t 30 e58dc2887ef8 0.39s user 0.09s system 1% cpu 31.515 total 这个输出表明，docker stop 真的等了 30s 后才执行成功，也就是，/bin/sh 确实没有处理 SIG 信号。最后被 kill 掉了。 初期结论和说明我们要尽量避免上面的现象，就要保证容器的1号进程，是应用的真正可执行程序，不能是 /bin/sh 进程，否则，Kubernetes 删除 pod，就会等待一段时间才能执行成功。另外，我们还是应该尽量使用官方推荐的 ENTRYPOINT 写法（数组写法） 使用centos作为基础镜像测试但是，问题到此并没有结束。我们之前的 Dockerfile 是使用 ubuntu 作为基础镜像的。我们尝试，换为 centos 作为基础镜像试一下 12FROM centos:7.5.1804ENTRYPOINT top -b 然后，我们生成新镜像，并运行为容器： 123456789$ docker run -it test-centos3top - 16:11:26 up 1 day, 20:29, 0 users, load average: 0.00, 0.00, 0.00Tasks: 1 total, 1 running, 0 sleeping, 0 stopped, 0 zombie%Cpu(s): 33.3 us, 33.3 sy, 0.0 ni, 33.3 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 stKiB Mem : 2046932 total, 328508 free, 237228 used, 1481196 buff/cacheKiB Swap: 1048572 total, 1048080 free, 492 used. 1604860 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 56032 3668 3236 R 0.0 0.2 0:00.05 top 我们能看到，只有1个进程，且进程ID为1。我们直接进入到容器内部看一下： docker exec -it e8fa215e3ad6link123456[root@e8fa215e3ad6 /]# ps -efUID PID PPID C STIME TTY TIME CMDroot 1 0 0 16:11 pts/0 00:00:00 top -broot 6 0 3 16:12 pts/1 00:00:00 /bin/bashroot 20 6 0 16:12 pts/1 00:00:00 ps -ef[root@e8fa215e3ad6 /]# 从上，我们确实只能看到 top 进程为1号进程，并没有 /bin/sh 进程。而这个 Dockerfile 和之前的 Dockerfile，唯一的区别就是基础镜像不同。 结论 /bin/sh 进程是无法处理 Linux 信号的。 不论使用哪种镜像做应用镜像的基础镜像，都要注意构建完应用镜像后测试一下，最好不要让 /bin/sh 成为 1 号进程。 尽量在 Dockerfile 中，为 ENTRYPOINT、CMD、RUN 使用数组方式写法。]]></content>
      <categories>
        <category>docker</category>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cAdvisor内存占用不断飙升导致其在k8s内不断crash问题排查]]></title>
    <url>%2F2018%2F10%2F24%2F5%2F</url>
    <content type="text"><![CDATA[背景我们的额监控方案为：Kubernetes（K8S）+cAdvisor+Prometheus+Grafana。然后，用cAdivor监控容器信息，其实，cAdivor其实到现在的主流K8S版本中，Kubelet进程已经将其内置了，但是我们没有这么用，因为没有必要因为让Prometheus定期去Kubelet上采集容器信息，平白增添对Kubelet的压力。相反，我觉得，还是应该还是应该单独部署cAdvisor，这样一来，不论是定制化cAdvisor，还是版本更新，都会更方面。所以，我使用DaemonSet部署了cAdvisor。 问题用DaemonSet的方式部署cAdvisor，本质上，就是每个K8S的宿主机都启动了一个pod，实际观测，发现这些Pod的状态，会随着时间的推移，开始频繁出现Crash。这个问题，势必会导致cAdvisor无法正常监控容器信息。下面是具体的排查过程。 排查初探首先，Pod Crash 必然有其原因，所以，一开始是通过下面的方式，看cAdvisor到底为何会Crash，通过1kubectl describe pod -n monitoring pod-xxxxx 找到Last State部分，发现其为：1State: OOMKilled 这说明，这个 Pod，是因为内存不够，cAdvisor在运行过程，超出了Pod的资源限制，被OOM杀掉了。既然资源不够，那么首先，就是调大其内存限制。 一开始为这个Pod设置的上限资源为1核CPU+1G内存，既然内存无法满足，那么调大为2G，继续观测，发现依然会OOM。然后又调整为3G、4G、10G、20G（机器内存大，土豪），发现虽然内存变大了会有一些缓解，但实际上，即使内存上限设置为20G，还是会有Crash的情况，那么，这时候就需要反思一下几个问题了： 是否是cAdvisor存在bug？ 哪个机器上的cAdvisor Pod总是重启？ 排查是否是cAdvisor版本问题针对第一点，我们升级了cAdivor镜像为最新版，问题依旧。 排查是否是cAdvisor参数配置问题google一些文章，有人提过类似的问题，官方issue的解释中，有人提到可能配置不对，可能采集的指标过多等，于是，我review了一下我的配置，调整后的完整配置如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697apiVersion: extensions/v1beta1kind: DaemonSetmetadata: labels: name: cadvisor name: cadvisor namespace: monitoringspec: revisionHistoryLimit: 10 selector: matchLabels: name: cadvisor template: metadata: annotations: prometheus.io/port: &quot;28762&quot; prometheus.io/scrape: &quot;true&quot; creationTimestamp: null labels: name: cadvisor spec: automountServiceAccountToken: false containers: - args: - -allow_dynamic_housekeeping=true - -global_housekeeping_interval=1m0s - -housekeeping_interval=3s - -disable_metrics=udp,tcp,percpu,sched - -storage_duration=15s - -profiling=true - -port=28762 - -max_procs=1 image: mine/cadvisor-test:v0.0.2 imagePullPolicy: IfNotPresent name: cadvisor ports: - containerPort: 28762 hostPort: 28762 name: http protocol: TCP resources: limits: cpu: &quot;1&quot; memory: 3000Mi requests: cpu: &quot;1&quot; memory: 500Mi terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /rootfs name: rootfs readOnly: true - mountPath: /var/run name: var-run readOnly: true - mountPath: /sys name: sys readOnly: true - mountPath: /var/lib/docker name: docker readOnly: true - mountPath: /dev/disk name: disk readOnly: true dnsPolicy: ClusterFirst hostNetwork: true restartPolicy: Always schedulerName: default-scheduler securityContext: &#123;&#125; terminationGracePeriodSeconds: 30 volumes: - hostPath: path: / type: &quot;&quot; name: rootfs - hostPath: path: /var/run type: &quot;&quot; name: var-run - hostPath: path: /sys type: &quot;&quot; name: sys - hostPath: path: /DATA/docker type: &quot;&quot; name: docker - hostPath: path: /dev/disk type: &quot;&quot; name: disk templateGeneration: 6 updateStrategy: rollingUpdate: maxUnavailable: 1 type: RollingUpdate 我调整的部分主要集中在： 12345678// 这个是禁用哪些指标，默认只有udp、tcp- -disable_metrics=udp,tcp,percpu,sched// 存储最近多久的数据，原来是1分多钟，调整为15s- -storage_duration=15s// 是否开启性能测试，默认为关闭，之所以开启，是要一会儿debug内存占用- -profiling=true// 使用多少CPU，默认不到1个- -max_procs=1 上面的方式，是减少了一些采集指标，以及采集数据的最多保留时长，稍微有些效果，但是发现效果不大，原来某些机器上频繁Crash的cAdvisor Pod，还是Crash，另外某些机器上从来不Crash的，也不会Crash。那么，说明参数配置没什么用，问题应该出现某些机器上。 排查为何cAdivosr Pod在某些机器上Crash我回顾了一下我们的K8S节点，发现cAdvisor Pod不OOM的机器上面，容器都比较少。越是容器多的机器，这机器上的cAdvisor Pod就越容易OOM Crash。 那么，我们看一下 cAdvisor 的 Pod 日志，发现其频繁报一个错误： 1fsHandler.go:135] du and find on following dirs took 57.562700809s: [/rootfs/DATA/docker/overlay2/d8c002c4dc33c22129124e70bf7ca15fd312cd8867c040708d11d7d462ee58df/diff /rootfs/DATA/docker/containers/16eb9120ce2da24d867ee287c093ce7221f1d3ed39e69c3a8d128313a5dc0d63]; will not log again for this container unless duration exceeds 4s 这说明，cAdvisor会统计每一个容器占用的磁盘使用大小，这个大小是通过du命令来处理的，而且，这个统计耗费的时间很长。我们可以实际去看一下，发现这个目录，确实比较大，有些在2-3G。这说明，这个机器上，必然存在一些容器，里边在搞事情，写了很多的文件，导致 du 命令在统计的时候，比较耗时。 问题初步总结K8S节点，有些容器存储或写入了比较多的文件，造成cAdvisor统计容器磁盘使用耗时，进而引发此cAdivosr内存占用升高。 排查深入探究既然上面已经初步定为问题，但是我们依然会疑惑，为什么cAdivosr统计容器磁盘耗时会引发内存飙升呢？ 我们需要借助一些工具来进一步排查 通过 go tool pprof 分析内存 通过查看 cAdvisor 源码分析流程 在源码中，打断点，验证猜想 通过 go tool pprof 分析内存首先，将 DaemonSet 启动的 cAdvisor，使用 Host 模式启动，这样我们就可以直接通过访问宿主机上，cAdvisor开放的端口，来做性能采样了。 1go tool pprof -cum -svg -alloc_space http://x.x.x.x:28762/debug/pprof/heap 上面的步骤，会生成内存性能采样图，类似如下： 详细采样图，可以通过此连接查看： 采样图全 从图中，先看红色部分，颜色越深，表示这部分资源消耗越严重，我们这个采样图是采集的内存，可以看到，有 2366.70M，是 Gather 函数的，但其实，这个函数本身，并没有多少内存消耗，它的内存占用这么大，是 collectContainersInfo 函数分配的。其实不论怎样，Gather函数都脱离不了干系。那么，我们从源码看一下 源码分析首先，入口函数main中，注册了/metrics对应的handler，因为cAdvisor要开发 /metirics路径，让 Prometheus 来定时采集 1234567891011// cadvisor.go#82func main() &#123; defer glog.Flush() flag.Parse() //注册HTTP路径 *prometheusEndpoint 值就是 /metirics cadvisorhttp.RegisterPrometheusHandler(mux, containerManager, *prometheusEndpoint, containerLabelFunc, includedMetrics) glog.V(1).Infof(&quot;Starting cAdvisor version: %s-%s on port %d&quot;, version.Info[&quot;version&quot;], version.Info[&quot;revision&quot;], *argPort) addr := fmt.Sprintf(&quot;%s:%d&quot;, *argIp, *argPort) glog.Fatal(http.ListenAndServe(addr, mux))&#125; 然后，看一下，是谁在处理 /metrics 路由对应的操作 123456789101112// 代码文件：http/handler.go#97func RegisterPrometheusHandler(mux httpmux.Mux, containerManager manager.Manager, prometheusEndpoint string, f metrics.ContainerLabelsFunc, includedMetrics container.MetricSet) &#123; r := prometheus.NewRegistry() r.MustRegister( metrics.NewPrometheusCollector(containerManager, f, includedMetrics), prometheus.NewGoCollector(), prometheus.NewProcessCollector(os.Getpid(), &quot;&quot;), ) //可以看到，真正执行 /metrics 的函数，是 promhttp.HandlerFor mux.Handle(prometheusEndpoint, promhttp.HandlerFor(r, promhttp.HandlerOpts&#123;ErrorHandling: promhttp.ContinueOnError&#125;))&#125; 可以看到，真正执行 /metrics 的函数，是promhttp.HandlerFor，具体深入HandlerFor看一下 12345678// 代码文件：vendor/github.com/prometheus/client_golang/prometheus/promhttp/http.go#82func HandlerFor(reg prometheus.Gatherer, opts HandlerOpts) http.Handler &#123; return http.HandlerFunc(func(w http.ResponseWriter, req *http.Request) &#123; //这里就是真正的 Gather 调用 mfs, err := reg.Gather() ... &#125;)&#125; 至此，可以说明，每一次HTTP调用（调用 x.x.x.x:8080/metrics），都会又一次Gather调用。 所以我们猜想，之所以Gather函数有这么大的内存占用，主要是因为Gather函数调用次数多，而每次Gather函数执行之间长，导致形成了并发调用，这种情况下，Gather函数从执行到结束期间，都不会释放内存，并发调用，就会导致内存积压。 修改源码，重新构建部署，验证猜想那么，我们在Gather调用处，打断点，看一下执行时间： 123456789101112// 代码文件：vendor/github.com/prometheus/client_golang/prometheus/promhttp/http.go#82func HandlerFor(reg prometheus.Gatherer, opts HandlerOpts) http.Handler &#123; return http.HandlerFunc(func(w http.ResponseWriter, req *http.Request) &#123; pp.Println(&quot;请求开始————————&quot;) start:=time.Now() //这里就是真正的 Gather 调用 mfs, err := reg.Gather() ... timeCost := time.Since(start) pp.Println(fmt.Sprintf(&quot;请求结束，耗时 %v&quot;, timeCost)) &#125;)&#125; 我们打印了Gather执行的耗时，然后重新构建 cAdvisor源码，打一个私有镜像出来，推送到私有镜像仓库。然后我们使用这个测试镜像，重新部署cAdvisor。 现在，我们挑一台之前cAdvisor频发OOM Crash的机器，看一下它的log 1kubectl logs -n monitoring cadvisor-k9kpt -f 日志输出大致如下： 123456&quot;请求开始————————&quot;I1023 14:21:19.126794 1 fsHandler.go:135] du and find on following dirs took 15.420205027s: [/rootfs/var/lib/docker/overlay2/67ec1868b2c0ed5ce5b22ee014eb4d08993accd68546a3de6aa2a6355bdc1a78/diff /rootfs/var/lib/docker/containers/cd910753386b3325af8bd5a69fc01b261ca14c1bfaf754677662e903b755d34f]; will not log again for this container unless duration exceeds 56sI1023 14:21:19.305938 1 fsHandler.go:135] du and find on following dirs took 15.278733582s: [/rootfs/var/lib/docker/overlay2/10621b60f26962cb1a90d7a7dc1ce4e3c8a15f6e4e30861b8433c5c37727bb9e/diff /rootfs/var/lib/docker/containers/b2a4d11c37aa9c63b4759c5728956253fad46fa174c7fe4d91336a4ac7532127]; will not log again for this container unless duration exceeds 1m34sI1023 14:21:19.827757 1 fsHandler.go:135] du and find on following dirs took 13.897447077s: [/rootfs/var/lib/docker/overlay2/29b3b0dfc22053937e9c40e004a6d31af489573ff3a385020feb22d88d1a3d0a/diff /rootfs/var/lib/docker/containers/af962971a0643418d28c03b374e31a0c58dd6302524ea06dc8a23c4eccf5d663]; will not log again for this container unless duration exceeds 1m20sI1023 14:21:20.042949 1 fsHandler.go:135] du and find on following dirs took 14.514122984s: [/rootfs/var/lib/docker/overlay2/27f1d3cb3d421567754cb7abb986c16c3f3bec0874e983a2604aa7eda8834d9a/diff /rootfs/var/lib/docker/containers/60cad8688e31b557e2e98c47beaa1f3af2ea2e6cbfab0c1f399887b3eecec86c]; will not log again for this container unless duration exceeds 1m56s&quot;请求结束，耗时 58.093771464s&quot; 日志其实我只是截图了一部分，基本上可以看出来，Gather请求十分耗时，这个耗时，就是由 du 操作耗时造成的，有时候，du 耗时非常严重，能将近2分钟。 这样，基本上，就印证了，Gather函数处理慢，而Prometheus每隔3s请求一次，造成同时有非常多的 Gather函数在并发处理，也就导致了内存积压的情况。 彻底解决综上，其实我们只需要让 du 磁盘统计快了就可以了，du 的快慢，是一个CPU密集和磁盘IO密集的操作，要加快 du 操作，就需要给到足够的运算能力。 回顾之前我们的 cAdvisor 的 DaemonSet 的yaml配置，我们在资源的 limit 部分，仅给到了一个 CPU，我们加 CPU 核数增加到6。如下： 1234567resources: limits: cpu: &quot;6&quot; memory: 3000Mi requests: cpu: &quot;2&quot; memory: 500Mi 然后，更新 DaemonSet 部署 1kubectl apply -f cadvisor.ds.yaml 再次去观察 cAdvisor 的pod日志，发现du耗时明显缩短到2秒钟以内，pod内存占用过高的情况，再也没有出现过。问题得解！]]></content>
      <categories>
        <category>kubernetes</category>
        <category>cAdvisor</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>kubernetes</tag>
        <tag>cAdvisor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何解除github repo的 froked from xxx/yyy 提示]]></title>
    <url>%2F2018%2F10%2F16%2F4%2F</url>
    <content type="text"><![CDATA[场景在某些情况下，我们有些需要克隆某个人的repo。 可能需要然后进行修改，提Pull Request进行代码合并，或者克隆repo后作为我们自己的项目做二次定制。 然而，一旦从其他人的repo上克隆为我们自己的repo，则项目名称下会一直提示 forked from xxx/yyy。有时候，出于一些原因，我们需要彻底解除我们的repo与原来repo之间的绑定关系。 我自己的场景，是博客repo从其他人的repo做了克隆，后期我完全重构了这个博客的主题选择等内容，基本和原来的那个远程repo完全不相干了，这时候，博客这个repo总是看到 forked from xxx/yyy 感觉有些扎眼。 解除方式方式1最笨拙的解除方式，就是将我们的远程repo删除，重新创建repo提交我们的代码文件。这种方式操作暴力，而且有些麻烦。我们之所以说它麻烦，是因为有时候，我们已经对这个repo做了一些自己的设置了，如果把repo删除重建并提交代码，还得再重新给这个repo做设置，比较繁琐。 方式2除了上面的方式，我们还可以把解除操作的步骤，交给github官方来做。具体来说，就是提交一个“工单”。地址：https://github.com/contact进入地址后，向下拉动，找到工单填写的地方。其中：Name：填写你的用户名Email：选择你的邮箱 Subject：这是工单标题，可以这样写：detach my repo aaa/bbb from the repo xxx/yyy How can we help?这部分需要你填写详细的求助说明，可以这样写：Please detach my repo aaa/bbb from the repo xxx/yyy, Thanks for helping! 填写完之后，直接点击 send request 按钮提交，等待github官方处理即可，一般情况下，半天时间内官方就会给予处理。]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s线上集群的机器，重启iptables后，造成CI系统（drone）构建偶发失败，docker run容器无法访问外网问题排查记录]]></title>
    <url>%2F2018%2F10%2F16%2F3%2F</url>
    <content type="text"><![CDATA[引子线上的k8s集群内部，每一台机器，都有2块网卡，一块儿em1（10网段），一个 em2（192网段）。然后，线上的机器，通过iptables限制为：访问192段的服务任意端口都是畅通的，而访问10段的服务端口，只有333端口可以连通。333端口，是线上机器的ssh端口。 线上集群的k8s，使用的calico网络。 线上有一个问题，就是容器访问当前宿主机192段IP的999端口不通，访问其他机器192段IP的9999端口畅通。 我们知道，当从容器访问宿主机的应用时，会重新进入iptables的INPUT链，而线上的iptables INPUT链如下： 1234567891011Chain INPUT (policy ACCEPT 0 packets, 0 bytes)num pkts bytes target prot opt in out source destination 1 1477K 2519M cali-INPUT all -- * * 0.0.0.0/0 0.0.0.0/0 /* cali:Cz_u1IQiXIMmKD4c */2 272K 20M KUBE-SERVICES all -- * * 0.0.0.0/0 0.0.0.0/0 /* kubernetes service portals */3 273K 20M KUBE-FIREWALL all -- * * 0.0.0.0/0 0.0.0.0/0 4 263K 20M ACCEPT all -- * * 0.0.0.0/0 0.0.0.0/0 state RELATED,ESTABLISHED5 0 0 ACCEPT icmp -- * * 0.0.0.0/0 0.0.0.0/0 6 393 17292 ACCEPT all -- lo * 0.0.0.0/0 0.0.0.0/0 7 11880 1019K ACCEPT all -- em2 * 0.0.0.0/0 0.0.0.0/0 8 7 308 ACCEPT tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:3339 228 62052 REJECT all -- * * 0.0.0.0/0 0.0.0.0/0 reject-with icmp-host-prohibited 意思是： ①：如果报文是途径em2网卡，访问任意其他网卡。则全部放行。（这就是为什么192段的访问全部放行） ②：如果报文是tcp，不管报文从哪个网卡出来，去往哪个网卡，只要目的端口是333，就放行。（这就是为什么10段的访问，只能访问333） 我们容器发出的报文，网卡是 cal 网卡（calico的网卡名称），所以，容器只能访问333。 错误操作要把9999端口放开，意味着，我要添加开放9999端口到INPUT链路中，为了让iptables再重启后依然生效，我将规则，添加到 1/etc/sysconfig/iptables 在添加完成后其实并未生效，然后，有2种方式，用于让iptables生效： ①：手动添加规则，比如 iptables -t filter -I INPUT 9 -s 172.20.0.0/14 -p tcp -m tcp –dport 9999 -j ACCEPT ②：重启iptables（会自动加载 /etc/sysconfig/iptables内容 ） 我选择了后者，之所以选择后者，其实是因为，即便重启了iptables，加载了刚刚的规则，原来的k8s和calico的iptables会丢失，但是，k8s会重建自己的整套iptables规则的。 事实上，k8s确实也在我重启iptables后，重建了iptables，但问题来了。 引发的2个网络问题1、我们线上的CI系统，是基于drone，做的二次开发，改动了很多的东西，添加了很多特性，结果上面一系列操作之后，我发现，我们的CI系统中，个别镜像的构建（Dockerfile中的RUN操作，有网络相关操作时，比如yum）会失败。 2、线上的机器，执行 docker run，起来的容器，无法访问外网了。然而，k8s起来的pod里的容器，却可以访问外网。 镜像的构建，其实本质上，也是起容器来做事情，而且，你无法定制docker build操作时，使用的网络模式，只能是bridge模式。而 docker run 起来的容器，默认也是 bridge 模式。结合上面2个内容，基本上可以确定，此问题，和k8s无关，而是纯粹的docker的问题了。 排查①：一开始以为是k8s的iptables重建不完整，所以，多次用线上的机器的机器 nodeA（无人访问的一台），不断尝试清空iptables，等待k8s重建iptables，多次尝试无效。 ②：stop docker -> stop kubelet -> clean iptables -> start docker -> start kubelet。这个操作的意义是，以为重启docker，重启k8s，docker 和 k8s 都会重建自己的 iptables。多次尝试无效。 ③：使用现在的k8s安装工具：kubespray，重新安装 nodeA，并添加到k8s集群内。寄希望于安装工具重装docker和k8s，结果依然无效。 ④：回归问题本身，问题一定出在iptables上。也是因为一开始重启了iptables导致此一系列问题。 解决过程既然问题出在iptables上，而我们要访问外网，那么，就得重iptables 报文转出入手。好在线上的机器，并没有都为了适配9999端口重启iptables，有3台 k8s 的 master 节点，并没有重启 iptables。 然后，对比有问题机器、没有问题机器的 FORWARD、POSTROUTING 的规则。发现，缺少DOCKER相关的规则，这些规则，本来是安装docker之后，由docker创建的。那么，解决此问题，就需要重建docker的iptables。 ①：找一台干净的机器（只安装了docker，未启动容器，为安装k8s），然后从中提取docker相关的规则 ②：将提取的规则，与出问题的机器的iptables默认规则融合。 ③：执行 iptables-restore 恢复规则。 后续验证在问题机器上，docker run centos 容器，然后ping qq.com，链路畅通。有一个小问题，就是如果你先启动的容器执行 ping 外网操作。然后才再目的机器上执行 iptables-restore 的话，会发现，容器的 ping，有一个短暂的耗时，然后才会畅通，这是因为 iptables-restore 恢复规则后，有一个过程。 最后我整理一个相对标准的docker iptables规则备份，如果你也遇到了启动的docker容器，无法访问外网的问题，可以通过下面的内容，恢复规则 将下面的规则，保存到一个文件，比如 docker-iptables-backup 123456789101112131415161718192021222324252627# Generated by iptables-save v1.4.21 on Thu Apr 30 20:48:42 2015*nat:PREROUTING ACCEPT [18:1080]:INPUT ACCEPT [18:1080]:OUTPUT ACCEPT [22:1550]:POSTROUTING ACCEPT [22:1550]:DOCKER - [0:0]-A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER-A OUTPUT ! -d 127.0.0.0/8 -m addrtype --dst-type LOCAL -j DOCKER-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE-A POSTROUTING -s 172.17.0.1/32 -d 172.17.0.1/32 -p tcp -m tcp --dport 80 -j MASQUERADE-A DOCKER ! -i docker0 -p tcp -m tcp --dport 3001 -j DNAT --to-destination 172.17.0.1:80COMMIT# Completed on Thu Apr 30 20:48:42 2015# Generated by iptables-save v1.4.21 on Thu Apr 30 20:48:42 2015*filter:INPUT ACCEPT [495:53218]:FORWARD ACCEPT [0:0]:OUTPUT ACCEPT [480:89217]:DOCKER - [0:0]-A FORWARD -o docker0 -j DOCKER-A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT-A FORWARD -i docker0 ! -o docker0 -j ACCEPT-A FORWARD -i docker0 -o docker0 -j ACCEPT-A DOCKER -d 172.17.0.1/32 ! -i docker0 -o docker0 -p tcp -m tcp --dport 80 -j ACCEPTCOMMIT# Completed on Thu Apr 30 20:48:42 2015 然后，执行恢复 iptables-restore docker-iptables-backup 注意： ①：如果你的docker在k8s集群内部，理论上来说，上面的操作会有清空iptables的能力，但是你不需要担心，因为k8s会自行修复本身的iptables，也就是说，k8s会重建自己的iptables。但是假如你的k8s没能完整重建，就需要手动恢复k8s的iptables。如果是calico网络，只需要删除当前节点的calico pod即可，删除之后，k8s会重启这个pod，自然也就会重建iptables了。同理，flannel网络也是一样的。 ②：上面的规则，有一个关键，是 172.17.0.0 和 172.17.0.1，这是docker默认安装后的网段和docker0网卡的IP。如果你是默认安装的docker，上面的规则不需要改动，但如果你定制了docker0网卡的IP，记得修改一下上面的规则，适配你的环境。]]></content>
      <categories>
        <category>docker</category>
        <category>drone</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>drone</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于XFS文件系统的overlayfs下使用docker，为何要使用d_type=1]]></title>
    <url>%2F2018%2F10%2F16%2F2%2F</url>
    <content type="text"><![CDATA[什么是overlayfs首先，overlayfs是一种文件系统，也是目前dokcer在使用的最新的文件系统，其他的文件系统还有：aufs、device mapper等。而 overlayfs 其实和 aufs 是类似的。更准确的说，overlayfs，其实是 Linux 文件系统的一种上层文件系统。下面的底层的文件系统格式，是支持overlayfs的： ①：ext4 ②：xfs（必须在格式化为xfs的是，指定ftype=1，如果在 未使用ftype=1的方式格式化的xfs文件系统上使用，否则docker可能出现未知问题） 如何查看当前操作是否支持overlay1lsmod |grep over 如果没有输出，表示不支持，可以通过下面的命令开启overlay 1modprobe overlay 需要注意的是： ①：docker官方，建议使用 overlay2，而不是 overlay，因为 overlay2 更高效。要使用 overlay2的话，需要 Linux 内核在版本4以上。 ②：docker 官方建议，在 docker-ee 17.06.02及以上的版本使用 overlay2，以及，在docker-ce的版本，也使用 overlay2。而 overlay 虽然在 docker-ce 版本中是支持的，但是并不推荐。 ③：只要当前操作系统支持overlay，那docker就可以使用overlay或者overlay2了。 ④：指定docker的overlay2驱动，需要在启动docker的时候，指定 –storage-driver 参数，或者，在配置文件 /etc/docker/daemon.json 中 ，指定驱动配置 123456&#123; "storage-driver": "overlay2", "storage-opts": [ "overlay2.override_kernel_check=true" ]&#125; xfs文件系统的 d_type是什么d_type 是 Linux 内核的一个术语，表示 “目录条目类型”，而目录条目，其实是文件系统上目录信息的一个数据结构。d_type，就是这个数据结构的一个字段，这个字段用来表示文件的类型，是文件，还是管道，还是目录还是套接字等。 d_type 从 Linux 2.6 内核开始就已经支持了，只不过虽然 Linux 内核虽然支持，但有些文件系统实现了 d_type，而有些，没有实现，有些是选择性的实现，也就是需要用户自己用额外的参数来决定是否开启d_type的支持。 为什么docker在overlay2（xfs文件系统）需要d_type不论是 overlay，还是 overlay2，它们的底层文件系统都是 overlayfs 文件系统。而 overlayfs 文件系统，就会用到 d_type 这个东西用来文件的操作是被正确的处理了。换句话说，docker只要使用 overlay 或者 overlay2，就等于在用 overlayfs，也就一定会用到 d_type。所以，docker 提供了 1docker info 此命令，用来检测你docker服务，是否在使用overlay的时候正确的使用 d_type。如果用了 overlay/overlay2，但 d_type 没有开，就报警告。 如果在不支持 d_typ 的 overlay/overlay 驱动下使用docker，也就意味着 docker 在操作文件的时候，可能会遇到一些错误，比如 无法删除某些目录或文件，设置文件或目录的权限或用户失败等等。这些都是不可预料的错误。举个具体的场景，就是，docker构建的时候，可能在构建过程中，删除文件等操作失败，导致构建停止。 如何检测当前的文件系统，是否支持 d_type ？ 1xfs_info / 它用于检测指定挂载点的文件xfs文件系统的信息。如果你的文件系统是 xfs，则会提示类似如下信息 12345678910$ xfs_info /meta-data=/dev/sda1 isize=256 agcount=4, agsize=3276736 blks = sectsz=512 attr=2, projid32bit=1 = crc=0 finobt=0 spinodes=0data = bsize=4096 blocks=13106944, imaxpct=25 = sunit=0 swidth=0 blksnaming =version 2 bsize=4096 ascii-ci=0 ftype=1log =internal bsize=4096 blocks=6399, version=2 = sectsz=512 sunit=0 blks, lazy-count=1realtime =none extsz=4096 blocks=0, rtextents=0 注意其中的 ftype，1表示支持 d_type，0表示不支持。 参考： https://linuxer.pro/2017/03/fix-chown-error-discourse-bootstrap/ https://linuxer.pro/2017/03/what-is-d_type-and-why-docker-overlayfs-need-it/ https://blog.csdn.net/zxf_668899/article/details/54667521 https://docs.docker.com/storage/storagedriver/overlayfs-driver/]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>ovlerlayfs</tag>
      </tags>
  </entry>
</search>
