<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[如何打通 K8s 虚拟网络（flannel vxlan 网络）和 K8s 2层网络（macvlan网络）]]></title>
    <url>%2F2020%2F03%2F10%2F20%2F</url>
    <content type="text"><![CDATA[背景谈论问题，先谈背景。 K8s 在虚拟网络下存在很多问题。比如说： Pod IP 无法被外部访问，只能在 K8s 集群内访问。直接从实例上无法做到与物理机实例互联互通。 Java 等依赖外部注册中心的服务，如果不放到同一个 K8s 集群里，服务之间无法互访。 复杂场景下，比如：多机房、集群灾备等，需要多 K8s 集群。原生 K8s 集群下，每个集群网络都是虚拟的，多个虚拟网络要互通。如果是 flannel 的话，可以借助同一个 etcd 来实现多 K8s 集群的 Pod 互通，但是 service ip 可就不行了，依然是个问题。 以应用中心为视角来看，从上层架构来说，底层 instance 具备可访问行，不论是容器实例，物理机，还是虚拟机实例，都应该具备可访问属性。而虚拟网络下的容器实例，违背了这一点。 所以说，K8s 虚拟网络模型，需要变迁到 K8s 2层容器网络模型。改造为2层网络的方式主要处理几个点： 从 CNI 中，选择容器网络2层化方案。 从 CNI 中，选择合适的 IPAM 方案。 K8s 官方的 CNI 中，能比较好的支持容器2层网络方案的，比如：bridge + veth pair、macvlan、ipvlan 等。而 IPAM ，可以说最基本的就是 host-local 方案了。 简单比较一下 bridge 模型和 macvlan 模式 bridge 就是虚拟交换机：网桥。从一些资料来看，如果单纯的说 bridge，其本身性能还是很高的，但是 bridge 下挂 veth pair，整套连起来运作，veth pair 会拖累 bridge 的性能表现。这样一来，就远没有 macvlan 性能高了。我们参考几组数据： 1、macvlan bridge 模式，对 CPU 负载的消耗，bridge 模式占用 CPU 资源最高。 2、对比 macvlan、bridge、OVS 及 Native Host 的性能 注意：CHC 表是的是，同主机上 Container->Host->Container 模式。CHHC 表是的是跨主机：Container->Host->Host->Container 模式。 可以看到，macvlan 模式下，不论是同主机间的容器互访，还是跨主机间的容器互访，其单位时间内的请求/响应量级都很高，几乎接近主机网络（Native Host）。 再来看一下吞吐量（容器互访，横坐标为TCP） 更详细的评测数据，可以参考：performance of container networking.pdf 总的来说，从某个角度看，macvlan 就是要替换 bridge + veth 模式的，就好比，macvtap 就是要替换 bridge + tap 模式的，macvlan 和 macvtap 只是用在场景不同而已。而我们团队，出于性能考虑，目前开发环境、线上环境，CNI 插件都是用 macvlan 做的2层容器网络，IPAM（IP Address Management） 模式，使用的是简单的 host-local（本机分配） 模式（后期要调整）。 是的。这里提到，“我们团队”。其实，在整个公司中，做 K8s 团队的不止我们一个。其他团队的实施，还主要以 flannel 虚拟网络为主。目前来说，2个团队现在已经整合为一个部门的团队，所以，K8s 基础设施也要统一，恰好，其中重要的一环，就是将现在虚拟网络的 K8s，也改造为 L2 层网络的 K8s 。目的主要有3个： 底层设施层面，统一 K8s 集群。不要虚拟和2层都有，增加维护成本。 在上层部署系统层面，由一套统一的部署系统，分别对接底层的 K8s。这就要求底层的 K8s 是统一的。 更上面的各种抽象模型统一，比如，对接服务到应用中心，而应用中心下的 instance 模型，要求 instance 实例都在 2层网络下，这样一来，围绕应用中心展开的：配置中心、发现中心、监控告警，都对接 2 层 instance 实例，就好开展了。 好了，问题来了： 虽然 K8s 集群做成 L2 K8s 不是很复杂的事儿，但是改造完得迁移服务。 迁移的过程，不是强制将原集群的实例，部署到新集群里，这个过程，是用户部署的动作来触发，应该是平滑的，对其业务无感知的。强制从1个集群迁移到另外一个集群，风险太高。 要保证业务平滑，必须会遇到一个现实问题：部分服务在虚拟网络 K8s 中，部分在2层网络 K8s 中。虚拟网络 K8s 集群里的实例访问 K8s L2层网络下的实例还好，不会有问题，但是，反过来，可就访问不通了（原生 K8s 虚拟网络的所有实例的可访问性，是只有集群内部生效的）。 所以，一个问题摆在眼前：K8s 虚拟网络，要和 K8s L2 层网络打通。这样才能保证新部署的服务，部署到 L2 层 K8s 集群里之后，它还可以访问到原来那个 K8s 虚拟网络下的实例。这一点对 Java 这种要通过注册中心，走RPC调用的服务，尤为重要。 注意：本文说的打通，其立场，是 2 个集群打通，而不是一个 L2 层网卡的 K8s 集群，要和多个 虚拟网络 K8s 集群打通。 K8s 虚拟网络（flannel）和 L2 层网络（macvlan）L2 macvlan 网络要做 2 个不同网络集群的打通，macvlan 网络最简单，简单理解为一个网桥就好，实际上，我们的 macvlan 就是用的 bridge 模式。 下面是 K8s 基于 macvlan 网络的一个简单架构图： 这个网络模式有几个核心的点： 容器之间，是可以通过 macvlan 互访的，不需要借助外部交换机。所以，macvlan 模式，用的是 bridge 模式，而不是 vepa 模式，vepa 模式的话，需要交换机开启“发夹”模式，对于我们的场景来说，没有必要在交换机上做这个事情，另外，交换机上还没有统一管理规则的需求。 macvlan 的父接口，是 bond0，bond0 是做了网卡聚合的。可以提高带宽。 宿主机网络上，有一个 macv-host 网卡。这个网卡其实是 macvlan 的一个子接口，其目的，是为了实现宿主机与容器互访，这一点非常重要。 K8s 集群的交换机是独享，且做了高可用的。为了应对以后容器数量增长提前做的准备。 我们是自建机房。如果是云服务器的话，macvlan 模式可能不一定用的了。因为 macvlan 依赖网卡特性以及网卡的混杂模式，这些特点，云服务器不一定支持。云服务器的话，可以考虑云服务厂商提供的现成的，且与 VPC 良好集合的方案。 flannel 网络flannel 网络，相对于 macvlan 网络来说，要复杂一些。 flannel 网络，是 CoreOS 团队为 K8s 打造的一款用于解决 POD 跨主机互访的网络实现。它是一个 overlay 网络，是构建在 L2 层网络之上的虚拟网络。每一个 Pod 实例，都有一个虚拟 IP 地址。 下面一是 flannel 的网络架构图： 这个网络模式有几个核心的点： 在主机上，flannel 的守护进程 flanneld 在启动之后，会创建一个 flannel.1 的网卡（如果是 vxlan backend 的话）。它的作用，就是为了容器跨主机访问做封包、解包、创建整个 POD 网络互访需要的路由规则等内容的。要点：解决容器跨节点通信。 在主机上，还有一个 cni0 网卡。它的作用，和 docker 本身网络下的 docker0 作用是一样的，它们都是网桥 bridge。目的是解决容器在同一个主机上通信的。另外，cni0 这个网卡，并不是 flannel 启动的，确切的说，它是 K8s flannel 的这个 CNI 插件（在第一次创建 Pod 的时候）创建的。 flannel 网络是一个虚拟网络，容器的流量要出访，必须经过 flannel 做封包处理，这也是 overlay 网络的特点。flannel vxlan 是做的 UDP 封包。 如果在 L2 层 K8s 网络里引入 flannel 网络，必须理清几个点打通 2 个网络的方案中，是可以考虑，在 L2 层 K8s 中，引入虚拟网络这种方案的。如果是这种方案的话，flannel 网络下 K8s 用的组件，是不一定都得用在 L2 层 K8s 网络里的。因此，要理清几个点： ①：访问虚拟网络 POD 实例，就一定需要知道目标实例在哪个物理机节点。这个是谁来维护的（flanneld 守护进程）。 ②：只是知道谁来维护 POD 网络信息还不行，必须得具备访问虚拟网络的能力，这就依赖 flannel 的 网卡：flannel.1 。这个网卡，也是 flanneld 守护进程创建的。 ③：通常 flannel 网络里，还有一个 cni0 网卡，它要解决的点是同一个主机上容器互通的。而 L2 层 K8s 集群访问虚拟网络 K8s 集群，是要跨节点访问的，所以，L2 层网络的 K8s 下，cni0 网卡并不需要。 因此，从表现上来看，如果我们的方案是在 L2 层 K8s 里起虚拟网络 flannel 的话，只用 flannel 网络里的 flanneld 进程，而不用 flannel 网络下的 CNI 插件。 实施方案关于打通 2 个现有不同网络模式的 K8s 集群，做到实例互通，其实最大的点在于，对哪个集群做改造。L2 层网络的 K8s 是基于 macvlan 来做的，这种模式的 K8s 下，容器 IP 就和虚拟机 IP 一样，可以认为 L2 层网络的 K8s 下的容器，就和现有的物理机、虚拟机一样。因此，这个问题的本质就成了：如何让 L2 层网络的 K8s 可以访问到虚拟网络的 K8s 实例。 方案有3个： 方案1：通过 VPN 的方式VPN，在 L2 层网络的每一个节点，通过 VPN 的方式接入虚拟网络中。所以，需要虚拟网络 K8s 的某个节点开启 VPN Server，2 层网络的每一个 K8s 节点，都通过 VPN Client 接入到虚拟网络中。这种方式，有2个弊端： 原来的虚拟网络集群的某个节点，得起一个 Server，这个 Server 所在节点，要承载 2个 集群的所有互访流量。这个 Server 所在主机的网络带宽可能会成为瓶颈，另外，Server 一旦挂了，2个集群的互通，全部中断。 L2 层网络 K8s 的所有节点，都得启动 VPN Client。这也是个维护成本。 方案2：找几个节点，做路由转发，相当于路由器的角色。这个方案的主要核心在于，在虚拟网络 K8s 中，挑选几个节点，作为路由器角色，暂且称之为中转节点。L2 层网络K8s 访问虚拟网络 K8s 的流量，都经过这几个中转节点。中转节点，必须要在虚拟网络 K8s 中。访问模式为：2层容器->中转节点->虚拟网络容器。这个方案，如果可以实施的话，还是会存在中转节点是网络瓶颈的问题（但是比方案1好多了）。 方案3：将 L2 层网络的 K8s 集群的节点，全部加入到虚拟网络 K8s 的网络中。这个方案的本质是，在 L2 层网络下的 K8s 节点上，部署 flannel 的组件。从网络层的角度看，L2 K8s 集群的实例，即属于 macvlan 2层网络，又属于 flannel 虚拟网络。但是这里有几个细节是要注意的： L2 K8s 集群部署的 flannel 其实是需要在每个节点都部署的。 L2 K8s 节点上的 flannel，其职责，只是为了启动 flannel.1 这个虚拟网卡，这个网络的职责，就是承担 L2 K8s 节点实例，访问虚拟网络的一个桥梁。 flannel 组件，必须，且不能干预到 L2 层 K8s 体系内的 2 个角色：Docker 、CNI。之所以提这个点，是因为，flannel 的部署往往和容器的 IP 分配耦合在一起。尤其是 flannel 官方提供的部署工具中，还有操作 docker 的脚本。这个是不需要执行的。 上面的3个方案，最终的落地方案是：方案3 。原因是操作简单，没有任何节点成为整个方案的瓶颈。 最终方案中，要解决及不准备解决的问题 虚拟网络 K8s 下的容器，访问 L2 层 K8s 下的容器和物理机。（ 要解决，且天然就没有屏障） L2 物理机访问虚拟网络容器。（要解决） L2 容器访问虚拟网络容器。（要解决，注意，这里没和第 2 点放到一起，是因为这个问题解起来不一样） L2 容器和物理机访问虚拟网络 service vip 。（不解决） 关于在 L2 层 K8s 中，【不打算】支持访问 flannel 虚拟网络里的 service ip 的理由如下： ①：目前，需要网络里的服务，不需要靠 K8s service ip 来调用。比如：Java 体系内的 Spring Cloud 实例互访就不需要 service 。 ②：L2 层网络 K8s 访问虚拟网络 service ip，相当于动作比较大，需要在 L2 层 K8s 部署 kube-proxy ，且这个 kube-proxy 要指向到另外一个虚拟网络集群的 kueb-apiserver 上。仅为支持这样一个弱需求，就要增加了整体架构复杂度。且，如果在 L2 层 K8s 里引入其他集群的 kube-proxy，它会新生成很多的 iptables 规则，有碍于现有 L2 层 K8s 的整体性能。 ③：在一个纯 L2 层的 K8s 网络里，支持 service ip ，并不是说起了 kube-proxy 就完了，这和 flannel 不一样。后边会专门开心的文章，将如何在 L2 层 K8s 对 service ip 的访问支持。 最终方案架构 说明： 左图为基于 flannel 的虚拟网络集群，右图为基于 macvlan 的 2层网络集群。 flannel 的网络相对经典，主机内容器互通（比如 集群1中，容器1 和容器2 互通），唯一的依赖就是网桥：bridge。 macvlan 网络类似，主机内容器互通（比如 集群2中，容器1 和容器2 互通），唯一的依赖，就是 bond0（网卡聚合），这里要注意，bond0 已经成了 macvlan 下的虚拟交换机，类似于网桥，和 bridge 最大的区别有2个，一个 macvlan 相比 bridge 来说，交换性能更高，另外就是，主机若要访问主机上的容器，必须借助基于 bond0 生成一个宿主机的新网卡（子接口）：macv-host。 主机内，不同网络的流转，借助主机内的路由表和 iptable。 集群2（2层网络）里的物理机，若要访问集群1（虚拟网络）里的容器。需要在2层网络集群的节点上，搭建 flannel。这个 flannel 版本、配置，和集群1里的 flannel 一样，并且使用同一套 etcd 集群，如上图。所有 flannel 基于 集群1 里的 etcd，共享网络发现能力和数据。所以，flannel 的职责非常清晰，只有3个：网络发现+同步路由表+overlay包处理。这种方式，解决了 L2物理机访问虚拟网络容器 这个点。 集群2（2层网络）里的容器，若要访问集群1（虚拟网络）里的容器，除了上面的工作之外，还需要一个事儿，那就是让集群2里的容器，知道如何访问集群1的网络。因此，需要将集群2的 K8s macvlan CNI 做一个配置，将访问虚拟网络的路由表，配置进去，且这个对虚拟网络的路由表依赖的网关，设置为当前物理主机。换句话说，当前主机，要充当路由器的角色。 2层K8s里的容器访问虚拟网络的流量的数据流为： 2层网络 K8s 容器->流转到当前物理机->主机路由表->主机flannel->出2层K8s的节点->到虚拟K8s的节点->节点路由表->节点bridge网桥->虚拟网络容器内部。 补充1、贴一下 L2 层 K8s 节点最终的 CNI 配置 这个是 L2 K8s CNI 必须要做的一个配置，其目的是，让 L2 K8s 里启动的容器里，包括一个路由表，告诉其如果访问的网络是虚拟网络的 IP 端口的话，流量不要走原来的网关，而是走到当前节点的物理机上。 另外，L2 K8s 节点还要做几个事儿： Linux 上 必须开启 ip_forward。 iptables 中也要适当做放行，也就是说，需要在 FORWARD 链添加 ACCEPT。 上面的2个点如果不做处理的话，发往虚拟网络下容器的流量，到了宿主机之后，也会被丢弃。 2、一些参考https://www.lijiaocn.com/%E6%8A%80%E5%B7%A7/2017/03/31/linux-net-devices.htmlhttps://blog.huiyiqun.me/2016/11/24/virtualization-with-libvirt-kvm-and-macvtap.htmlhttps://events.static.linuxfound.org/sites/events/files/slides/LinuxConJapan2014_makita_0.pdfhttps://cs.nju.edu.cn/tianchen/lunwen/2017/sgws-Zhao.pdfhttps://tigerprints.clemson.edu/cgi/viewcontent.cgi?article=1034&amp;context=computing_pubshttps://www.cnblogs.com/echo1937/p/7249037.htmlhttps://blog.51cto.com/dog250/1652063https://www.praqma.com/stories/debugging-kubernetes-networkinghttps://msazure.club/flannel-networking-demystify]]></content>
      <categories>
        <category>k8s</category>
        <category>网络</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>网络</tag>
        <tag>macvlan</tag>
        <tag>flannel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CoreDNS全解—CoreDNS插件开发入门]]></title>
    <url>%2F2019%2F05%2F13%2F19%2F</url>
    <content type="text"><![CDATA[前序在之前的文章里提过，伴随云原生应用概念以及CNCF社区推广，CoreDNS 慢慢作为 K8S 集群的官方DNS服务，正在替代 KubeDNS 作为 K8S 集群的服务发现组件，但是，CoreDNS 因为 Golang 编写的高性能、“插件式”支持等特性，使得 CoreDNS 又不仅仅可以应用在 K8S 体系内，作为一款DNS应用，CoreDNS 可以借助众多插件，扩展其能力，进而应用在更多复杂的场景中，比如：整个公司级的基于服务发现和注册中心的DNS服务能力。 CoreDNS 自然不仅仅可以应用在这么大型的场景下，由于 CoreDNS 本身基于 Golang 编写，所以，利用其跨平台编译的能力，你也可以将其替换 DNSmasq、PowerDNS、SmartDNS 等家庭或个人的私有服务器上，比如，利用 CoreDNS 的 TLS based DNS 查询能力或 gRPC 查询能力，可以防止运营商的 DNS 污染，如果个人开发者具备一定能力，也可以编写插件，实现 SmartDNS（此DNS应用并不开源）的功能，是非常简单的一个事。 本文就主要聊一聊 CoreDNS 插件编写这个事情。CoreDNS 插件开发分为2个部分，CoreDNS 插件编写概述，以及 CoreDNS插件开发进阶。本文，便是第一篇。 其实，CoreDNS 要展开来讲东西还挺多，本文主要讲插件开发，后续，我会专门编写一个 《CoreDNS 全解析》的中文电子书，涵盖关于 CoreDNS 的更多内容。 CoreDNS 插件编写概述概述篇主要包含2部分内容： 如何实现插件 如何注册插件 我相信在你读完整篇内容后，会有一个基本的认知。说的再多不如一次实战。等你自己写插件的时候，带着写插件的基本认知，再去看看其他插件怎么写的，就会有自己的一个体会了。 写插件的话，基本上来说，可以参考的插件推荐列表： hosts forward kubernetes 这几个插件的实现复杂度是由浅入深的。在有些文章里还在提 proxy 插件，我个人是不推荐再去看了。因为首先，从 v1.5.0（截止2019.04.23的最新稳定版） 版本开始，官方已经将其从默认加载的插件列表移除了。再一个，forward 插件很多功能和 forward 类似，完全可以用 forward 插件，替代 proxy 插件。 开始CoreDNS的能力，和插件非常密切，有什么插件，就有什么样的能力，所以，我们如何编写自己的插件呢？ 目前，CoreDNS 的插件机制，是借助 Caddy 项目来做的。所以，虽然本文在讲 CoreDNS 如何编写插件，但是其中有很多插件开发的内容，是和如何利用 Caddy 项目编写插件有相似之处的。 CoreDNS官方的介绍中提到，如果你想自己编写插件，并且希望这个插件能够被官方默认包含到 CoreDNS 项目中去的话，你需要先在 github 上提一个 issue 并在里边描述一下你的插件的设计。所以，打动官方开发人员也是蛮重要的。 如何注册插件我们先不说怎么实现插件，先说一个插件，怎么把一个插件，注册到 CoreDNS 中。 插件的注册很简单，你需要在创建的插件包（package）中，写一个 init 函数。插件的注册，就放到 init 函数中（Golang的 init 函数会在包被使用的时候就自动执行）。 12345678import &quot;github.com/mholt/caddy&quot;func init() &#123; caddy.RegisterPlugin(&quot;foo&quot;, caddy.Plugin&#123; ServerType: &quot;dns&quot;, Action: setup, &#125;)&#125; 每个插件都必须有一个名称，上面的例子中，插件名称是 foo。ServerType 必须且只能是 dns 。这里描述的 Action 就是说只要在 Corefile 中遇到指令 foo，CoreDNS 就会调用一个名为 setup 的函数。 setup：插件参数解析与插件注册上面提到在插件包的 init 函数中，怎么做插件注册，其中，Action 字段的值，就是一个 setup 函数。而 add.RegisterPlugin 有2个参数，第一个是插件名称，第二个是插件的实体。这2个参数，保证了这个插件的唯一性。可以说，Action 字段的内容——setup函数，是为了 做CoreDNS解析和执行Corefile时运行的函数。也是我们马上要讲解的内容。 那么，setup 函数具体应该是什么样子的？我们看着例子来说： 1234567func setup(c *caddy.Controller) error &#123; if err != nil &#123; return plugin.Error(&quot;foo&quot;, err) &#125; return nil&#125; 简单来说，setup 这个函数，它接受一个类型为 caddy.Controller 的参数。返回值是一个 Golang 的 error 类型(在这个例子中，使用 plugin.Error 方法，做了一个对原有 error 的 wrap 封装，形成一个带有 plugin/foo 前缀的 error）。 setup 函数的主要职责，就是是解析指令的标记用的。我们看一下之 hosts 插件的语法： 1234567hosts [FILE [ZONES...]] &#123; [INLINE] ttl SECONDS no_reverse reload DURATION fallthrough [ZONES...]&#125; hosts 插件，怎么解析 ttl、no_reverse、reload 等指令呢？其实就是在 setup 函数里做的解析。如果我们自己手写这种解析，十分麻烦，但是，借助 setup 函数的参数： *caddy.Controller 就很容易做这样的解析了，这就是 setup 为什么需要这么一个参数的根本原因。 按照上面的例子，我们的 setup 方法怎么做插件参数的解析？ 123456789101112131415161718192021222324// 下面的 c 指的其实就是 *caddy.Controllerfunc setup(c *caddy.Controller) error &#123; // ... // 参数解析 for c.Next() &#123; // 跳过指令名称，也就是跳过 &quot;hosts&quot; 这个名称 args := c.RemainingArgs() // 获取所有参数，也就是 [FILE [ZONES...]] 对应的实际内容 for c.NextBlock() &#123; // 获取下一个块内容，并遍历 switch c.Val() &#123; // 获取指令名 case &quot;fallthrough&quot;: h.Fall.SetZonesFromArgs(c.RemainingArgs()) case &quot;no_reverse&quot;: options.autoReverse = false case &quot;ttl&quot;: ... &#125; &#125; &#125; //... &#125; 最后，其实插件的注册，也是在 setup 函数内做的，插件的注册方式如下： 12345678910func setup(c *caddy.Controller) error &#123; // .... // 参数解析 // ... // 插件注册 dnsserver.GetConfig(c).AddPlugin(func(next plugin.Handler) plugin.Handler &#123; k.Next = next return k &#125;)&#125; 我们看一个相对完成的 setup 函数示例： 1234567891011121314151617181920212223242526func setup(c *caddy.Controller) error &#123; // See comment in the init function. os.Stderr = os.Stdout // 插件参数解析，并返回一个插件实体（先不用管插件实例怎么写，后边会介绍） // 参数解析，放到了其他函数内，防止一大坨代码 k, err := paramsParse(c) if err != nil &#123; return plugin.Error(&quot;multikube&quot;, err) &#125; // 插件初始化 err = k.InitKubeCacheMulti() if err != nil &#123; return plugin.Error(&quot;multikube&quot;, err) &#125; k.RegisterKubeCache(c) // 插件注册 dnsserver.GetConfig(c).AddPlugin(func(next plugin.Handler) plugin.Handler &#123; k.Next = next return k &#125;) return nil&#125; 将你的插件添加到CoreDNS方式1：1、把插件添加到CoreDNS，单纯从代码上来说，就是得把下面的一行（你的插件包）1import _ &quot;your/plugin/package/path/you-package&quot; 添加到 core/coredns.go 中，这一步可以保证你的包能编译到CoreDNS可执行文件中。 2、但是，只是编译进去了，并不表示这个插件在CoreDNS可以启用，所以，你还得把它添加到 directive.go，在这个文件中，你需要把你的插件名称，添加到 directives 这个 slice 里去。需要注意的是，slice 里插件的顺序非常的重要，这个顺序，决定了CoreDNS处理DNS请求时所执行的插件顺序（我们之前的文章提过，CoreDNS的插件执行顺序，并不是在Corefile中定义的。其实本质上，就是这个 slice 里定义的）。 方式2：1、上面的2个操作，还是有点麻烦。有更简单的方式：修改 plugin.cfg ，添加你的插件（注意：在这个里边定义好插件顺序，就是 CoreDNS 编译后插件的执行顺序） 2、执行 go generate coredns.go 即可自动生成 directive.go 和 core/coredns.go。然后执行构建即可。 CoreDNS里的插件的实现逻辑前面已经提到了如果自己写插件，插件怎么做参数解析（在 setup 函数内），怎么注册到CoreDNS（也是在 setup 函数内）、怎么定义插件顺序编译，没有提的是，我们自己的插件，到底应该如何实现具体的逻辑。先回顾一下 setup 函数的基本示例： 1234567891011121314151617181920212223242526func setup(c *caddy.Controller) error &#123; // See comment in the init function. os.Stderr = os.Stdout // 1、插件参数解析，并获得具体插件的实例化对象 // 我们既要要讲的就是这个 k，怎么实现。 k, err := paramsParse(c) if err != nil &#123; return plugin.Error(&quot;multikube&quot;, err) &#125; // 2、插件初始化 err = k.InitKubeCacheMulti() if err != nil &#123; return plugin.Error(&quot;multikube&quot;, err) &#125; k.RegisterKubeCache(c) // 3、插件注册 dnsserver.GetConfig(c).AddPlugin(func(next plugin.Handler) plugin.Handler &#123; k.Next = next return k &#125;) return nil&#125; 我们这次要讲的就是，插件实例化对象 怎么编写。 如果你去看 godoc for the plugin package 的话，里边有一个非常最重要的类型是 plugin.Handler。这个 Handler 是一个处理DNS请求的函数。虽然CoreDNS会为你设置启动DNS服务器的各项事宜，但是，插件的实例（也就是你插件的功能）你还得自己来实现（实现 plugin.Handler 接口）。 实现插件接口plugin.Handler 是一个类似于 http.Handler 的接口（写过Golang的HTTP Server的同学一定不陌生），不同的是，它是处理 DNS 请求用的，这个接口里边，有一个 ServeDNS 方法（这个方法，返回 int，error）。 int 是 DNS rcode，也就是 DNS 响应状态码，error 是 处理 DNS 请求过程中的错误。有关这些返回值的更多详细信息，请阅读 plugin.md 文档。 前面提到，plugin.Handler 仅仅是一个接口，我们需要创建一个结构体类型来实现此接口，通常来说，实现这个接口的结构体大概是这样的： 123type MyHandler struct &#123; Next plugin.Handler&#125; 然后，为这个接口体，添加 ServeDNS 方法，这个方法是真正处理DNS请求的方法： 12345678func (h MyHandler) ServeDNS(ctx context.Context, w dns.ResponseWriter, r *dns.Msg) (int, error) &#123; // 此处是具体实现 // ... // 最后，如果前面的逻辑未能找到DNS记录，可以将DNS请求转给下一个插件处理 return h.Next.ServeDNS(ctx, w, r)&#125; plugin.Handler 还需要一个方法func Name（）string，它主要返回当前的插件名称。 插件编写总结 在 coredns/plugin 目录下创建自己的插件 package 在插件 package 下，创建 setup.go 在其中的 init 方法中，做插件配置工作（init 调用 setup 方法） setup 方法中做3个事情：参数解析、实例化插件实体、将插件实体注册到CoreDNS中。 实现插件实现：实现 ServeDNS 和 Name 方法。 更改 plugin.cfg 文件添加你的插件，执行 go generate coredns.go 生成代码，最后执行 go build （或者make)构建。]]></content>
      <categories>
        <category>coredns</category>
        <category>golang</category>
      </categories>
      <tags>
        <tag>golang</tag>
        <tag>coredns</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一个奇怪的golang对切片的竞争检测问题]]></title>
    <url>%2F2019%2F04%2F19%2F18%2F</url>
    <content type="text"><![CDATA[问题示例1、首先，在开始之前，先说一点相关的东西。 在 Golang 中，有很多数据结构的操作，都不是线程安全的，比如大家熟知的 map ，比如 container/list 包。线程安全，指的是基于这类数据结构实例化的变量，可以并发操作，也就是多个 goroutine 同时进行操作。 另外，也许你也知道，golang 在编译时，是支持并发竞争检测的。go build –race ，很多 gopher 其实并不陌生。这里需要说一点是，–race 并非只支持构建时，也支持单测时，也就是 go test –race。 好了，结合上面2个点，我们看一个例子（文件 xx_test.go）（代码示例1）： 123456789101112131415161718192021222324package mainimport ( "sync" "testing")func TestAppend(t *testing.T) &#123; x := []string&#123;"start"&#125; wg := sync.WaitGroup&#123;&#125; wg.Add(2) go func() &#123; defer wg.Done() y := append(x, "hello", "world") t.Log(cap(y), len(y)) &#125;() go func() &#123; defer wg.Done() z := append(x, "goodbye", "bob") t.Log(cap(z), len(z)) &#125;() wg.Wait()&#125; 代码很简单，初始化一个切片，起2个协程，并发操作这个切片。 我们做一下单测并做竞争检测： 123$ go test --racePASSok test/test12 1.017s 从结果来看，竞争检测结果是通过的。 2、我们将上面的代码做一点变更，上面代码第 9 行，切片初始化是这样的： 1x := []string&#123;"start"&#125; 我们做一个改动，给它一个大小 1x := make([]string, 0, 6) 仅此而已，什么都不变，然后我们再看一下完整的代码，并再次做一次竞争检测（代码示例2）。 123456789101112131415161718192021222324package mainimport ( "sync" "testing")func TestAppend(t *testing.T) &#123; x := make([]string, 0, 6) wg := sync.WaitGroup&#123;&#125; wg.Add(2) go func() &#123; defer wg.Done() y := append(x, "hello", "world") t.Log(cap(y), len(y)) &#125;() go func() &#123; defer wg.Done() z := append(x, "goodbye", "bob") t.Log(cap(z), len(z)) &#125;() wg.Wait()&#125; 我们再次做测试，看一下测试结果： 123456789101112131415161718$ go test --race==================WARNING: DATA RACEWrite at 0x00c0000a4120 by goroutine 8: test/test12.TestAppend.func2() /Users/shuai/go/src/test/test12/aaa_test.go:20 +0xbePrevious write at 0x00c0000a4120 by goroutine 7: // ... --- FAIL: TestAppend (0.00s) aaa_test.go:16: 6 2 aaa_test.go:21: 6 2 testing.go:809: race detected during execution of testFAILexit status 1FAIL test/test12 0.012s 结果： 很直接，golang 直接告诉我们有数据竞争，数据竞争检测不通过。而我们仅仅只改了 slice 的初始化方式而已。 为什么测试会失败要理解为什么会失败，就需要看我们2个例子中，切片的内存变化。 代码示例1的切片内存布局竞争检测通过的代码示例1（也就是第一个代码例子）中的 x 初始化方式我们回顾一下： 1x := []string&#123;"start"&#125; 在这个切片中，名称为 x ，长度为 1，容量也为 1 。 但是需要注意，在代码示例1，2个不同的协程，要向 x 中分别添加元素：”hello”, “world” 和 “goodbye”, “bob” ，所以，Golang 需要新开辟内存空间，切片 x 的内存变化如图： 这个图有几个关键点： 原始切片为 x，长度和容量都是 1。 协程1为切片 x，添加元素，并将结果赋值给新的变量 y。相当于直接开辟了内存空间 y，做元素新增的操作。 协程2为切片 x，添加元素，并将结果赋值给新的变量 z。相当于直接开辟了内存空间 z，做元素新增的操作。 当多个线程读取内存 x 时，由于 x 底层一直就没变化，因此，不会发生数据争用。竞争检测是通过的。 代码示例2的切片内存布局在后来的例子，也就是代码示例2中，代码有所变化，我们回顾一下： 1x := make([]string, 0, 6) 从图中可以看到，切片 x 的内存布局有所变化，长度为 0，但是容量为 6。在代码示例2中，有2个协程，在往 x 中，分别添加2个 元素。问题是，在这个切片 x 中，是有足够的空间，可以放下 6个新元素的。因此，协程1和协程2，都会往切片 x 的内存空间中，添加新元素。 而竞争，就是发生是因为两个goroutine都试图写入相同的内存区域。因此，数据竞争产生了。golang test –race 也就失败了。 竞争对切片 x 写数据的示意图如下： 如图，协程1 和 协程2 ，竞争操作了同一个切片 x。最终也不知道谁赢了。 结论： 在 Golang 的切片操作中，每次调用 append 并不会强制执行新的内存分配。因此，上面的情况，这是 golang 本身的特性，而不是bug。 如何避免上述问题解决方式1：预先分配好目标变量内存最简单的解决方法是，做 append 操作时，如果你希望 append 后是一个新的数据，那么，一开始就不要不使用有共享状态的变量，作为要追加的第一个变量。 比如，使用你需要的总容量创建一个新切片，并使用新切片作为要追加的第一个变量。 下面是一个代码示例： 1234567891011121314151617181920212223242526272829303132package mainimport ( "sync" "testing")func TestAppend(t *testing.T) &#123; // 原始切片x x := make([]string, 0, 6) x = append(x, "start") wg := sync.WaitGroup&#123;&#125; wg.Add(2) go func() &#123; defer wg.Done() // 先为变量 y，初始化切片。 y := make([]string, 0, len(x)+2) // 将 x 的所有元素放到 y 中，然后再执行 append 操作。 y = append(y, x...) y = append(y, "hello", "world") t.Log(cap(y), len(y), y[0]) &#125;() go func() &#123; defer wg.Done() z := make([]string, 0, len(x)+2) z = append(z, x...) z = append(z, "goodbye", "bob") t.Log(cap(z), len(z), z[0]) &#125;() wg.Wait()&#125; 总的来说（以协程1的操作为例）： append 之前，先创建新的变量 y。 将 x 原有的数据，添加到 y 中。 执行你需要 append 的新元素。 这个操作其实有点繁琐，谈不上优雅，而且内存效率也有一定程度上的浪费。 解决方式2：加锁12345678910111213141516171819202122232425262728293031323334package mainimport ( "github.com/k0kubun/pp" "sync" "testing")func TestAppend(t *testing.T) &#123; x := make([]string, 0, 6) // 实例化一个锁 lock := sync.Mutex&#123;&#125; wg := sync.WaitGroup&#123;&#125; wg.Add(2) go func() &#123; // 协程1在进行append前加锁 lock.Lock() defer lock.Unlock() defer wg.Done() y := append(x, "hello", "world") t.Log(cap(y), len(y)) &#125;() go func() &#123; // 协程2在进行append前加锁 lock.Lock() defer lock.Unlock() defer wg.Done() z := append(x, "goodbye", "bob") t.Log(cap(z), len(z)) &#125;() wg.Wait() pp.Println(x)&#125; 当然，如果你有更好的解决方式，欢迎指正。]]></content>
      <categories>
        <category>golang</category>
      </categories>
      <tags>
        <tag>golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聊一聊golang的结构体标签]]></title>
    <url>%2F2019%2F04%2F14%2F17%2F</url>
    <content type="text"><![CDATA[前言Golang的结构体标签可能每一个Gopher都在用，尤其是在json处理的地方用。比如： 123456type NetConf struct &#123; Master string `json:"master"` Mode string `json:"mode"` MTU int `json:"mtu"` Debug bool `json:"debug"`&#125; 毋庸置疑，这个 NetConf 结构体实体变量在转换 json 的时候，Master 字段会变为 json 字符串中的 “master “。 如果你对结构体标签了解仅此不多，那可以继续往下看了。 从一个结构体标签可以从属多个字段开始说123456type T struct &#123; f1 string "f one" f2 string f3 string `f three` f4, f5 int64 `f four and five`&#125; 上面代码在声明的时候，f4 和 f5 是一起进行的声明，因此后边的字段标签 “f four and five” 即是 f4 字段的，也是 f5 字段的。 聪明如你，你的问题就来了。 上面代码的字段标签比较特殊，是一个字符串 “f four and five” ，我们常用在 json 处理时的字段标签往往是这样的 json:”debug”` ，那么，同样的字段标签，用在多个字段上，是否可行？ 我们验证一下这个问题： 123456789101112131415161718192021package mainimport ( "encoding/json" "fmt")type NetConf struct &#123; Master string `json:"master"` Mode string `json:"mode"` // 下面2个字段，字段标签都是 test MTU int `json:"test"` Debug bool `json:"test"`&#125;func main() &#123; n := NetConf&#123;"master1", "mode1", 1500, true&#125; if d,e:=json.Marshal(n);e==nil&#123; fmt.Println(string(d)) &#125;&#125; 代码输出： 1&#123;"master":"master1","mode":"mode1"&#125; 可以看到，同一个结构体中，多个字段拥有同一个字段标签后，golang 内置的 json 解析器将直接忽略这个字段标签对应的所有字段的解析。 换句话说，对于 golang 的 json 包，同样的字段标签，不能用在多个字段上。但是，json 包不能处理，并不等于 golang 不允许，这只是个别 package 的行为，仅此而已。 虽然我们在对 NetConf 的 MTU 和 Debug 字段都设置的标签为 test，我们怎么证明编译之后，这个字段标签确实存在呢（而不是被编译器主动忽略）？ 这就引出我们下一个内容。 如何拿到结构体的标签内容还是用上面的结构体，代码如下： 1234567891011121314151617181920212223package mainimport ( "fmt" "reflect")type NetConf struct &#123; Master string `json:"master"` Mode string `json:"mode"` MTU int `json:"test"` Debug bool `json:"test"`&#125;func main() &#123; n := reflect.TypeOf(NetConf&#123;&#125;) //打印字段MTU的标签 mtu, _ := n.FieldByName("MTU") fmt.Println(mtu.Tag) //打印字段Debug的标签 de, _ := n.FieldByName("Debug") fmt.Println(de.Tag)&#125; 输出： 123$ go run main.gojson:"test"json:"test" 说明什么，同一个结构体的不同字段，确实是可以有相同的字段标签的。只是 golang 内置的 json 解析器自己处理了这种特殊情况而已（或者说，json解析器自己认为这种情况对它而言有点特殊）。 其实，结构体标签是有常规格式和非常规格式的。而，golang 的 json 包，只是用了结构体标签的常规格式而已。 json:&quot;master&quot; 这种就是常规格式。除此之外就是非常规格式。下面，我们重点来说。 结构体标签的【常规格式】上面已经举例了一个常规格式了，下面再举一个 123type T struct &#123; f string `one:"1" two:"2" blank:""`&#125; 可以看出来，这种具备一个个键值对形式组成的结构体标签，便是常规格式。 我们可以使用反射，获取结构体标签的每一个键值对内容 123456789101112131415161718192021222324package mainimport ( "fmt" "reflect")type T struct &#123; f string `one:"1" two:"2" blank:""`&#125;func main() &#123; n := reflect.TypeOf(T&#123;&#125;) //获取字段：f t, _ := n.FieldByName("f") // 获取整个结构体标签内容 fmt.Println(t.Tag) // 获取第1个键值对内容的值，以及是否设置了值 fmt.Println(t.Tag.Lookup("one")) // 获取第2个键值对内容的值，以及是否设置了值 fmt.Println(t.Tag.Lookup("two")) // 获取不存在的键的值，以及是否设置了值 fmt.Println(t.Tag.Lookup("xxxx"))&#125; 12345$ go run main.goone:"1" two:"2" blank:""1 true2 true false 可以看出来，用 reflect 包的 Tag ，可以获取每一个键值对的内容，以及是否存在这个键。 另外需要特别注意：结构体标签的多个键值对之间，必须用空格分割。，不能用逗号！！！，不能用逗号！！！，不能用逗号！！！ 结构体标签的键值对中，键有什么用键的用处，通常来说，就是 package name 。比如 json 解析的时候，结构体标签的键值对中，键通常就设置为 json。值设置为要解析或泛解析为什么字段名称。啰嗦一点，再看一下示例 12345678type T1 struct &#123; // json 包将 F1 字段最终解析为 json字符串内容 foo 。 F1 int `json:"foo"` &#125; type T2 struct &#123; // json 包将 F2 字段最终解析为 json字符串内容 bar 。 F2 int `json:"bar"` &#125; 结构体标签对类型转换没有影响将一个结构体的值，转换为其他类型时，需要结构体底层类型必须相同。但是字段标签不包括在内。 12345678910type T1 struct &#123; f int `json:"foo"` &#125; type T2 struct &#123; f int `json:"bar"` &#125; t1 := T1&#123;10&#125; var t2 T2 t2 = T2(t1) fmt.Println(t2) // &#123;10&#125; 结构体标签的使用场景(Un)marshaling在 Golang 里边，结构体标签用的最多的地方，就是编码和解码。上面我们已经举了好几个关于 json 编码和解码的例子，不再多说。json（encoding/json） 只是编码解码的一种格式而已，其实 xml（encoding/xml）也在使用结构体标签。 在HTTP表单数据处理上，比如这个包 gorilla/schema，可以将HTTP的POST表单解析为一个结构体。 ORMORM，是对象关系映射的缩写（Object-relation mapping），主要用在数据库操作上。Golang里边有很多类似的工具，比如：gorm 等。 go vetgolang 的编译器对结构体标签的格式并不会强制检查其是否是合法的键值对。但是，go vet 工具会做检查。所以，为了程序的健壮性和正确性，我们可以用 go vet 作为 CI pipeline 的一部分，用在编译之前。 前面我提到，golang的结构体标签的键值对之间用空格分割，如果你一不小心用逗号分隔，还忘了这个事情，后续程序出问题，还真的无法立刻就能排查出来，这个时候，go vet 用处就很大了。 12345package maintype T struct &#123; f string "one two three"&#125;func main() &#123;&#125; 12&gt; go vet tags.gotags.go:4: struct field tag `one two three` not compatible with reflect.StructTag.Get: bad syntax for struct tag pair 其他结构体标签还有很多其他用处。比如：配置管理、结构体字段的默认值、校验、命令行参数描述等等。golang 官方仓库的wiki中也有一些说明，有兴趣的可以看一下： Well-known-struct-tags]]></content>
      <categories>
        <category>golang</category>
      </categories>
      <tags>
        <tag>golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一个奇怪的golang等值判断问题]]></title>
    <url>%2F2019%2F04%2F03%2F16%2F</url>
    <content type="text"><![CDATA[问题场景分析一下，下面代码的输出是什么（判断a==c）的部分 12345678910111213141516package mainimport ( "fmt" "runtime")type obj struct&#123;&#125;func main() &#123; a := &amp;obj&#123;&#125; fmt.Printf("%p\n", a) c := &amp;obj&#123;&#125; fmt.Printf("%p\n", c) fmt.Println(a == c)&#125; 很多人可能一看，a和c完全是2个不同的对象实例，便认为a和c具备不同的内存地址，故而判断a==c的结果为false。我也是一样。我们看一下实际输出： 1230x1181f880x1181f88true 问题分析要分析上面的问题，就需要了解一些Golang内存分配，以及变量在内存逃逸的知识。上面的代码，有打印a和c的内存地址。倘若我们去掉任意一个（或者将打印内存的地址都去掉也一样），则 a==c 的判断输出，就是 false。再看一下代码： 123456789101112131415package mainimport ( "fmt")type obj struct&#123;&#125;func main() &#123; a := &amp;obj&#123;&#125; //fmt.Printf("%p\n", a) c := &amp;obj&#123;&#125; fmt.Printf("%p\n", c) fmt.Println(a == c)&#125; 输出： 120x1181f88false 那么，可以看出，是 fmt.Printf 影响了最终结果的判断。好吧，我们看一下，上面代码的内存逃逸情况分析： 1go run -gcflags '-m -l' main.go 123456789# command-line-arguments./main.go:13:16: c escapes to heap./main.go:12:10: &amp;obj literal escapes to heap./main.go:14:19: a == c escapes to heap./main.go:10:10: main &amp;obj literal does not escape./main.go:13:15: main ... argument does not escape./main.go:14:16: main ... argument does not escape0x1181f88false 可以看到，变量c从栈内存，逃逸到了堆内存上。而变量a没有逃逸（注意：上面代码中，有 fmt.Printf(“%p\n”, c)，没有 fmt.Printf(“%p\n”, a) ）。由此可以简单判断，是 fmt.Printf 导致变量产生了内存由栈向堆的逃逸。 回到最开始的问题上。 如果代码中，即打印 a，也打印b 的变量内存地址。则会导致 a 和 c，都逃逸到堆内存上。所以，我们的问题就来了。 为什么 fmt.Printf 会导致变量的内存逃逸？ 为什么逃逸到了堆内存，2个变量就一样了？ 问题1：为什么 fmt.Printf 会导致变量的内存逃逸？其实，fmt.Printf 第二个参数，是一个 interface 类型。而 fmt.Printf 的内部实现，使用了反射 reflect，正是由于 reflect 才导致变量从栈向堆内存的逃逸成为可能（注意，并非所有reflect操作都会导致内存逃逸，具体还得看怎么使用reflect的）。我们简单总结为： 使用 fmt.Printf 由于其函数第二个参数是接口类型，而函数内部最终实现使用了 reflect 机制，导致变量从栈逃逸到堆内存。 问题2：为什么变量 a 和 c 逃逸到堆内存后，内存地址就一样了？这是因为，堆上内存分配调用了 runtime 包的 newobject 函数。而 newobject 函数其实本质上会调用 runtime 包内的 mallocgc 函数。这个函数有点特别： 123456789101112131415// Allocate an object of size bytes.// Small objects are allocated from the per-P cache's free lists.// Large objects (&gt; 32 kB) are allocated straight from the heap.func mallocgc(size uintptr, typ *_type, needzero bool) unsafe.Pointer &#123; if gcphase == _GCmarktermination &#123; throw("mallocgc called with gcphase == _GCmarktermination") &#125; // 关键部分，如果要分配内存的变量不占用实际内存，则直接用 golang 的全局变量 zerobase 的地址。 if size == 0 &#123; return unsafe.Pointer(&amp;zerobase) &#125; // ...&#125; 函数比较长，我做了截取。这函数内有一个判断。 如果要分配内存的变量不占用实际内存，则直接用 golang 的全局变量 zerobase 的地址。而我们的变量 a 和 变量 c 有一个共同特点，就是它们是“空 struct”，空 struct 是不占用内存空间的。 所以，a 和 c 是空 struct，再做内存分配的时候，使用了 golang 内部全局私有变量 zerobase 的内存地址。 如何验证 a 和 c 都使用的是 runtime包内的 zerobase 内存地址？改一下 runtime 包中，mallocgc 函数所在的文件 runtime/malloc.go 增加一个函数 GetZeroBasePtr ，这个函数，专门用于返回 zerobase 的地址，如下： 123456// base address for all 0-byte allocationsvar zerobase uintptrfunc GetZeroBasePtr() unsafe.Pointer &#123; return unsafe.Pointer(&amp;zerobase)&#125; 好了，我们回过头再改一下测试代码： 123456789101112131415161718192021package mainimport ( "fmt" "runtime")type obj struct&#123;&#125;func main() &#123; a := &amp;obj&#123;&#125; // 打印 a 的地址 fmt.Printf("%p\n", a) c := &amp;obj&#123;&#125; // 打印 c 的地址 fmt.Printf("%p\n", c) fmt.Println(a == c) // 打印 runtime 包内的 zerobase 的地址 ptr := runtime.GetZeroBasePtr() fmt.Printf("golang inner zerobase ptr： %p\n", ptr)&#125; 重新编译： 12// 注意，改了 golang 的源码，再编译的话，必须加 -a 参数go build -a 结果输出如下： 12340x1181f880x1181f88truegolang inner zerobase ptr： 0x1181f88 问题得证。 参考： https://studygolang.com/topics/8655\#reply0 https://golang.org/src/runtime/malloc.go https://studygolang.com/articles/5790 http://legendtkl.com/2017/04/02/golang-alloc/ http://reusee.github.io/post/escape_analysis/]]></content>
      <categories>
        <category>golang</category>
      </categories>
      <tags>
        <tag>golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Golang实现数据结构“栈”的三种实现，性能对比及应用示例]]></title>
    <url>%2F2019%2F04%2F02%2F15%2F</url>
    <content type="text"><![CDATA[前言本文主要讲一讲栈这种非常基础的数据结构，以及其如何用Golang来实现的3种方式，简单用golang bench做一个性能比对，以字符串括号匹配的一个例子来看其一个简单的应用场景。 栈的特性栈是一种FILO类型的数据结构，FILO 即 Fisrt In Last Out，也就是先进后出，也可以说是后进先出。它很像一个只有一个出口的站立的杯子，后边进入杯子的东西，会被最先取出来。 栈实现的三种方式首先，实现栈，远非只有这三种方式，这里，只是举例3种相对比较典型的方式。每一种实现方式都很简单，也没什么需要太费周章讲的必要。 1、利用Golang的slice 12345678910111213141516171819202122232425262728293031323334353637383940414243444546package mainimport ( "fmt" "sync")// Item the type of the stacktype Item interface&#123;&#125;// ItemStack the stack of Itemstype ItemStack struct &#123; items []Item lock sync.RWMutex&#125;// New creates a new ItemStackfunc NewStack() *ItemStack &#123; s := &amp;ItemStack&#123;&#125; s.items = []Item&#123;&#125; return s&#125;// Pirnt prints all the elementsfunc (s *ItemStack) Print() &#123; fmt.Println(s.items)&#125;// Push adds an Item to the top of the stackfunc (s *ItemStack) Push(t Item) &#123; s.lock.Lock() s.lock.Unlock() s.items = append(s.items, t)&#125;// Pop removes an Item from the top of the stackfunc (s *ItemStack) Pop() Item &#123; s.lock.Lock() defer s.lock.Unlock() if len(s.items) == 0 &#123; return nil &#125; item := s.items[len(s.items)-1] s.items = s.items[0 : len(s.items)-1] return item&#125; 此方式特点： 利用Golang的Slice，足够简单。 允许添加任意类型的元素。 Push和Pop有加锁处理，线程安全。 在一些文章里（Reddit）提到，使用slice作为Stack的底层支持，速度更快。但是，slice最大的问题其实是存在一个共用底层数组的的问题，哪怕你不断的Pop，但可能对于Golang来说，其占用的内存，并不一定减少。 性能测试如下： 12345678910111213141516171819202122232425262728package mainimport ( "testing")var stack *ItemStackfunc init() &#123; stack = NewStack()&#125;func Benchmark_Push(b *testing.B) &#123; for i := 0; i &lt; b.N; i++ &#123; //use b.N for looping stack.Push("test") &#125;&#125;func Benchmark_Pop(b *testing.B) &#123; b.StopTimer() for i := 0; i &lt; b.N; i++ &#123; //use b.N for looping stack.Push("test") &#125; b.StartTimer() for i := 0; i &lt; b.N; i++ &#123; //use b.N for looping stack.Pop() &#125;&#125; 12345678$ go test -test.bench=".*" -benchmem -vgoos: darwingoarch: amd64pkg: test/test8Benchmark_Push-4 10000000 222 ns/op 94 B/op 0 allocs/opBenchmark_Pop-4 20000000 65.0 ns/op 0 B/op 0 allocs/opPASSok test/test8 7.644s 2、利用Golang的container/list内置包 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package mainimport ( "container/list" "sync")type Stack struct &#123; list *list.List lock *sync.RWMutex&#125;func NewStack() *Stack &#123; list := list.New() l := &amp;sync.RWMutex&#123;&#125; return &amp;Stack&#123;list, l&#125;&#125;func (stack *Stack) Push(value interface&#123;&#125;) &#123; stack.lock.Lock() defer stack.lock.Unlock() stack.list.PushBack(value)&#125;func (stack *Stack) Pop() interface&#123;&#125; &#123; stack.lock.Lock() defer stack.lock.Unlock() e := stack.list.Back() if e != nil &#123; stack.list.Remove(e) return e.Value &#125; return nil&#125;func (stack *Stack) Peak() interface&#123;&#125; &#123; e := stack.list.Back() if e != nil &#123; return e.Value &#125; return nil&#125;func (stack *Stack) Len() int &#123; return stack.list.Len()&#125;func (stack *Stack) Empty() bool &#123; return stack.list.Len() == 0&#125; 简单来说，container/list 是一个链表。用链表模拟栈，要么都向链表的最后做push和pop，要么都向链表的起点做push和pop，这种方法也非常简单。 性能测试如下： 12345678$ go test -test.bench=".*" -benchmem -v -count=1goos: darwingoarch: amd64pkg: test/test10Benchmark_Push-4 5000000 222 ns/op 48 B/op 1 allocs/opBenchmark_Pop-4 20000000 73.5 ns/op 0 B/op 0 allocs/opPASSok test/test10 10.837s 3、godoc的实现参考（自定义数据结构实现） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657package mainimport ( "sync")type ( Stack struct &#123; top *node length int lock *sync.RWMutex &#125; node struct &#123; value interface&#123;&#125; prev *node &#125;)// Create a new stackfunc NewStack() *Stack &#123; return &amp;Stack&#123;nil, 0, &amp;sync.RWMutex&#123;&#125;&#125;&#125;// Return the number of items in the stackfunc (this *Stack) Len() int &#123; return this.length&#125;// View the top item on the stackfunc (this *Stack) Peek() interface&#123;&#125; &#123; if this.length == 0 &#123; return nil &#125; return this.top.value&#125;// Pop the top item of the stack and return itfunc (this *Stack) Pop() interface&#123;&#125; &#123; this.lock.Lock() defer this.lock.Unlock() if this.length == 0 &#123; return nil &#125; n := this.top this.top = n.prev this.length-- return n.value&#125;// Push a value onto the top of the stackfunc (this *Stack) Push(value interface&#123;&#125;) &#123; this.lock.Lock() defer this.lock.Unlock() n := &amp;node&#123;value, this.top&#125; this.top = n this.length++&#125; 此方式特点： 实现比较精巧，唯一需要理解的地方就是 node 结构体中，prev 的部分，它指向的是前一个node成员。 允许添加任意类型的元素。 Push和Pop是线程安全的。 性能测试如下： 12345678$ go test -test.bench=".*" -benchmem -vgoos: darwingoarch: amd64pkg: test/test9Benchmark_Push-4 10000000 178 ns/op 32 B/op 1 allocs/opBenchmark_Pop-4 20000000 75.5 ns/op 0 B/op 0 allocs/opPASSok test/test9 9.776s 对比三种方式，总的来看，第三种基于自定义数据结构的实现方式，在push上效率最高，而且实现也比较精巧。个人其实是推荐使用这种方式的。其次，是基于 container/list 实现的方式。 特性对比 push速度 pop速度 push内存分配 pop内存分配 基于slice 222ns/op 65ns/op 94B/op 0B/op container/list链表 222ns/op 73.5ns/op 48B/op 0B/op 自定义数据结构 178ns/op 75ns/op 32B/op 0B/op 栈数据结构的用途栈这种数据结构通途很多。典型的应用，比如编辑器的撤销（undo）操作，以及校验字符匹配问题。下面主要举一个校验字符匹配的例子： 题目：给定一个包含了右侧三种字符的字符串， ‘(‘, ‘)’, ‘{‘, ‘}’, ‘[‘ and ‘]’，判断字符串是否合法。合法的判断条件如下： 必须使用相同类型的括号关闭左括号。 必须以正确的顺序关闭打开括号。 示例1： 12Input: "()[]&#123;&#125;"Output: true 示例2： 12Input: "([)]"Output: false 参考实现： 12345678910111213141516171819202122func isValid(s string) bool &#123; // 括号对字典 bracketsMap := map[uint8]uint8&#123;'&#123;': '&#125;', '[': ']', '(': ')'&#125; // 传入字符串为空则返回false（leetcode认为空字符串应该返回true，这里注意） if s == "" &#123; return false &#125; // 初始化栈 stack := NewStack() for i := 0; i &lt; len(s); i++ &#123; // 如果栈中有数据，则进行比对，如果栈顶元素和当前元素匹配，则弹出，其他情况向栈中压入元素 if stack.Len() &gt; 0 &#123; if c, ok := bracketsMap[stack.Peek().(uint8)]; ok &amp;&amp; c == s[i] &#123; stack.Pop() continue &#125; &#125; stack.Push(s[i]) &#125; // 到最后如果栈不为空，则说明未完全匹配掉（完全匹配的话，栈只能为空） return stack.Len() == 0&#125; 参考 https://flaviocopes.com/golang-data-structure-stack/ https://github.com/hishboy/gocommons/tree/master/lang https://www.reddit.com/r/golang/comments/25aeof/building_a_stack_in_go_slices_vs_linked_list/]]></content>
      <categories>
        <category>golang</category>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>golang</tag>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K8S CNI之：利用ipvlan+host-local+ptp打通容器与宿主机的平行网络]]></title>
    <url>%2F2019%2F03%2F19%2F14%2F</url>
    <content type="text"><![CDATA[CNI 和 IPAM 概述我们常说的CNI，可以包括IPAM（IP地址管理），也可以不包括IPAM。不过，通过情况下，CNI插件的实现和 IPAM插件的实现是分开为不同的可执⾏⽂件的。但是如果你写到⼀起，这样的CNI也可以⽤。按照K8S本⾝规 范，我们在使⽤CNI的时候，是要区分CNI和IPAM的。 K8S 启动 POD ⽹络的配置流程是，kubelet-&gt;docker(pause)-&gt;cni-&gt;ipam。因此，K8S⽹络的中⼼，便是 CNI 插 件和 IPAM 插件。因为 CNI 和 IPAM 插件很多，所以不打算都讲⼀讲。整体来说，CNI + IPAM 的实现可以分为2 类： ①：第⼀类：与宿主机平⾏⽹络（2层⽹络或3层⽹络）②：第⼆类：利⽤ SDN 技术的虚拟⽹络 ⽽第⼀类的 CNI ⽹络插件，主要是：bridge、macvlan、ipvlan、calico bgp、flannel host-gw 等。第⼆类的⽹络 插件主要有：flannel vxlan、calico ipip、weave 等。 虚拟SDN网络的优势和劣势⼤部分使⽤K8S的公司，都是使⽤的K8S经典⽹络模型，⽐如：K8S+Flannel（VXLAN）、K8S+Calico（IPIP） 等。这种模式下的⽹络有⼀个共同特点，就是容器⽹络在K8S集群内是隔离的，集群外的宿主机是⽆法直接访问容 器IP的。之所以会这样，是因为容器⽹络是虚拟的，它这个K8S的虚拟⽹络，⼤部分上都是借助 路由表 + iptables + 隧道 模式来做的。 这种经典的⽹络模式，有优势，也有劣势。 优势1：⽹络隔离。 这种模式下，K8S集群内的节点的容器，是集群外宿主机节点⽆法访问的，所以，它能很好的起到隔离性的作⽤。 在有些公司这种模式⽐较好，⽐如：为多客户服务的公有云⼚商。另外，不同K8S的集群⽹络隔离，各个集群完全 可以使⽤同⼀个虚拟IP段，没有太多IP地址分配和管理的痛苦。 优势2：部署⽅便。 虽然这种⽅式很多公司会使⽤flannel vxlan模式的⽹络插件，或者calico ipip模式的⽹络插件，这2个插件的部署存 在⼀定复杂度，但是这种经典⽹络模式，有⾮常多的⼀键部署⼯具。⽐如：https://github.com/kubernetes-sigs/kubespray。 劣势1：⽹络隔离。 ⽹络隔离，既是优势，也是劣势。关键还是看公司的使⽤场景是什么。如果是公有云，就有⼀定优势，但如果是私 有云，⽐如⼀个公司做整个容器平台。后期遇到的痛点之⼀，就是⾮K8S集群的机器，如何访问K8S集群内容器的 IP地址。 这⼀点可能很多⼈说不要直接访问容器IP就⾏了，但问题在于，业务的发展往往超过预期，搞K8S的部分，往往都 是基础架构部，或者系统部，但问题在于，这种部分，是⽀持部门，其职责之⼀，就是更好的⽀撑业务发展的多变 性。举个例⼦：如何让 Java 的服务，既可以部署到K8S集群内，还可以使⽤ Spring Cloud 或者 Dubbo+Zookeepr 这种服务发现架构？你可能会说，Java 的微服务要么⽤它们⾃⼰的体系，要么⽤ K8S 体系就⾏了，这就是⽀持部 分，⽀持的程度不够了。 换句话说，⽹络隔离，对公司多技术语⾔和其语⾔⽣态的的发展，存在⼀定阻碍。 劣势2：流量穿透的情况。 K8S 内虽然是⽹络隔离的。但其隔离的并不彻底。它的隔离性，原来的是 kube-proxy 组件⽣成的⼤量 iptables 规 则和路由表来实现的。这种模式下，iptables 的动态变化，其实依赖 kube-proxy 这个组件，⽽这个组件，其实监 听的 K8S 集群的 service + endpoint 资源。我们部署在集群内的服务，访问某个内部服务，⽤的是基于 DNS 的服 务发现机制。这就带来了下⾯⼀个问题： 如果服务A访问服务B，必先经过 DNS 解析拿到 service ip（⽐如 172.18.42.56） 这个虚拟 IP 地址。然⽽，如果 A 是拿着这个IP作为长连接服务，频繁对B发包。这个时候，B服务下掉了。下掉后，kube-proxy 组件，将从 iptables 中删除 B 服务的iptables 规则，也就是说，172.18.42.56 这个虚拟 IP 从 iptables 中删除了。此时，A 如果 还拿着这个 IP 发包，那么因为 集群内已经缺失了这个虚拟 IP 的记录，必然导致这部分流量，将穿越 iptables 规 则，发到上层的⽹关。如果这个流量⾮常⼤的话，将对⽹关路由器造成⾮常⼤的冲击。 这个是切实存在的⼀个问题，也是我们之前遇到的⼀个问题。这个问题，你只能去⼿动修改iptables规则来处理。 因为 kube-proxy 也在修改 iptables 规则，所以，这个操作存在⼀定风险性，需要谨慎操作。 劣势3：最重要的，⽹络的性能损耗严重。 这个问题，要从2个⽅⾯看： ①：基本上做 K8S 和 Docker 的同学都知道。flannel vxlan 和 calico ipip 模式 这种虚拟⽹络，在容器跨宿主机通 信时，存在包的封包和解包操作，性能损耗⼤。②：K8S 内 有⼤量服务和容器的时候，iptables 会⾮常多，这对⽹络的性能损耗也是很⼤的。 容器和宿主机平⾏⽹络打通通常会遇到的问题基于上⾯的内容，很多⼈开始尝试将容器⽹络和公司所有物理机的⽹络打通，也就是形成⼀个平⾏⽹络体系。这个 体系，去除了封包和解包操作，也不再使⽤ kube-proxy 组件⽣成iptables规则。⽹络的性能⾮常⾼。 打通平⾏⽹络的⽅案又很多，⽐如 Calico BGP、Linux Bridge、Linux Macvlan、Linux IPVlan 等等。 这⼏个⽅案的特点如下： ⽅案/特点 复杂度 机房改造成本 运维成本 calico bgp 很⾼ 很⾼ ⾼ linux Bridge 低 无 低 linux ipvlan 很低 无 很低 linux macvlan 很低 无 很低 总体来说： calico bgp 这种⽅案：实现成本⽐较⾼，需要整个公司的基础⽹络架构的物理设备（交换机或路由器）⽀持 BGP 协议才⾏。⽽且，⼀旦出现问题，排除的复杂度也⾼，需要公司的⼈很懂才⾏。我个人觉得，如果公司的研发和网络的掌控力没有达到一定程度的话，不建议采用这种方式。 linux bridge：这种⽅案的成本较低，不过需要将物理⽹关挂到⽹桥下，如果是单⽹卡的物理机，这个操作会断 ⽹，建议多⽹卡环境进⾏操作。 linux macvlan：这种⽅案成本很低，需要⽹卡⽀持混杂模式（⼤部分⽹卡都是⽀持的）。混杂模式需要⼿动开启。 linux ipvlan：这种⽅案成本很低，和 Macvlan 很像。区别是不同 IP 公⽤ mac 地址，性能可能相对 macvlan 好⼀ 些，具体其优势不做细谈。 打通容器和宿主机平⾏⽹络，其实也会存在⼀些问题，需要提前规划好。⽐如，是直接2层⽹络打通，还是3层⽹ 络打通？2层⽹络打通，可能既需要⼀个强⼒的交换机设备，也需要防⽌⼴播风暴产⽣。如果是3层⽹络打通，意 味着多⽹段，⽽多⽹段的情况，每新增⼀个IP段，宿主机都要进⾏⼀个路由表的处理操作，这倒也不是⼀个⿇烦的 问题。 另外，macvlan、ipvlan 对操作系统的内核版本都有⼀定的要求。内核版本⽀持不好的情况下，会遇到各种各样的 异常问题，不好排查。⽐如 macvlan 在 Linux 4.20.0 这个内核版本上，就有很多问题，⽐如： 1kernel:unregister_netdevice: waiting for eth0 to become free. Usage count = 1 ipvlan和macvlan⽅案实施遇到的容器与宿主机互访问题我⾸先考虑的⽅案，就是实施成本要低、维护简单，⽬的不变，也就是投⼊产出⽐要⾼。所以，macvlan 和 ipvlan 就是⾸选。 但是，macvlan 和 ipvlan 都有⼀个共同的特点，就是虚拟 ip 所在的⽹络，⽆法直接和宿主机⽹络互通。简单来 ⾸，利⽤这2个模型，将虚拟 ip 加⼊到容器⾥后，容器⽆法直接 ping 通宿主机。这个不是bug，⽽是这2个⽹卡虚 拟化技术，在设计之初，就是这样的。 但是，实际情况下，我们不太可能不让容器的⽹络访问宿主机的⽹络。所以，这个问题，必须要解决。 要解决这个问题，使⽤ macvlan 的话，有⼀个⽐较好的天然解决⽅案。就是使⽤ macvlan 也在宿主机上⽣成⼀个 虚拟⽹卡。这样⼀来，容器⽹络就可以和宿主机互访，操作上就是⼀⾏命令的事⼉。但是！！我们本机利⽤苹果本 虚拟机调试是没有问题的，⽤ KVM 调试，但 Pod 变化触发容器⽹络销毁和新建这种变化时，宿主机就⽆法访问容 器⽹络了，⽽且，在同⼀个宿主机上，宿主机对其内的有些容器可以访问，有些容器不能访问，⾮常奇怪，这个问 题，可能在物理机上不⼀定存在，但是在调研阶段，我们必须要考虑其在线上物理机环境也⽆法实施或者后续遇到 类似问题的情况。所以，我们尝试更换了多个操作系统的Linux内核版本，问题依然，只能另寻他法。 使⽤ ptp 优雅的解决使⽤macvlan（或ipvlan）时容器和宿主机互访问题解决互访问题，前⾯提到的⽅案是，如果使⽤macvlan可以⽤其在宿主机虚拟⼀个⽹卡出来配置⼀个和宿主机的平 ⾏⽹络IP。这个弊端前⾯提到了很重要的⼀部分，另外还有⼀部分没有提到，就是IP资源浪费的情况。相当于每个 主机都需要多⽤⼀个IP地址。 需要⼀个⽅案，可以通⽤的解决 macvlan 和 ipvlan 这种容器和宿主机⽹络⽆法互访的问题。 解决这个问题，我们可以借助 veth pair 来完成。其原理是，创建⼀对 veth pair，⼀端挂⼊容器内，⼀端挂⼊宿主 机内。如图： 如上： ①：借助 ipvlan 或 macvlan 在 em1 上创建2个虚拟⽹卡，然后分别加⼊到 container-1 和 container-2 中，容器内 ⽹卡命名为 eth0，且 IP地址 和宿主机平⾏。②：上⾯的操作完成后，容器之间可以互访，但是，容器和宿主机⽆法互访，这是 ipvlan/macvlan 的制约。③：创建 2 个 veth-pair 对，⼀对加⼊宿主机和容器 container-1 中（容器端叫 veth0，宿主机端为 veth-0x1），另 外⼀对加⼊宿主机和容器 container-2 中（容器端叫 veth0，宿主机端为 veth-0x2），然后在容器内创建指向到宿 主机⽹卡的路由表规则，于此同时，在宿主机上，也要建⽴指向到容器⽹络的路由表规则。 综上，可以完成容器和宿主机⽹络互访。 使⽤这种⽅案有⼏个有点： ①：CNI 插件使⽤ macvlan 或 ipvlan 都没有关系。②：不需要借助 macvlan 在宿主机上虚拟⽹卡出来，因为这种⽅案，不具备通⽤性，⽽且，并不稳定（在某些vm 主机的场景下）。 其实，上述⽅案的实现，便是 K8S CNI 插件 ptp 的原理。 IPAM的使⽤上⾯的操作完成后，解决了 CNI 的问题。剩下的，就是 IPAM 的问题。在整个流程中，CNI 插件主要负责⽹络的 定制，⽽ IP 如何获取，并⾮ CNI 的⼯作内容，它可以通过调⽤ IPAM 插件来完成IP的管理。 IPAM 插件有很多，最简单的，是 K8S CNI 官⽅提供的 host-local IPAM 插件：https://github.com/containernetworking/plugins 。CNI 的插件，K8S 官⽅也有提 供：https://github.com/containernetworking/cni 。 我们在实际测试过程中发现，K8S 的 CNI 插件，在 kubelet 1.13.4 版本上存在问题，报找不到 cni 名称的问题。所 以，我将 lyft 的⼀个开源项⽬ cni-ipvlan-vpc-k8s 改造了⼀下（主要是解决 ptp 插件设置默认路由规则失败导致 CNI整体调⽤失败的问题，此问题不⼀定在所有场景中存在，需要使⽤者结合⾃⼰业务来处理）。 下⾯是⼀个 CNI 配置⽂件的样例： 1//cat /etc/cni/net.d/10-maclannet.conflist 123456789101112131415161718192021222324252627282930313233&#123; &quot;name&quot;: &quot;cni0&quot;, &quot;cniVersion&quot;: &quot;0.3.1&quot;, &quot;plugins&quot;: [ &#123; &quot;nodename&quot;: &quot;k8s-node-2&quot;, &quot;name&quot;: &quot;myipvlan&quot;, &quot;type&quot;: &quot;ipvlan&quot;, &quot;debug&quot;: true, &quot;master&quot;: &quot;eth0&quot;, &quot;mode&quot;: &quot;l2&quot;, &quot;ipam&quot;: &#123; &quot;type&quot;: &quot;host-local&quot;, &quot;subnet&quot;: &quot;172.18.12.0/24&quot;, &quot;rangeStart&quot;: &quot;172.18.12.211&quot;, &quot;rangeEnd&quot;: &quot;172.18.12.230&quot;, &quot;gateway&quot;: &quot;172.18.12.1&quot;, &quot;routes&quot;: [ &#123; &quot;dst&quot;: &quot;0.0.0.0/0&quot; &#125; ] &#125; &#125;, &#123; &quot;name&quot;: &quot;ptp&quot;, &quot;type&quot;: &quot;unnumbered-ptp&quot;, &quot;hostInterface&quot;: &quot;eth0&quot;, &quot;containerInterface&quot;: &quot;veth0&quot;, &quot;ipMasq&quot;: true &#125; ]&#125; 配置⽂件说明：插件是2个，⼀个是名字为 myipvlan 的插件，类型为 ipvlan（也就意味着 kubelet 会去 CNI 插件 ⽬录找名为 ipvlan 的可执⾏⽂件），⼀个名为 unnumbered-ptp。 其中，ipvlan 插件要配置的 IP，会从 CNI 插件⽬录，寻找名为 host-local 的可执⾏⽂件来获取 IP 地址。 再次说明，ipvlan 和 unnumbered-ptp 是基于 lyft 的⼀个开源项⽬ cni-ipvlan-vpc-k8s 改造过的。]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>ipvlan</tag>
        <tag>容器与宿主机通信</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解决网卡丢包问题，以及丢包问题解决后系统网络还是慢的问题]]></title>
    <url>%2F2019%2F01%2F17%2F13%2F</url>
    <content type="text"><![CDATA[背景线上的K8S集群有一台节点，装了Istio的遥测组件，这个组件，之前在哪个节点哪个节点就出问题。出现的问题主要体现在几个点： ①：服务器间歇性 NodeNotReady②：服务器丢包极为严重③：服务器网络非常慢，以至于在服务器上执行 curl youku.com 都要响应很久。 Istio的遥测组件，是一个服务，由Istio的Ingress以及SideCar发送数据给遥测服务，而且发送的是UDP的包，量非常非常大，能达到 500Mbps。UDP包量大应该是导致了网卡丢包以及网络缓慢的主要问题，所以，要解决这个问题，就需要坐下Linux网卡收包层面的优化。开始优化前，需要先明确一下步骤： 几个基本概念 网卡相关的基础知识 网卡收包问题排查 几个基本概念NIC：NIC 是 Network Interface Card，即网络接口卡，也就是网卡。 IRQ：传统上，NIC会生成中断请求（IRQ），指示数据已到达。IRQ也是有多种的，有三种常见类型的IRQ：MSI-X，MSI和传统IRQ。 网卡相关的基础知识网卡的队列数量查看和设置网卡使用的队列数量（注意，是队列的数量，不是队列本身的大小，仅针对收包） 12// 小写 l 查看ethtool -l em2 1234567891011Channel parameters for em2:Pre-set maximums:RX: 4TX: 4Other: 0Combined: 0Current hardware settings:RX: 4TX: 1Other: 0Combined: 0 从上面看，队列有几种：RX、TX、Combined 等。有些网卡，只支持 Combined 队列（发送和接收公用，这种叫做组合队列），有些可以单独设置 RX 和 TX 队列（RX 就是接收队列，TX 就是发送队列）。 设置网卡队列的大小 12// 大写 L 为设置ethtool -L eth0 rx 8 注意： ①：并非所有网卡都支持这个操作命令，这得看网卡驱动是否支持 ethtool get_channels 操作，一般支持这个操作的是物理网卡，虚拟网卡、虚拟网桥一般不支持。②：设置队列操作，会导致网卡关闭后再次启动，所以和这个网卡相关的链接就会关闭。 网卡的队列大小查看和设置网卡的队列大小（注意是大小，不是数量） 查看网卡队列大小 12// 小写 g 为查看ethtool -g em2 1234567891011Ring parameters for em2:Pre-set maximums:RX: 2047RX Mini: 0RX Jumbo: 0TX: 511Current hardware settings:RX: 200RX Mini: 0RX Jumbo: 0TX: 511 这个设置说明，网卡本身的队列大小，支持最高 2047，目前的设置是 200。 设置网卡队列大小 12// 大写 G 为设置ethtool -G eth0 rx 4096 注意： ①：并不是所有网卡都支持通过 ethtool 查看和修改网卡队列大小。②：这个操作，同样会关闭和启动网卡，所以，和这个网卡相关的连接也会中断。 网卡的队列权重调整网卡队列的处理权重（仅针对收包） 查看网卡队列的权重 12// 小写 x 为查看ethtool -x em2 1234567891011121314151617RX flow hash indirection table for em2 with 4 RX ring(s): 0: 0 1 2 3 0 1 2 3 8: 0 1 2 3 0 1 2 3 16: 0 1 2 3 0 1 2 3 24: 0 1 2 3 0 1 2 3 32: 0 1 2 3 0 1 2 3 40: 0 1 2 3 0 1 2 3 48: 0 1 2 3 0 1 2 3 56: 0 1 2 3 0 1 2 3 64: 0 1 2 3 0 1 2 3 72: 0 1 2 3 0 1 2 3 80: 0 1 2 3 0 1 2 3 88: 0 1 2 3 0 1 2 3 96: 0 1 2 3 0 1 2 3 104: 0 1 2 3 0 1 2 3 112: 0 1 2 3 0 1 2 3 120: 0 1 2 3 0 1 2 3 说明：如上，有4个队列，队列编号是：0、1、2、3。散列值为2的数据，要发往2号队列，散列值为3的，要发往3号队列，散列值为8的，要发往0号队列。 设置网卡队列权重 12345678// 大写 X 为设置// 设置权重均分，equalethtool -X eth0 equal 2// 单独设置权重// 下面这个操作，是为网卡的0号队列设置权重为6，网卡的1号队列设置权重为2。这样一来，队列0会处理更多的数据ethtool -X eth0 weight 6 2 针对网卡队列的网卡收包的哈希策略调整网卡上收包（RX）过程用于哈希的字段（仅针对收包） 首先，对数据包哈希的意义，是用于放入不同的队列。哈希可以选择数据包中的不同字段来哈希。 查看用于哈希的字段： 12// 小写 n 查看ethtool -n eth0 rx-flow-hash udp4 123UDP over IPV4 flows use these fields for computing Hash flow key:IP SAIP DA 上面的输出说明，计算接受UDP包的哈希字段，是 IP地址的源地址和目的地址。 配置用于哈希的字段： 12// 大写的 N 为设置ethtool -N eth0 rx-flow-hash udp4 sdfn 其中 sdfn 是一个字符串，可以通过 man ethtool 来查看。 网卡 RX的 ntuple 过滤功能（仅针对收包）有一些网卡，是支持“ntuple过滤”的功能。这个功能用于过滤硬件中的传入网络数据并将其排入特定的RX队列。这个可以做到很多过滤特性，比如： ①：用户可以指定发往特定端口的TCP数据包应该发送到RX队列1。换句话说，让发往80端口的是数据包，指定到某个队列。②：让发往某个端口的数据，由某个CPU处理。 这个功能，有不同的叫法，比如说，Intel 的网卡管这个叫做 Internet Ethernet Flow Director。 举一个此功能的使用场景：提供Web服务的响应效率，步骤如下： ①：让80上运行的Web服务器固定在CPU 2上运行（也就是对进程的CPU绑定）。②：RX队列（收包队列）的IRQ（请求中断）被分配为由CPU 2处理。也就是，让某个收包队列，由某个CPU处理。③：发往端口80的TCP流量使用ntuple“过滤”到CPU 2。 正题，查看网卡是否支持 ntuple 过滤： 12// 小写 k 查看ethtool -k eth1 1234567891011121314151617181920Features for eth1:rx-checksumming: off [fixed]tx-checksumming: on tx-checksum-ipv4: off [fixed] tx-checksum-ip-generic: on tx-checksum-ipv6: off [fixed] tx-checksum-fcoe-crc: off [fixed] tx-checksum-sctp: off [fixed]scatter-gather: on tx-scatter-gather: on tx-scatter-gather-fraglist: off [fixed]tcp-segmentation-offload: on tx-tcp-segmentation: on tx-tcp-ecn-segmentation: off [fixed] tx-tcp6-segmentation: on tx-tcp-mangleid-segmentation: off....ntuple-filters: off [fixed]....hw-tc-offload: off [fixed] 如上，nutple-filters 为 off ，表示 ntuple 过滤功能没有开启。 开启 nutple 过滤： 12// 大写 K 设置ethtool -K eth0 ntuple on 查看 ntuple 过滤规则： 12// 小写 u 查看规则ethtool -u eth0 1240 RX rings availableTotal 0 rules 如上，Total 0 rules 表示此设备没有 ntuple 过滤规则。 添加 ntuple 过滤规则： 123// 大写 U 设置规则// 添加 ntuple 过滤规则，让目标端口80的TCP流发送到RX队列2ethtool -U eth0 flow-type tcp4 dst-port 80 action 2 注意：可以使用ntuple过滤在硬件级别丢弃特定流的数据包。比如，丢弃特定IP地址的传入流量。 软中断（softirq）中断简介： 中断：外部设备，需要通过中断机制（中断信号），来通知CPU处理响应的任务。这样CPU就响应中断信号对应的处理动作即可。避免CPU盲目等待某一个任务而无法处理其他任务的情况。中断是由外部设备引起的，计算机能够接受的外部信号，非常的有限，因为不可能为每一个外部的设备都定义好信号的格式，所以，计算机给外部信号，只约定了一种信号格式，这种信号就是中断信号，这套信号的接受和处理共同组成中断机制。 计算机的中断，可以看做一个过程，它在看电视，然后水烧开了（中断信号），基于计算机去处理烧水的事情。但是计算机不具备连续性，处理完了烧水的事儿，就不知道怎么回到原来的过程再去看电视。计算机要完成上面的看似具备逻辑性的事情，就需要硬件+软件协同，实现处理中断的全部过程。 中断需要尽可能的快，软中断（softirq）的意义就是，处理中断没有处理完的事情。 举例来说：比如在网卡的驱动程序里，在中断环境里，只是把包放到一个队列里，然后由软中断来把包传递给进程，或者转发包等。 Linux 里边的软中断不可重入，在一个CPU上，一次只能有一个软中断在执行。多个CPU的话，可以有多个软中断。 查看当前机器的系统中断 12345// 第一步top// 第二步按1 // si 列，就是系统中断，si 就是 softirqs 查看软中断统计信息： 1cat /proc/softirqs 1234567891011 CPU0 CPU1 CPU2 CPU3 HI: 0 0 0 0 TIMER: 2831512516 1337085411 1103326083 1423923272 NET_TX: 15774435 779806 733217 749512 NET_RX: 1671622615 1257853535 2088429526 2674732223 BLOCK: 1800253852 1466177 1791366 634534BLOCK_IOPOLL: 0 0 0 0 TASKLET: 25 0 0 0 SCHED: 2642378225 1711756029 629040543 682215771 HRTIMER: 2547911 2046898 1558136 1521176 RCU: 2056528783 4231862865 3545088730 844379888 可以看到，不同的 CPU 处理关于网卡收包的软中断次数不一样（速度不一样），并不均匀，这说明有其他的因素，影响了这个速度。 IRQ coalescing（中断合并）这个东西，允许中断前缓存数据包，如果简单来说，一个包就会触发中断，那么它可以做到10个包触发一次中断，有效降低中断次数，提高系统响应效率。 启用自适应RX / TX IRQ合并的结果是，当数据包速率较低时，将调整中断传送以改善延迟，并在数据包速率较高时提高吞吐量。 查看网卡的中断合并是否已开启 1ethtool -c eth0 1234567891011121314151617181920212223242526Coalesce parameters for eth0:Adaptive RX: off TX: offstats-block-usecs: 0sample-interval: 0pkt-rate-low: 0pkt-rate-high: 0rx-usecs: 20rx-frames: 5rx-usecs-irq: 0rx-frames-irq: 5tx-usecs: 72tx-frames: 53tx-usecs-irq: 0tx-frames-irq: 5rx-usecs-low: 0rx-frame-low: 0tx-usecs-low: 0tx-frame-low: 0rx-usecs-high: 0rx-frame-high: 0tx-usecs-high: 0tx-frame-high: 0 其中，Adaptive 显示了当前 RX 和 TX 是否开启了中断合并。 设置中断合并 1ethtool -C eth0 adaptive-rx on IRQ（中断）的CPU亲和性对于支持多队列的网卡来说，可以尝试让特定的CPU来处理NIC生成的中断。通过设置特定的CPU，可以分割将用于处理哪些IRQ的CPU。 不过，如果要调整IRQ亲和力，首先应该检查是否运行 irqbalance 守护程序。 这个守护进程尝试自动平衡IRQ到CPU，所以，在开启了 irqbalance 的情况下，它可能会覆盖你原有的设置。 如果正在运行 irqbalance，则应该禁用 irqbalance 或将 –banirq 与 IRQBALANCE_BANNED_CPUS 结合使用，或者，让 irqbalance 知道它不应该触及你想要自己分配的一组 IRQ 和 CPU 。 查看是否开启了 irqbalance 服务 1systemctl status irqbalance 如果没有开启，可以将其开启，开启后，网卡硬中断所绑定的CPU，将由 irqbalance 来自动调配。 netdev_max_backloginux 内核从网卡驱动中读取报文后可以缓存的报文数量，默认是 1000。积压在 backlog 的数据包，就是由软中断进行处理的。 调整方式： 1sysctl -w net.core.netdev_max_backlog=2000 注意：检查你网卡的驱动程序，如果它调用 netif_receive_skb 并且你没有使用 RPS，那么增加 netdev_max_backlog 将不会产生任何性能提升，因为没有数据会进入 input_pkt_queue。 套接字接收队列内存可以通过下面的方式，调整 socket 接收队列的缓冲区大小 12// 8388608 就是 8M 左右， 26214400 就是25M 左右sysctl -w net.core.rmem_max=8388608 sk_rcvbuf 从 net.core.rmem_default 值开始，所以，也可以通过设置sysctl来调整 net.core.rmem_default 1sysctl -w net.core.rmem_default=8388608 我们的应用程序，也可以调用 setsockopt 的时候，配置 SO_RCVBUF 来决定套接字接收队列的内存大小，不过，你最大能设置的值，无法超出 rmem_max （注意：这个说法不完全准确，因为可以通过调用 setsockopt 并传递 SO_RCVBUFFORCE 来覆盖net.core.rmem_max限制，但是，需要你具备 CAP_NET_ADMIN 的能力） 网卡收包排查先解决丢包问题要排查问题，说明已经存在问题，存在的问题比如，服务失败率变高（HTTP服务接口失败率高）->进而发现网卡流量太大->进而发现网卡丢包。 先从丢包问题入手，因为有些场景下，解决丢包问题即是第一步，可能这个问题解决了，服务响应问题也就解决了。 查看丢包问题 1ifconfig eth0 查看其中的 dropped 是否为0，以及是否递增。 通过下面几种操作，往往可以解决丢包问题： 123456// 增大socket缓冲区上限sysctl -w net.core.rmem_max=8388608// 增大socket缓冲区默认大小sysctl -w net.core.rmem_default=8388608// 调整 Linux 系统的网卡缓冲区大小sysctl -w net.core.netdev_max_backlog=2000 可以通过监控系统进行观测，或者ifconfig进行查看。 虽然网卡不丢包了，系统还是慢，甚至一个 curl youku.com 都能明显感觉卡顿，这种情况，就需要进一步探究。 排查网卡软中断过高的问题首先，网卡相关的事件，是会发生硬中断和软中断。 ①：硬中断，网卡收包就会触发。②：软中断，网卡收包后，系统将其后续处理，比如发往进程，或者进行包转发等。 软中断，一般是一个或者多个CPU来处理，现在很多网卡都是支持多队列网卡，这种情况，每一个队列都可能绑定一个CPU做中断处理。所以，我们先要确认一下，我们在使用的网卡，有几个队列，每个队列由哪个CPU在处理 123456789101112131415// 查看网卡 RX 的队列数量ethtool -l em1// 下面是输出，说明收包丢列（RX）有4个。Channel parameters for em1:Pre-set maximums:RX: 4TX: 4Other: 0Combined: 0Current hardware settings:RX: 4TX: 1Other: 0Combined: 0 12// 查看系统中断统计cat /proc/interrupts 找到对应的网卡，比如 eth0 网卡对应的行（有可能多行，因为网卡可能使用的是多队列），找出来其中中断次数多的那一列对应的CPU，比如我找到的是：CPU9、CPU21、CPU58、CPU68 然后，我们需要验证，是不是这几个CPU在处理中断，并且繁忙 123// 执行 top 命令，然后按1，看一下 si 列的值，是不是被动较大，或者一直比较高top // 基本上能看到，就是那几个CPU的si列，一直占用比较高 后续的处理，分2个方向： ①：为硬中断绑定CPU②：为软中断分散CPU压力 为硬中断绑定CPU（不推荐）首先，这个操作，并不推荐。因为硬中断和软中断的目的不一样，上边说过了，我们可以先从修改软中断的修改看效果，如果Ok，那就没有必要做硬中断的修改了。最主要的是，修改硬中断，需要停止系统的某个服务，而修改软中断不需要。 下面是修改硬中断的CPU绑定操作 给网卡绑定CPU了，要么手动，要么自动，要么两者结合。之所以这么说，是因为，系统可能存在 irqbalance 服务，这个服务起来后，会自动处理中断和CPU的绑定关系。我们如果手动修改，比较麻烦，手动修改的值，会被 irqbalance 冲掉。所以，我们得关闭这个服务，才能 好了，我们先执行手动操作，执行手动操作之前，先要知道，我们要操作谁。Linux 中，关于中断与CPU亲和性绑定关系的，在 proc/irq/ 目录。每一个子目录，都是一个 中断信号的编号。所以，我们要知道网卡中断对应的编号是什么，并进去修改。所以，第一步，我们如何知道网卡软中断的信号编号是什么 首先，停止 irqbalance 服务 1systemctl stop irqbalance 然后，查看网卡对应的中断信号 12// 还是这个操作，看统计，找到网卡，最前面的那一列，就是中断信号！cat /proc/interrupts 比如，我们拿到的是 120、121、122、123 。 然后，修改中断信号和CPU的绑定关系 1234echo 9 &gt; /proc/irq/120/smp_affinity_listecho 10 &gt; /proc/irq/121/smp_affinity_listecho 11 &gt; /proc/irq/122/smp_affinity_listecho 12 &gt; /proc/irq/123/smp_affinity_list 为软中断分散CPU压力修改硬中断其实并不够彻底，因为硬中的处理虽然分散到了上面几个核，但是硬中断之后，软中断才能开始工作，且，也在同一个核上，一个核即做硬中断处理，也做软中断处理，还是会大大降低硬中断和软中断的处理速度。 通过设置 rps 以及 rfs ，可以将软中断，均匀分配到很多个CPU的核上。 12345// 让em1每个队列的处理，均分到32个CPU核上。echo ffffffff &gt; /sys/class/net/em1/queues/rx-0/rps_cpusecho ffffffff &gt; /sys/class/net/em1/queues/rx-1/rps_cpusecho ffffffff &gt; /sys/class/net/em1/queues/rx-2/rps_cpusecho ffffffff &gt; /sys/class/net/em1/queues/rx-3/rps_cpus 解释： ①：为什么是8个f，一个f，是4个1，1111，8个f，就是32个1，就是代表32个核。②：为什么是 rx-0 到 rx-4，因为上面说过了，这个网卡，有4个队列。③：为什么不设置更多个核，是因为设置多了，系统会终止这个操作：“bash: echo: write error: Value too large for defined data type”说明： 收尾经过上面的操作，网卡丢包问题基本解决，另外，系统网络缓慢问题也解决。 上面的过程，也并非所有的内核参数都尝试做了调整，也有一些并没有调，总结来说，对 netdev_max_backlog 以及 rmem_max 的调整，可以有效的减少丢包。通过调节软中断并对其分散到多个CPU来减少CPU的中断压力，可以有效提高系统网络的RX效率。 另外，前面提到，有些网卡支持 “中断合并” 特性，这个特定对于缓解中断压力有好处，并且默认情况下这个特性是关闭的，对这个服务器的网卡，设置 “中断合并” 操作，并没有生效，设置完成后也不报错，通过 ethtool -C eth0 也依然看到是 Off 状态。 文章参考 https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/#irqs http://ju.outofmemory.cn/entry/104879 https://www.cnblogs.com/shenlinken/p/6657931.html?utm_source=itdadao&amp;utm_medium=referral]]></content>
      <categories>
        <category>udp dropped</category>
        <category>istio</category>
      </categories>
      <tags>
        <tag>udp dropped</tag>
        <tag>istio</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 虚拟化之：Net Namespace 下的网络通信]]></title>
    <url>%2F2018%2F12%2F21%2F12%2F</url>
    <content type="text"><![CDATA[总览Linux 的虚拟化技术之一，就是 Namespace，其中 Net Namespace 是网络命名空间，这个在 Docker 里用的比较多，这篇文章主要想做的是，如何模拟的一个跨 Namespace 的通信过程。 提一点：跨Namespace，可以是多个 Net Namespace 之间，也可以是 宿主机和某一个 Net Namespace。我们要模拟的是： ①：只有一个自定义的网络命名空间（名为 blue ）。②：在宿主机网络和 blue 命名空间网络 两者上，分别建立虚拟网卡，并设置不同IP，进行通信（比如 ping，比如 curl）③：尝试在名为 blue 的网络命名空间里启动 http 服务，在宿主机上网络上，访问 blue 空间里的这个 http 服务。 所以，我们需要完成以下几个步骤： 一、创建新命名空间二、为新命名空间 blue ，添加网卡三、打通宿主机命名空间和新命名空间之间的通信四、在 blue 网络命名空间下，创建 http 服务，宿主机访问。 实施步骤创建名为 blue 的网络命名空间1ip netns add blue ip 工具集非常强大，即可处理网络命名空间，也可以处理网络设备。 为新命名空间，添加网卡创建 veth pairveth pair，主要就是用来处理不同命名空间通信用的（确切的说，是 Linux 虚拟化技术的产物之一，目前在容器化上用的比较多），veth 其实是 virtual ethernet 的缩写（虚拟以太网卡）。而 veth pair，就是一次性创建 一对 虚拟网卡，它们成对出现，向 veth pair 的一端发送的数据，经过操作系统内核后，可以通过另外一端读取到数据。 veth pair 在不同的命名空间使用如下： 1、创建 veth pair 1ip link add vethtest01 type veth peer name vethest002 2、查看 veth pair 1ip link list 12345// 输出大概如下：18: vethest002@vethtest01: &lt;BROADCAST,MULTICAST,M-DOWN&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT qlen 1000 link/ether b2:13:25:45:40:89 brd ff:ff:ff:ff:ff:ff19: vethtest01@vethest002: &lt;BROADCAST,MULTICAST,M-DOWN&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT qlen 1000 link/ether a6:41:82:ca:4c:fd brd ff:ff:ff:ff:ff:ff 注意：1、ip link list 里，这个 link 指的就是网络设备，感觉叫 link 不太理解，可以通过 man ip 手册，找到 link 部分的说明。2、其实我们通过 ifconfig 也可以看到，不过需要加 -a，即：ifconfig -a。ifconfig 默认只能看到已up的网络设备。 将 veh pair 的一端，添加命名空间，并处理好网卡设置1、添加网卡到 blue 网络命名空间 1ip link set vethest002 netns blue 添加完成后，宿主机的环境中，已经无法通过 ip link list 看到 vethest002 这个网卡设备了，因为它已经到 blue 网络命名空间里去了。 2、去到查看 blue 网络命名空间内，查看网卡设备列表： 1ip netns exec blue ip link list 12345// 输出：1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN mode DEFAULT qlen 1 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:0018: vethest002@if19: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT qlen 1000 link/ether b2:13:25:45:40:89 brd ff:ff:ff:ff:ff:ff link-netnsid 0 3、为 blue 里的网卡，配置网卡接口 现在，blue 网络空间下，有 vethest002 设备，但这个设备第一，它没启动，第二，它没有设置网络IP信息，我们做一下这个处理： 1ip netns exec blue ifconfig vethest002 10.1.1.2/16 up 注意：我这里设置的是 10.1.1.2/16，这里设置为16，而不是24，是因为，一会儿我要为宿主机网络的 veth pair 另一端 vethest001 设置另外一个网络段 10.1.2.2/16，这样，他俩才是在同一个网段。 再次查看 blue 网络命名空间内的网卡接口： 1ip netns exec blue ifconfig 12345678vethest002: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 10.1.1.2 netmask 255.255.0.0 broadcast 10.1.255.255 inet6 fe80::61:ff:feb0:f65f prefixlen 64 scopeid 0x20&lt;link&gt; ether 02:61:00:b0:f6:5f txqueuelen 1000 (Ethernet) RX packets 25 bytes 1911 (1.8 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 25 bytes 1859 (1.8 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 通过上面的命令，可以看到，我们可以在某个命名空间内执行某个命令，比如添加网卡接口、查看网卡列表，都是通过 ip netns exec 来做的，所以，我们也可以在某个网络命名空间下开启一个 shell，此 shell 中执行的所有操作，都会在这个网络命名空间下。 123// 比如在某个命名空间下，开启一个 shell，去执行其他命令。// 这个命令可以不执行，只是一个例子而已，但如果你执行了，记得退出来，否则你的网络操作都是在这里。ip netns exec blue /bin/bash 打通宿主机命名空间和新命名空间之间的通信bridge 概述：我们已经为新的网络命名空间 blue 做了配置，但现在这个命名空间无法连通宿主机的网络。 我们需将通宿主机网络和 blue 网络命名的网络打通，就需要引入一个新的工具：bridge。 ①：bridge，是虚拟网络设备，可以为 bridge 配置网络设备的特征，比如：IP、MAC等。 ②：bridge 也是虚拟交换机，具备和物理交换机的类似功能。 bridge的特殊性：普通网络设备，一般有2个端，发送端和接收端。比如网卡，从外网接收数据，转到内核，或者从内核接收数据，再转发出去。但是，bridge 不同，它可以有很多的端口，数据可以从任意的一端进来，至于进来之后，要从哪里出去，就和物理交换机差不多，看目的地址了，bridge 可以正确的进行路由。 在开始配置 bridge 之前，我们可以看一下，在 K8S flannel（backend为overlay网络vxlan）这种网络模型下的 bridge 示例： 1234567891011121314151617181920[root@sja22 /]# brctl showbridge name bridge id STP enabled interfacescni0 8000.0a580ae94101 no veth002d4c2b veth02267e47 veth063995d0 veth0ac99849 veth0e651a57 veth13d7ac2c veth141294eb veth196c3d37 veth1be51bad veth1c17df21 veth2036d552 veth204659c2 veth22261c72 veth23ffa84b veth24b8dbe2 veth2b24c23e veth2bebd32bdocker_gwbridge 8000.02424ebd5efe no veth550eff2 如上，可以看到，有一个名为 cni0 的 bridge，下面挂了一堆的 veth pair。 用 bridge 打通不同网络命名空间的原来图（图是摘的），大致如下： 总得来说有几个关键点： ①：得有一个网桥。②：创建一对（或者多对）veth pair，取决于你想让几个 Namespace 网络进行通信。③：veth pair 的一端，要接入 Namespace里，另外一端，接到 bridge 上。④：为 Namespace 里的 veth pair 设备设置 IP ，并启动。⑤：为 网桥配置 IP（这个 IP 必须和 Namespace 那头的 IP 在同一个网段），并启动。 创建 bridge 并连接 veth pair 的一端1、创建一个 bridge（网桥） 创建 bridge 有2种方式，用到的命令也不同，最基本的，就是 ip 命令 方式1：ip 命令创建和操作 bridge 1ip link add name test-bridge type bridge 这个命令执行后，无法通过 ifconfig 看到这个设备，因为这个设备没有启动，所以需要启动 bridge 设备： 1ip link set test-bridge up 还是用的 ip link 操作，所以，其实 bridge 也是一个网络设备，和 veth pair 是同一个层级的东西。 此命令执行后，可以通过 ifconfig 看到设备 123456789101112131415161718192021222324252627[root@192-168-20-143 ~]# ifconfig ens33: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 192.168.20.143 netmask 255.255.255.0 broadcast 192.168.20.255 inet6 fe80::4873:514:f637:51d7 prefixlen 64 scopeid 0x20&lt;link&gt; ether 00:0c:29:22:41:78 txqueuelen 1000 (Ethernet) RX packets 4226 bytes 444256 (433.8 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 42796 bytes 3718608 (3.5 MiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10&lt;host&gt; loop txqueuelen 1 (Local Loopback) RX packets 1869903 bytes 259940119 (247.8 MiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 1869903 bytes 259940119 (247.8 MiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0test-bridge: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet6 fe80::d045:68ff:fe56:5a05 prefixlen 64 scopeid 0x20&lt;link&gt; ether d2:45:68:56:5a:05 txqueuelen 1000 (Ethernet) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 5 bytes 418 (418.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 方式2：brctl 1yum -y install bridge-utils 1brctl addbr test-bridge up 同样，上面的操作后，无法通过 ifconfig 看到此网桥设备，因为此设备还没有启动，还是需要手动启动 注意：新创建的 bridge 除了一端连接了网络协议栈外，啥都没有，所以，我们需要将 bridge 和 veth pair 连接起来。 2、将 bridge 和 veth pair 连接 我们之前，已经将创建的 veth pair 的一端（vethest002），连接到 blue 网络命名空间内，并配置好 IP 了，宿主机上还有 veh pair 另外一端（vethest001），我们要把它接入刚才创建的网桥上。 1ip link set dev vethtest01 master test-bridge 3、为网桥配置IP地址并做连通测试 1ifconfig test-bridge 10.1.2.2/16 up 经过上面的操作，其实我们已经可以连通这2个网络了。我们在宿主机网络，ping blue 命名空间的网络空间的 IP 明确一点：宿主机网桥IP是 10.1.2.2，网络命名空间blue内的是 10.1.1.2 12345[root@test ~]# ping 10.1.1.2PING 10.1.1.2 (10.1.1.2) 56(84) bytes of data.64 bytes from 10.1.1.2: icmp_seq=1 ttl=64 time=0.327 ms64 bytes from 10.1.1.2: icmp_seq=2 ttl=64 time=0.085 ms64 bytes from 10.1.1.2: icmp_seq=3 ttl=64 time=0.056 ms 如上，网络已经连通。 在 blue 网络命名空间下，创建 http 服务，宿主机访问先创建一个简单的HTTP服务我们直接编译一个非常简单的，基于 C 语言的 HTTP 服务就行，所以需要准备下 1yum -y install gcc 1、创建 http_server.c 文件，写入如下内容： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071#include &lt;stdio.h&gt;#include &lt;netinet/in.h&gt;#include &lt;arpa/inet.h&gt;#include &lt;unistd.h&gt;#include &lt;string.h&gt;#include &lt;sys/socket.h&gt;#include &lt;pthread.h&gt;#include &lt;stdlib.h&gt; int startup()&#123; int sock = socket(AF_INET,SOCK_STREAM,0); if(sock &lt; 0) &#123; exit(1);//退出进程 &#125; int opt = 1; setsockopt(sock,SOL_SOCKET,SO_REUSEADDR,&amp;opt,sizeof(opt)); struct sockaddr_in local; local.sin_family = AF_INET; local.sin_addr.s_addr = htonl(INADDR_ANY); local.sin_port = htons(8080); // 指定固定端口 int ret = bind(sock,(struct sockaddr *)&amp;local,sizeof(local)); if( ret &lt; 0 ) &#123; exit(2); &#125; if( listen(sock,5) &lt; 0 ) &#123; exit(3); &#125; return sock;&#125;void* handler_request(void * arg)&#123; int sock = (int)arg; char buf[4896]; ssize_t s = read(sock,buf,sizeof(buf)-1); if( s &gt; 0 ) &#123; buf[s] = 0; printf(" %s ",buf); const char *echo_str = "HTTP/1.0 200 ok\n\n&lt;html&gt;&lt;h1&gt;Welcome to my http server!&lt;/h1&gt;&lt;html&gt;\n"; write(sock,echo_str,strlen(echo_str)); &#125; close(sock);&#125;int main()&#123; int listen_sock = startup(); while(1) &#123; struct sockaddr_in client; socklen_t len = sizeof(client); int sock = accept(listen_sock,(struct sockaddr*)&amp;client,&amp;len); if(sock &lt; 0) &#123; continue; &#125; pthread_t tid; pthread_create(&amp;tid,NULL,handler_request,(void *)sock); pthread_detach(tid); &#125; return 0; &#125; 2、创建 Makefile 文件，写入如下内容 123456.PHONY:all cleanall:http_serverhttp_server:http_server.c gcc $^ -o $@ -lpthreadclean: rm -rf http_server 编译出二进制文件 1make 可以看到，编译出一个可执行文件：http_server 3、在 blue 网络命名空间下，启动这个 server 12ip netns exec blue /bin/bash./http_server 在这个网络下启动的这个 server 监听 8080 端口，也意味着，我们在宿主机，理论上可以通过 curl 访问这个服务。 4、新开一个窗口，访问 blue 网络内的 8080 HTTP 服务 12[root@test ~]# curl 10.1.1.2:8080&lt;html&gt;&lt;h1&gt;Welcome to my http server!&lt;/h1&gt;&lt;html&gt; 参考 https://blog.csdn.net/sld880311/article/details/77650937 https://blog.csdn.net/qq_37941471/article/details/80789725 https://segmentfault.com/a/1190000009491002]]></content>
      <categories>
        <category>linux</category>
        <category>namespace</category>
        <category>bridge</category>
        <category>veth pair</category>
        <category>docker</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>namespace</tag>
        <tag>bridge</tag>
        <tag>veth pair</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K8S 如何使用具有配额限制的 ceph 共享存储]]></title>
    <url>%2F2018%2F12%2F18%2F11%2F</url>
    <content type="text"><![CDATA[前言 首先，你得有个 Ceph 集群，而这个集群具体怎么搭建，这里不展开说，有很多文章讲怎么搭建，至于其中方便点的部署方式，可以尝试使用 ceph-deploy。这里主要说的是，K8S 怎么使用 Ceph 存储，但经过一番探索，发现，K8S 支持 Ceph 共享存储这个事儿虽然不是很复杂，但问题是，想让 K8S 支持带配额能力的 Ceph 还是有那么一点麻烦的。 K8S 在使用共享存储时，其a支持很多 backend，比如 GlusterFS、Ceph，这2个比较典型。之所以选择用 Ceph ，主要有几个方面： ①：Ceph 目前更主流，更多主流大厂在使用，市场更好一些。②：整体上看，性能上可能 Ceph 更好一些（这个可能仁者见仁智者见智，性能调优之后也不好说，而且他俩的适用场景本身就有一些区别，比如 GlusterFS 更适合存储大文件，但也有 GluterFS 对小文件存储的优化方式） 其实具体用哪个存储，还是要考后边的团队，对什么存储比较熟，更有掌控力。 另外，有些文章可能提到 CephFS（Ceph 文件存储）还不能用到生产环境，而且文章还是18年中旬，但很多英文文章都在17年提及，CephFS 早就 Production Ready 了。在生产环境使用应该问题不大，不过具体能不能在生产环境使用，可能还需要摸索一番。 K8S 如何使用共享存储 那么，K8S 在对分布式存储的支持上，主要有2种方式来做： ①：静态方式：PVC + PV②：动态方式：PVC + StorageClass 静态方式：PV + PVCPV 其实就是持久化Volume，它是资源的定义，比如说，存储空间多大、存储类型、存储后端等，它侧重于资源本身。 PVC 是一个声明，声明自己需要多少资源，声明需要多少资源，以及访问模式等，它侧重于“需求”。 在 K8S 中，我们可以为 POD 设置 ResourceRequest，比如 CPU、内存等，然后 K8S 为其匹配合适的 Node 节点，调度过去。同理，K8S 的 PVC 也是 ResourceRequest，只是 K8S 为其匹配的目标是 PV 而已。有一点不同，K8S 为 PVC 匹配得到 PV 后，会做一个绑定，这就意味着，一个 PVC，绑定一个 PV 后，PV 的状态就会变为 Bound（束缚），两者绑定后，不会再与其他资源匹配。 使用 PV 和 PVC 模式，需要注意的是 PV。PVC很简单，但 PV 有点麻烦，其中涉及很多内容，以一个 PV 示例来说： 1234567891011121314151617181920212223242526272829303132apiVersion: v1kind: PersistentVolumemetadata: finalizers: - kubernetes.io/pv-protection name: pvc-21f06e44-fdfe-11e8-be0a-801844e392e8spec: accessModes: - ReadWriteMany capacity: storage: 500Mi cephfs: monitors: - 172.18.12.235:6789 - 172.18.12.236:6789 - 172.18.12.237:6789 path: /volumes/kubernetes/kubernetes/kubernetes-dynamic-pvc-21f5369d-fdfe-11e8-91a2-82b342c34790 secretRef: name: ceph-kubernetes-dynamic-user-21f536d3-fdfe-11e8-91a2-82b342c34790-secret namespace: cephfs user: kubernetes-dynamic-user-21f536d3-fdfe-11e8-91a2-82b342c34790 claimRef: apiVersion: v1 kind: PersistentVolumeClaim name: pvc-1 namespace: cephfs resourceVersion: "19338037" uid: 21f06e44-fdfe-11e8-be0a-801844e392e8 mountOptions: - --client-quota persistentVolumeReclaimPolicy: Delete storageClassName: cephfs PVC 示例如下： 1234567891011121314151617181920apiVersion: v1kind: PersistentVolumeClaimmetadata: finalizers: - kubernetes.io/pvc-protection name: pvc-1 namespace: cephfsspec: accessModes: - ReadWriteMany resources: requests: storage: 500Mi volumeName: pvc-21f06e44-fdfe-11e8-be0a-801844e392e8status: accessModes: - ReadWriteMany capacity: storage: 500Mi phase: Bound 其中的 PV 里的 path 内容，需要我们提前去分布式存储系统上创建好。包括配额，也需要先做好才行。 这就有点麻烦了，我们每次创建一个 PV 之前，需要先在分布式存储系统，创建好path。但频繁这么去做，成本是有点大。所以，K8S 也有一个动态方式。 注意：PVC 有命名空间、PV 没有命名空间。 动态方式：PVC + StorageClass动态的方式，需要借助 PVC + StorageClass ，PVC 不用说了，主要说一下 StorageClass。先来看一个 StorageClass 示例： 1234567891011121314apiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: cephfsmountOptions:- --client-quotaparameters: adminId: admin adminSecretName: ceph-secret-admin adminSecretNamespace: cephfs claimRoot: /volumes/kubernetes monitors: 172.18.12.235:6789,172.18.12.236:6789,172.18.12.237:6789provisioner: ceph.com/cephfsreclaimPolicy: Delete StorageClass 定义了后端存储系统的：类型、 servers、根目录、回收策略等。 我们在创建了 PVC（持久化Volume资源声明）后，K8S 会根据 PVC 和 和 StorageClass，动态创建 PV，然后动态自行去分布式存储系统进行创建目录操作，然后进行挂载，最终映射到 POD 内。用动态方式的好处是，我们只关心资源需求 PVC 就可以了，非常方便。 但是：目前 K8S所有版本（截止到 v1.12.3），对动态方式的支持，还不全面。仅支持一些分布式存储，比如： Volume Plugin Internal Provisioner Config Example CephFS - - Glusterfs ✔️ Glusterfs 注意：Ceph 和 CephFS 不一样，Ceph 是整个服务，而 CephFS 仅仅是 Ceph 的文件存储而已。Ceph 除了文件存储 CephFS 外，还支持 对象存储、块存储。 从上面来看，K8S 根本不支持 CephFS 的 动态管理文件存储支持，所以，要么，我们需要寻找一种使其支持的方式，要么，转投 GlusterFS。我们要做的，就是寻找方式，动态管理 CephFS 的文件存储。 让K8S支持动态管理CephFS（并支持配额） 这里要提一下 K8S 能够支持动态存储管理的简单原理了。 要做到动态存储管理，就意味着2点： ①：用户只需要创建 PVC 资源。②：K8S 需要监听 PVC 资源的变化，动态创建 PV。③：用户需要在 POD 中使用 PVC 资源。④：K8S 需要能够在宿主机创建 Volume。 安装 cephfs-provisioner其实，在 Kubernetes incubator （K8S 孵化器组）中，专门有一个外部存储支持的项目：https://github.com/kubernetes-incubator/external-storage/ 。这个项目中，有关于 Ceph 动态存储管理的方式，换句话说，这不是官方原装的，所以，我们需要安装它。 在 K8S 中，安装这个项目比较简单，这个项目中，也给出了 deploy 示例。总的来说，需要单独创建 ServiceAccount、Role、ClusterRole、RoleBinding、ClusterRoleBinding 等资源。 其中 ClusterRole 主要用来获取 K8S 监听和操作 PVC、PV 资源的权限、Role 是获取 Secret 资源的权限，因为 K8S 要操作 Ceph 集群，就需要认证信息，而认证信息就是放到 Secret 资源中的。下面是具体的创建内容： ①：首先是 Secret 资源，这个 CephFS 集群的访问权限（文件名如：ceph-secret-admin.yaml ），这个数据，需要你先去 CephFS 集群，通过下面方式获取： 1ceph auth get-key client.admin | base64 然后，将上面的结果，放到下面的 key 的内容中： 12345678apiVersion: v1kind: Secretmetadata: name: ceph-secret-admin namespace: cephfstype: "kubernetes.io/rbd" data: key: QVFDbmhBNWNJSmUvSUxxs32323WC9LZUJJR3EzdnpkOTkrVmc9PQ== ②：创建 ClusterRole 资源，clusterrolebinding.yaml 123456789101112kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: cephfs-provisionersubjects: - kind: ServiceAccount name: cephfs-provisioner namespace: cephfsroleRef: kind: ClusterRole name: cephfs-provisioner apiGroup: rbac.authorization.k8s.io ③：创建 ClusterRolebinding 资源，clusterrolebinding.yaml 123456789101112kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: cephfs-provisionersubjects: - kind: ServiceAccount name: cephfs-provisioner namespace: cephfsroleRef: kind: ClusterRole name: cephfs-provisioner apiGroup: rbac.authorization.k8s.io ④：创建 Role 资源，role.yaml 123456789101112apiVersion: rbac.authorization.k8s.io/v1kind: Rolemetadata: name: cephfs-provisioner namespace: cephfsrules: - apiGroups: [""] resources: ["secrets"] verbs: ["create", "get", "delete"] - apiGroups: [""] resources: ["endpoints"] verbs: ["get", "list", "watch", "create", "update", "patch"] ⑤：创建 Rolebinding 资源，rolebinding.yaml 123456789101112apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata: name: cephfs-provisioner namespace: cephfsroleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: cephfs-provisionersubjects:- kind: ServiceAccount name: cephfs-provisioner ⑥：创建 ServiceAccount 资源，serviceaccount.yaml 123456789101112131415161718apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata: name: cephfs-provisioner namespace: cephfsroleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: cephfs-provisionersubjects:- kind: ServiceAccount name: cephfs-provisioner[root@node008037 dynimcpv]# cat serviceaccount.yamlapiVersion: v1kind: ServiceAccountmetadata: name: cephfs-provisioner namespace: cephfs ⑦：创建 StorageClass 资源，storageClass.yaml 1234567891011121314kind: StorageClassapiVersion: storage.k8s.io/v1metadata: name: cephfs namespace: cephfsprovisioner: ceph.com/cephfsmountOptions:- "--client-quota"parameters: monitors: 172.18.12.235:6789,172.18.12.236:6789,172.18.12.237:6789 adminId: admin adminSecretName: ceph-secret-admin adminSecretNamespace: cephfs claimRoot: /volumes/kubernetes ⑧：创建 cephfs-provisioner 的 Deployment 资源，cephfs-provisioner-deployment.yaml 1234567891011121314151617181920212223242526apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: cephfs-provisioner namespace: cephfsspec: replicas: 1 strategy: type: Recreate template: metadata: labels: app: cephfs-provisioner spec: containers: - name: cephfs-provisioner image: "quay.io/external_storage/cephfs-provisioner:latest" env: - name: PROVISIONER_NAME value: ceph.com/cephfs command: - "/usr/local/bin/cephfs-provisioner" args: - "-id=cephfs-provisioner-1" - "-enable-quota=true" serviceAccount: cephfs-provisioner 好了，执行下面的命令，进行资源创建： 12345// 创建 Namespacekubectl create ns cephfs// 依次执行下面命令，创建如上资源kubectl apply -f 文件名 如果如上操作后，cephfs-provisioner 就已经安装好了。上面的文件比较多，其中有几个需要注意的地方： cephfs-provisioner Deployment 的 args 参数中，有一个 -enable-quota=true ，这个不写的话，是无法做到磁盘配额的。 创建完成后，在 cephfs 命令空间下，会有 cephfs-provisioner 的实例： 123[root@nodex test]# kubectl get pods -n cephfsNAME READY STATUS RESTARTS AGEcephfs-provisioner-6f77cd58b4-75hll 1/1 Running 0 16h 测试CephFS动态管理上面准备工作 cephfs-provisioner 已经安装好，下面就需要测试了，我们测试，其实仅需要创建2个资源，一个是 PVC、一个是 Deployment 的 POD 资源。 PVC 资源：pvc.yaml 12345678910111213kind: PersistentVolumeClaimapiVersion: v1metadata: name: pvc-1 namespace: cephfs annotations: volume.beta.kubernetes.io/storage-class: "cephfs"spec: accessModes: - ReadWriteMany resources: requests: storage: 500Mi Deployment 资源：deploy.yaml 1234567891011121314151617181920212223apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nginx-use-rbd namespace: cephfsspec: replicas: 1 template: metadata: labels: app: nginx spec: containers: - image: nginx:1.13 imagePullPolicy: IfNotPresent name: nginx-use-rbd volumeMounts: - mountPath: /test-pvc name: cephfs-pvc volumes: - name: cephfs-pvc persistentVolumeClaim: claimName: pvc-1 然后，分别用 kubectl apply -f 文件名 创建即可。 问题处理 创建后，我们需要观察在 cephfs 命名空间下，有没有自动创建出来 pv。 1kubectl get pv -n cephfs -w 如果自动创建，且后期自动完成了匹配，且 nginx POD 创建后变为 Running ，则表示一切正常。但往往事实并不如人意。我们可能遇到下面几个问题： ①：PV 没有自动创建出来。②：我们使用了共享存储的 nginx 示例 POD 一直是 ContainerCreating 状态。 PV 无法自动创建问题如果是 PV 没自动创建出来 ，说明 cephfs-provisioner 有问题，它没能创建好相关的资源，我们主要看它的日志信息 1kubectl logs -n cephfs cephfs-provisioner-6f77cd58b4-75hll 通常来说，PV 没创建出来最大的可能就是出在 cephfs-provisioner-6f77cd58b4-75hll 这个 POD 上，其实这个 POD 做的工作主要有几个： ①：连接 CephFS 集群②：创建 Volume③：设置配额属性，setxattr④：创建 PV 而这个 cephfs-provisioner 本身操作 CephFS 集群的方式，其实是通过 Golang，调用的 Python 脚本来做的这个事情。如果观察其 log ，出错在 setxattr 部分，则最大的可能性，就是版本问题。 cephfs-provisioner 处理方式如下： 官方的镜像，在我们之前的 cephfs-provisioner-deployment.yaml 中，其实它使用的 Ceph 的版本，默认是 mimic ，可以从这里看：https://github.com/kubernetes-incubator/external-storage/blob/master/ceph/cephfs/Dockerfile ，而我们的 Ceph 版本，是 hammer，而官方并没有提供 hammer 版本的 cephfs-provisioner，所以，我们只能自己来构建。构建步骤如下： 克隆 https://github.com/kubernetes-incubator/external-storage 项目。 进入 ceph/cephfs 目录下 修改 Dockerfile ，将 CEPH_VERSION 里的 mimic ，改为 hammer 。 修改 Makefile，将 REGISTRY 里的地址，改为私有镜像仓库地址，目的是推送镜像使用。 执行 make &amp;&amp; make push 。 完成后，更改 deploy.yaml 里 image 为新镜像地址，然后执行 kubectl apply -f deploy.yaml 完成后，观测 cephfs-provisioner POD 正常启动，同时看到 PV 自动被创建出来了。 POD持续ContainerCreating状态处理这种问题，很有可能是 POD 使用了 PVC 形式的 Volume，而这个 Volume 却迟迟无法创建出来，所以，这时候，可以通过 kubectl describe 来看具体原因 1kubectl describe pod -n cephfs nginx-use-rbd-dcfb58dc6-8f7gj 123[root@node008037 glusterfs]# kubectl get pods -n test -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODEnginx-rbd-tttttt-77bc5d46b7-5w294 0/1 ContainerCreating 0 9m &lt;none&gt; node012044 &lt;none&gt; 如果通过结果能看到相关 Volume 信息，且在不断输出重试内容，则大概能说明是 Volume 无法创建，进而无法挂载到容器内部导致的 1234567Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 9m default-scheduler Successfully assigned test/nginx-rbd-tttttt-77bc5d46b7-5w294 to node012044 Normal SuccessfulAttachVolume 9m attachdetach-controller AttachVolume.Attach succeeded for volume "pvc-c7c9dcce-0298-11e9-be0a-801844e392e8" Warning FailedMount 1m (x11 over 7m) kubelet, node012044 MountVolume.WaitForAttach failed for volume "pvc-c7c9dcce-0298-11e9-be0a-801844e392e8" : fail to check rbd image status with: (executable file not found in $PATH), rbd output: () Warning FailedMount 34s (x4 over 7m) kubelet, node012044 Unable to mount volumes for pod "nginx-rbd-tttttt-77bc5d46b7-5w294_test(4c970495-0299-11e9-be0a-801844e392e8)": timeout expired waiting for volumes to attach or mount for pod "test"/"nginx-rbd-tttttt-77bc5d46b7-5w294". list of unmounted volumes=[cephfs-pvc]. list of unattached volumes=[cephfs-pvc default-token-89ltk] 这种情况其实是宿主机缺少 ceph 的 client 工具，可以通过下面的方式解决： 123// centos 系统yum -y install ceph-commonyum -y install ceph-fuse 安装结束后，k8s 会自动尝试重新挂载。 参考 https://kubernetes.io/docs/concepts/storage/persistent-volumes/ https://blog.csdn.net/xudawenfighting/article/details/80125389 https://www.cnblogs.com/hukey/p/8323853.html https://www.cnblogs.com/yangxiaoyi/p/7795274.html http://dockone.io/article/558 https://www.cnblogs.com/yswenli/p/7234579.html https://www.jianshu.com/p/a4e1ba361cc9 https://www.cnblogs.com/ltxdzh/p/9173706.html https://baijiahao.baidu.com/s?id=1612194635156434300&amp;wfr=spider&amp;for=pc http://blog.chinaunix.net/uid-22166872-id-4959819.html https://www.gluster.org/glusterfs-vs-ceph/ http://www.sysnote.org/]]></content>
      <categories>
        <category>cephr</category>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CoreDNS系列2：KubeDNS 架构组成及实现原理]]></title>
    <url>%2F2018%2F11%2F22%2F10%2F</url>
    <content type="text"><![CDATA[KubeDNS简述KubeDNS，是K8S官方推荐的DNS解析组件之一，从 K8S 1.11 开始，K8S 已经使用 CoreDNS，替换 KubeDNS 来充当其 DNS 解析的重任。 KubeDNS的前身，是 skyDNS，这个组件，会把 DNS 的解析记录等数据，存储到 K8S 所使用的 etcd 集群中，我们这里不讨论这个，仅讨论现今的 KubeDNS，KubeDNS，并不把 DNS 的解析规则存储到 etcd，而是放到进程的内存中，当 KubeDNS 的服务 POD 重启后，会重建一遍 DNS 规则到内存。 在前篇，我们已经了解了 K8S 中，DNS 解析的原理，本篇，我们侧重 KubeDNS 本身。包含下面几个点： KubeDNS 服务，包含了哪些组件，职责是什么？ KubeDNS 是如何区分 K8S 内部域名还是外部域名的？ KubeDNS 如何配置上游DNS服务器？ KubeDNS 如何配置自定义域名解析？ KubeDNS 解析 K8S 内部域名的的实现原理是什么？ KubeDNS 如何做弹性扩缩容？ KubeDNS组件构成在 K8S 中，KubeDNS 的实例是 POD，配置一个 KubeDNS 的 Service，对 KubeDNS 的 POD 进行匹配。在 K8S 的 其他 POD 中，使用这个 Service 的 IP 地址，作为 /etc/resolv.conf 里 nameserver 的地址，从而达到 POD 里使用 KubeDNS 的目的。这是 K8S 的默认行为，我们不需要手动干预。 其实严格来说，是 Service 匹配 Endpoint，因为 POD 创建之后可能会有IP，但此IP可能是一个 POD 非 完全Ready 状态下的 IP，理论上，这种 IP 是无无法提供服务的，所以，说 Service 匹配 Endpoint 更合适一些。 然而，KubeDNS POD 是由 Deployment 控制启动的，POD 中，并非只有一个容器。KubeDNS 的配置，是使用的 ConfigMap，总的来看，有2个主要资源： 一个标准 KubeDNS 的 Deployment 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189apiVersion: extensions/v1beta1kind: Deploymentmetadata: labels: addonmanager.kubernetes.io/mode: Reconcile k8s-app: kube-dns kubernetes.io/cluster-service: &quot;true&quot; name: kube-dns namespace: kube-systemspec: progressDeadlineSeconds: 600 replicas: 2 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: kube-dns strategy: rollingUpdate: maxSurge: 10% maxUnavailable: 0 type: RollingUpdate template: metadata: annotations: scheduler.alpha.kubernetes.io/critical-pod: &quot;&quot; creationTimestamp: null labels: k8s-app: kube-dns spec: affinity: nodeAffinity: preferredDuringSchedulingIgnoredDuringExecution: - preference: matchExpressions: - key: node-role.kubernetes.io/master operator: In values: - &quot;true&quot; weight: 100 podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchLabels: k8s-app: kube-dns topologyKey: kubernetes.io/hostname containers: - args: - --domain=cluster.local. - --dns-port=10053 - --config-dir=/kube-dns-config - --v=2 env: - name: PROMETHEUS_PORT value: &quot;10055&quot; image: gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.10 imagePullPolicy: IfNotPresent livenessProbe: failureThreshold: 5 httpGet: path: /healthcheck/kubedns port: 10054 scheme: HTTP initialDelaySeconds: 60 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 5 name: kubedns ports: - containerPort: 10053 name: dns-local protocol: UDP - containerPort: 10053 name: dns-tcp-local protocol: TCP - containerPort: 10055 name: metrics protocol: TCP readinessProbe: failureThreshold: 3 httpGet: path: /readiness port: 8081 scheme: HTTP initialDelaySeconds: 3 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 5 resources: limits: memory: 170Mi requests: cpu: 100m memory: 70Mi terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /kube-dns-config name: kube-dns-config - args: - -v=2 - -logtostderr - -configDir=/etc/k8s/dns/dnsmasq-nanny - -restartDnsmasq=true - -- - -k - --cache-size=1000 - --dns-loop-detect - --log-facility=- - --server=/cluster.local/127.0.0.1#10053 - --server=/in-addr.arpa/127.0.0.1#10053 - --server=/ip6.arpa/127.0.0.1#10053 image: gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.10 imagePullPolicy: IfNotPresent livenessProbe: failureThreshold: 5 httpGet: path: /healthcheck/dnsmasq port: 10054 scheme: HTTP initialDelaySeconds: 60 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 5 name: dnsmasq ports: - containerPort: 53 name: dns protocol: UDP - containerPort: 53 name: dns-tcp protocol: TCP resources: requests: cpu: 150m memory: 20Mi terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /etc/k8s/dns/dnsmasq-nanny name: kube-dns-config - args: - --v=2 - --logtostderr - --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,A - --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,A image: gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.10 imagePullPolicy: IfNotPresent livenessProbe: failureThreshold: 5 httpGet: path: /metrics port: 10054 scheme: HTTP initialDelaySeconds: 60 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 5 name: sidecar ports: - containerPort: 10054 name: metrics protocol: TCP resources: requests: cpu: 10m memory: 20Mi terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: Default nodeSelector: beta.kubernetes.io/os: linux restartPolicy: Always schedulerName: default-scheduler securityContext: &#123;&#125; serviceAccount: kube-dns serviceAccountName: kube-dns terminationGracePeriodSeconds: 30 tolerations: - key: CriticalAddonsOnly operator: Exists - effect: NoSchedule key: node-role.kubernetes.io/master operator: Equal volumes: - configMap: defaultMode: 420 name: kube-dns optional: true name: kube-dns-config 从这个 Deployment 可以看到2个重点： KubeDNS 使用 名称为 kube-dns 的 ConfigMap 作为其配置（ConfigMap没有，POD也可以运行）。 KubeDNS 使用了3个容器，组合提供DNS服务。 KubeDNS组件构成及实现KubeDNS，使用3个容器组合服务，分别是：dnsmasq、kube-dns、sidecar，这3者，职责不同，整个DNS架构组成如下： POD的DNS请求打到 dnsmasq 容器的53端口，dnsmasq 决定此请求时自己处理，还是转到 kubedns 容器处理。各组件具体职责： dnsmasq 容器dnsmasq 容器：负责整个 KubeDNS 的请求入口，53端口，就是它开放的，因此，在 K8S 中，内部域名和外部域名请求处理的区分，也是 dnsmasq 来做的。它充当 DNS 的请求入口，有几个作用： 充当 DNS 请求入口。 区别 K8S 内部和外部域名走不通的策略。 DNS 缓存，进行过DNS请求后的域名会进行缓存，提高DNS请求效率。 在此容器的启动参数包含下面部分： 123- --server=/cluster.local/127.0.0.1#10053- --server=/in-addr.arpa/127.0.0.1#10053- --server=/ip6.arpa/127.0.0.1#10053 这个参数说明，cluster.local 结尾的域名（这种是 K8S 的内部域名，不理解为什么这种是 K8S 内部域名的，可以翻看之前的文章），dnsmasq 进程会把此 DNS 请求，转发到 127.0.0.1:10053 端口上。而 10053 端口，是 kube-dns容器进程，正是监听的 10053 端口。所以说， dnsmasq 通过 - –server=/cluster.local/127.0.0.1#10053 这个配置，来决策 K8S 内部的域名的 DNS 请求，往 kube-dns 容器转发。 dnsmasq 先解析本地 /etc/hosts 文件再解析 /etc/dnsmasq.d/*.conf 文件然后解析 /etc/dnsmasq.conf最后解析自定义上游DNS的部分，也就是 /etc/dnsmasq.conf 中 resolv-file 的字段部分，一般我们将 resolv-file 字段配置为 /etc/resolv.conf。 dnsmasq为KubeDNS提供缓存加速能力KubeDNS 缓存，其实，利用的便是 dnsmasq 具备的缓存能力。另外，缓存，在 KubeDNS 中非常重要，能最大程度的发挥 DNS 响应效率，我们通过一个实际例子测试一下： 第一次请求一个不存在的域名时，第一次 DNS 请求： 123456789101112131415161718[root@test8-5646b97977-p6z8p /]# dig jjjjjjjj1212121212.com; &lt;&lt;&gt;&gt; DiG 9.9.4-RedHat-9.9.4-61.el7_5.1 &lt;&lt;&gt;&gt; jjjjjjjj1212121212.com;; global options: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NXDOMAIN, id: 61155;; flags: qr rd ra; QUERY: 1, ANSWER: 0, AUTHORITY: 1, ADDITIONAL: 0;; QUESTION SECTION:;jjjjjjjj1212121212.com. IN A;; AUTHORITY SECTION:com. 900 IN SOA a.gtld-servers.net. nstld.verisign-grs.com. 1542785400 1800 900 604800 86400;; Query time: 864 msec;; SERVER: 10.233.0.3#53(10.233.0.3);; WHEN: Wed Nov 21 07:30:18 UTC 2018;; MSG SIZE rcvd: 113 请求耗时 864ms，耗时相当长。再次执行 DNS 请求： 1234567891011121314151617[root@test8-5646b97977-p6z8p /]# dig jjjjjjjj1212121212.com; &lt;&lt;&gt;&gt; DiG 9.9.4-RedHat-9.9.4-61.el7_5.1 &lt;&lt;&gt;&gt; jjjjjjjj1212121212.com;; global options: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NXDOMAIN, id: 60767;; flags: qr rd ra; QUERY: 1, ANSWER: 0, AUTHORITY: 0, ADDITIONAL: 1;; OPT PSEUDOSECTION:; EDNS: version: 0, flags:; udp: 4096;; QUESTION SECTION:;jjjjjjjj1212121212.com. IN A;; Query time: 1 msec;; SERVER: 10.233.0.3#53(10.233.0.3);; WHEN: Wed Nov 21 07:41:35 UTC 2018;; MSG SIZE rcvd: 51 请求耗时仅仅1毫秒。加速效果相当明显。 dnsmasq 容器内的进程组成及职责dnamasq 容器内，其实有2个进程，一个是 dnsmasq-nanny，一个是 dnsmamsq。其实，我们上面看到的 KubeDNS 的 deployment 里关于 kube-dns 容器的配置里的相关参数，并不是直接对 dnsmasq 进程生效的，而是对 dnsmasq-nanny 进程生效的。 dnsmasq-nanny 进程，是 dnsmasq 容器进程的1号进程，是保姆进程，而 dnsmasq 进程，就是由 dnsmasq-nanny 进程 fork 出来的，dnsmasq-nanny 具备 fork、restart 及传递配置参数给 dnsmasq 进程的能力。 我们可以通过进程树来看 dnsmasq-nanny 和 dnsmasq 的关系。 12345678910// 查看进程树/ # pstree -pdnsmasq-nanny(1)---dnsmasq(30)// dnsmasq-nanny 是 dnsmasq 的父进程。// 通过 ps 命令也可以看出来，dnsmasq 的参数，是 dnsmasq-nanny 传过去的/ # cat sss PID USER TIME COMMAND 1 root 12:38 /dnsmasq-nanny -v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053 30 root 268:54 /usr/sbin/dnsmasq -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053 既然 dnsmasq-nanny 是保姆进程，其有几个重要参数： 12345// dnsmasq-nanny 进程的配置目录，这个目录，是需要挂载 configmap 类型 Volume 到此目录的-configDir=/etc/k8s/dns/dnsmasq-nanny// 开启配置检测，当配置发生变更时，重启 dnsmasq 进程-restartDnsmasq=true dnsmasq 容器负责让KubeDNS更新配置生效更新配置很简单，只需要更新名称为 kube-dns 的 ConfigMap 的内容，即可，但这里我们主要从源码角度，讲一下原理是什么。首先再次明确一遍，KubeDNS由多个组件构成，但真正负责处理配置更新的，只有 dnsmasq 容器的 dnsmasq-nanny 进程。 我们通过部分源码，也可以看出来，dnsmasq-nanny 是 如何 重启 dnsmasq 进程的： 1234567891011121314151617181920212223242526272829303132333435363738394041// dns/pkg/dnsmasq/nanny.go#169// RunNanny runs the nanny and handles configuration updates.func RunNanny(sync config.Sync, opts RunNannyOpts) &#123; defer glog.Flush() currentConfig, err := sync.Once() if err != nil &#123; glog.Errorf(&quot;Error getting initial config, using default: %v&quot;, err) currentConfig = config.NewDefaultConfig() &#125; nanny := &amp;Nanny&#123;Exec: opts.DnsmasqExec&#125; nanny.Configure(opts.DnsmasqArgs, currentConfig) if err := nanny.Start(); err != nil &#123; glog.Fatalf(&quot;Could not start dnsmasq with initial configuration: %v&quot;, err) &#125; configChan := sync.Periodic() for &#123; select &#123; case status := &lt;-nanny.ExitChannel: glog.Flush() glog.Fatalf(&quot;dnsmasq exited: %v&quot;, status) break case currentConfig = &lt;-configChan: // 如果接受到新的配置的数据变化，先杀掉dnsmasq，再使用最新配置，启动一个新的dnsmasq进程 if opts.RestartOnChange &#123; glog.V(0).Infof(&quot;Restarting dnsmasq with new configuration&quot;) nanny.Kill() nanny = &amp;Nanny&#123;Exec: opts.DnsmasqExec&#125; nanny.Configure(opts.DnsmasqArgs, currentConfig) nanny.Start() &#125; else &#123; glog.V(2).Infof(&quot;Not restarting dnsmasq (--restartDnsmasq=false)&quot;) &#125; break &#125; &#125;&#125; 那么，配置是什么时候产生变化的呢？ dnsmasq-nanny 进程处理配置变化的方式比较粗暴，如果设置了 -configDir 配置目录的话，此进程会间隔 10s ，进行一次配置检测，如果发生变化，就 重启 dnsmasq。 123456789101112// dns/cmd/dnsmasq-nanny/main.go#72// 进程启动入口部分func main() &#123; parseFlags() glog.V(0).Infof(&quot;opts: %v&quot;, opts) // 运行 dnsmasq-nanny 之前，做配置变化的检测 sync := config.NewFileSync(opts.configDir, opts.syncInterval) dnsmasq.RunNanny(sync, opts.RunNannyOpts)&#125; 123456789101112131415161718192021222324252627// dns/pkg/dns/config/sync.go#81// 计划任务，检测配置目录里的文件，是否发生变化，发生了，则返回func (sync *kubeSync) Periodic() &lt;-chan *Config &#123; // 检测配置变更的动作，用协程开启，异步进行 go func() &#123; // 开启一个间隔为 10s 的计划任务，每次获取最新的配置版本和数据 // resultChan 是一个结构体，核心，是一个版本，以及这个版本对应的所有文件名及文件数据 // 它只是获取数据的版本和具体数据，数据是否变了，sync.syncSource.Periodic 并不管 resultChan := sync.syncSource.Periodic() for &#123; syncResult := &lt;-resultChan // 这里是判断是否数据发生了变化，如果是，则将最新的数据返回 // 后边，我们会看，这个操作是怎么判断数据产生了变化的 config, changed, err := sync.processUpdate(syncResult, false) if err != nil &#123; continue &#125; if !changed &#123; continue &#125; // 返回最新数据（如果走到这一步，说明配置绝对发送了变化，否则会在上面的步骤继续走循环逻辑） sync.channel &lt;- config &#125; &#125;() return sync.channel&#125; 我们看看，是如何判断文件发生变化的 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253// dns/pkg/dns/config/sync.go#99//判断数据是否产生了变化func (sync *kubeSync) processUpdate(result syncResult, buildUnchangedConfig bool) (config *Config, changed bool, err error) &#123; glog.V(4).Infof(&quot;processUpdate %+v&quot;, result) // 这里是判断核心，如果 sync 对象中保存的版本，与 result 对象中保存的版本不同，则认为数据发生了变化 if result.Version != sync.latestVersion &#123; glog.V(3).Infof(&quot;Updating config to version %v (was %v)&quot;, result.Version, sync.latestVersion) changed = true sync.latestVersion = result.Version &#125; else &#123; glog.V(4).Infof(&quot;Config was unchanged (version %v)&quot;, sync.latestVersion) // short-circuit if we haven&apos;t been asked to build an unchanged config object if !buildUnchangedConfig &#123; return &#125; &#125; if result.Version == &quot;&quot; &amp;&amp; len(result.Data) == 0 &#123; config = NewDefaultConfig() return &#125; // 下面都是解析配置的过程了，无需细看 config = &amp;Config&#123;&#125; for key, updateFn := range map[string]fieldUpdateFn&#123; &quot;federations&quot;: updateFederations, &quot;stubDomains&quot;: updateStubDomains, &quot;upstreamNameservers&quot;: updateUpstreamNameservers, &#125; &#123; value, ok := result.Data[key] if !ok &#123; glog.V(3).Infof(&quot;No %v present&quot;, key) continue &#125; if err = updateFn(key, value, config); err != nil &#123; glog.Errorf(&quot;Invalid configuration for %v, ignoring update: %v&quot;, key, err) return &#125; &#125; if err = config.Validate(); err != nil &#123; glog.Errorf(&quot;Invalid configuration: %v (value was %+v), ignoring update&quot;, err, config) config = nil return &#125; return&#125; 综上：dnsmasq-nanny 保姆进程，会一直检测自己参数 -configDir 配置的目录里的文件是否发生了变化，判断的逻辑是：每隔10秒钟，读取一次这个目录下的所有【配置文件数据】（一个Map，key为文件名，值为文件内容），用 sha256 计算一次摘要，作为这个目录下所有数据的版本。然后记录下来，下次的时候执行同样的逻辑，如果发现版本不同，则认为配置文件发送了变化。然后将得到的【配置文件数据】重新解析为 dnsmasq-nanny 的配置数据，最后，杀死 dnsmasq 进程，启动一个系的 dnsmasq 进程。但是，这里边，有几个细节点： 第一：重启 dnsmasq 的方式，先杀后起，方式台粗暴，很可能导致这个时间点的大量DNS请求失败。不优雅。 第二：dnsmasq-nanny 检测数据变化的方式，这种方式就有2个值得注意的问题： ①：官方是，每次遍历目录下的所有文件，然后，使用 ioutil.ReadFile 读取文件内容。如果目录下文件多，可能导致，你遍历的同时，配置文件也在变化，你遍历的速度和文件更新速度不一致，导致，读取的配置，并不一定是最新的，可能你遍历完，某个配置文件才更新完。那么此时，你读取的一部分文件数据并不是和当前目录下文件数据完全一致，本次会重启 dnsmasq。进而，下次检测，还认为有文件变化，到时候，又重启一次 dnsmasq。这种方式不优雅，但问题不大。 ②：文件的检测，直接使用 ioutil.ReadFile 读取文件内容，也存在问题。如果文件变化，和文件读取同时发生，很可能你读取完，文件的更新都没完成，那么你读取的并非一个完整的文件，而是坏的文件，这种文件，dnsmasq-nanny 无法做解析，不过官方代码中有数据校验，解析失败也问题不大，大不了下个周期的时候，再取到完整数据，再解析一次。 kube-dns服务容器及实现原理讲完 KubeDNS 服务的 dnsmasq 容器，现在开始 kube-dns 容器。kube-dns 容器，最主要的职责，就是负责解析 K8S 的内部域名记录，这个解析，它监听了 10053 端口，本质上，kube-dns 是接受 dnsmasq 请求的（ dnsmasq 容器负责处理所有 DNS 请求，对于 K8S 的内部域名请求，转发给 kube-dns 来处理）。 kube-dns 的进程职责是，监视Kubernetes master上 Service 和 Endpoint 的改变，并在内存中维护 lookup 结构用于服务DNS请求。 此容器启动参数为： 1234- --domain=cluster.local.- --dns-port=10053- --config-dir=/kube-dns-config- --v=2 –domain：表示在哪个domain下创建域名记录。–dns-port：这个是启动端口。–config-dir：这个是使用的配置，通常来说，我们的 KubeDNS POD，会使用 ConfigMap，将配置挂载到容器内的 –config-dir 指定的目录上。 这里打算源码层面，追踪一下具体实现，但不打算从头追到尾，这样篇幅太大，只罗列一部分核心点。核心点内容包括： kube-dns 容器，都是监听的哪些 K8S 资源数据，作为域名记录的解析依据？ kube-dns 使用什么技术或数据结构实现内存级数据查询的？ 如果 K8S 有 Service 资源，但没有对应的 POD 资源，域名解析是否还能成功？ kube-dns容器监听哪些K8S资源？我们知道，K8S 内部域名的 DNS 解析，得到的是 Service 的 IP 地址。kube-dns 肯定监听了 Service 资源了，为的是，处理内部域名映射到 Service 的 IP 地址。从源码看，其实 kube-dns 容器，其实监听了2个资源，分别是： Service 资源 Endpoints 资源 至于为何还需要监听 Endpoints 资源，我们一步步解开谜底 123456789101112131415161718192021222324// 开启一个 KubeDNS 处理实例func NewKubeDNS(client clientset.Interface, clusterDomain string, timeout time.Duration, configSync config.Sync) *KubeDNS &#123; kd := &amp;KubeDNS&#123; kubeClient: client, domain: clusterDomain, cache: treecache.NewTreeCache(), cacheLock: sync.RWMutex&#123;&#125;, nodesStore: kcache.NewStore(kcache.MetaNamespaceKeyFunc), reverseRecordMap: make(map[string]*skymsg.Service), clusterIPServiceMap: make(map[string]*v1.Service), domainPath: util.ReverseArray(strings.Split(strings.TrimRight(clusterDomain, &quot;.&quot;), &quot;.&quot;)), initialSyncTimeout: timeout, configLock: sync.RWMutex&#123;&#125;, configSync: configSync, &#125; // 监听并处理 Endpoints 资源 kd.setEndpointsStore() // 监听并处理 Services 资源 kd.setServicesStore() return kd&#125; 123456789101112131415161718192021222324252627282930313233343536373839// setServicesStore 负责处理 Service 资源func (kd *KubeDNS) setServicesStore() &#123; // Returns a cache.ListWatch that gets all changes to services. kd.servicesStore, kd.serviceController = kcache.NewInformer( kcache.NewListWatchFromClient( kd.kubeClient.Core().RESTClient(), &quot;services&quot;, v1.NamespaceAll, fields.Everything()), &amp;v1.Service&#123;&#125;, resyncPeriod, // 这里也可以看出来，它其实主要针对 Service 的 增删改做 handle 处理 kcache.ResourceEventHandlerFuncs&#123; AddFunc: kd.newService, DeleteFunc: kd.removeService, UpdateFunc: kd.updateService, &#125;, )&#125;// 同上func (kd *KubeDNS) setEndpointsStore() &#123; // Returns a cache.ListWatch that gets all changes to endpoints. kd.endpointsStore, kd.endpointsController = kcache.NewInformer( kcache.NewListWatchFromClient( kd.kubeClient.Core().RESTClient(), &quot;endpoints&quot;, v1.NamespaceAll, fields.Everything()), &amp;v1.Endpoints&#123;&#125;, resyncPeriod, kcache.ResourceEventHandlerFuncs&#123; AddFunc: kd.handleEndpointAdd, UpdateFunc: kd.handleEndpointUpdate, // If Service is named headless need to remove the reverse dns entries. DeleteFunc: kd.handleEndpointDelete, &#125;, )&#125; 我们看一下，如果 K8S 有 Service 资源创建出来，kube-dns 容器都做些什么 12345678910111213141516171819202122232425262728293031323334// 针对 K8S 中 Service 资源的创建，做处理func (kd *KubeDNS) newService(obj interface&#123;&#125;) &#123; if service, ok := assertIsService(obj); ok &#123; glog.V(3).Infof(&quot;New service: %v&quot;, service.Name) glog.V(4).Infof(&quot;Service details: %v&quot;, service) // 1、如果 Service 是 ExternaName 类型，则创建 cname 记录 // 稍微提一下，K8S 中的 Service，有几种类型：ClusterIP、NodePort、LoadBalancer、ExternaName // ExternalName services are a special kind that return CNAME records if service.Spec.Type == v1.ServiceTypeExternalName &#123; kd.newExternalNameService(service) return &#125; // 2、如果是无头服务，则处理无头服务方式的DNS记录（A记录），无头服务的域名记录有些不同，所以这里是单独进行处理的 // 需要说明的是，我们刚刚提到的 Endpoints 资源，其实主要就是用在无头服务上的。 // if ClusterIP is not set, a DNS entry should not be created if !v1.IsServiceIPSet(service) &#123; if err := kd.newHeadlessService(service); err != nil &#123; glog.Errorf(&quot;Could not create new headless service %v: %v&quot;, service.Name, err) &#125; return &#125; if len(service.Spec.Ports) == 0 &#123; glog.Warningf(&quot;Service with no ports, this should not have happened: %v&quot;, service) &#125; // 3、创建正常的 Service DNS 记录 kd.newPortalService(service) &#125;&#125; 上面提到，Service 的创建操作，会涉及到 newHeadlessService 无头服务域名记录的操作，具体如下： 123456789101112131415161718192021222324252627282930// 创建无头服务Service的域名记录// Generates skydns records for a headless service.func (kd *KubeDNS) newHeadlessService(service *v1.Service) error &#123; // Create an A record for every pod in the service. // This record must be periodically updated. // Format is as follows: // For a service x, with pods a and b create DNS records, // a.x.ns.domain. and, b.x.ns.domain. key, err := kcache.MetaNamespaceKeyFunc(service) if err != nil &#123; return err &#125; // 根据 Service 名称获取key，在根据 Key，获取这个 Service 下的 Endpoints，用来生成特殊的无头服务的域名记录 e, exists, err := kd.endpointsStore.GetByKey(key) if err != nil &#123; return fmt.Errorf(&quot;failed to get endpoints object from endpoints store - %v&quot;, err) &#125; // 如果这个 Service 下没有 Endpoints，将不生成域名记录，一旦有endpoints之后，就会生成。 if !exists &#123; glog.V(1).Infof(&quot;Could not find endpoints for service %q in namespace %q. DNS records will be created once endpoints show up.&quot;, service.Name, service.Namespace) return nil &#125; if e, ok := e.(*v1.Endpoints); ok &#123; return kd.generateRecordsForHeadlessService(e, service) &#125; return nil&#125; 如果 Service 下没有Endpoints，不处理，一旦有 Endpoints 产生，立即生成域名记录： 123456789101112131415// 根据 Endpoints，创建无头服务记录func (kd *KubeDNS) addDNSUsingEndpoints(e *v1.Endpoints) error &#123; // 先根据 Endpoints 超出其属于哪个 Service svc, err := kd.getServiceFromEndpoints(e) if err != nil &#123; return err &#125; // 判断这个 Service 是不是无头服务的 Service，如果不是，直接返回 if svc == nil || v1.IsServiceIPSet(svc) || svc.Spec.Type == v1.ServiceTypeExternalName &#123; // No headless service found corresponding to endpoints object. return nil &#125; // 如果是无头服务的 Service，生成域名记录 return kd.generateRecordsForHeadlessService(e, svc)&#125; kube-dns使用何种数据结构实现DNS检索？参看后面附录 dns-sidecar容器dns-sidecar容器职责简述这是其实是一个健康监测容器，检查 dnsmasq 和 kube-dns 2个容器的监控状态。先回顾一下 sidecar 容器的参数 123456- args: - --v=2 - --logtostderr - --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,A - --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,A image: gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.10 这个容器，主要职责是探测 dnsmasq 以及 kubedns 服务的状态。但从上面 deployment 的参数，看不出太多东西，从源码角度可以看到更多： 12345678910111213141516// sidecar 容器的默认启动参数// NewOptions creates a new options struct with default values.func NewOptions() *Options &#123; return &amp;Options&#123; DnsMasqAddr: &quot;127.0.0.1&quot;, DnsMasqPort: 53, DnsMasqPollIntervalMs: 5000, // sidecar 容器，开放了一个 10054 端口访问，这样一来，我们可以通过 Prometheus 来收集 sidecar 的数据。 PrometheusAddr: &quot;0.0.0.0&quot;, PrometheusPort: 10054, PrometheusPath: &quot;/metrics&quot;, PrometheusNamespace: &quot;kubedns&quot;, &#125;&#125; sidecar 容器，开放了一个 10054 端口访问，这样一来，我们可以通过 Prometheus 来收集 sidecar 的数据，另外需要说明的是，其实 sidecar 容器开放的 metrics 接口，暴露出来的数据，是 sidecar 探测 dnsmasq 以及 kube-dns 两个目标后，汇总的数据，数据包括： 基本的 Go 应用性能数据（协程数量、CPU使用、打开的最大文件描述符数、内存使用等） dnsmasq 容器发生的错误数 dnsmasq 容器已经发生的 DNS 缓存驱逐次数 dnsmasq 容器已经发生的 DNS 缓存插入次数 dnsmasq 容器缓存未命中次数 dnsmasq 域名解析失败次数 探测到的 dnsmasq 延迟时间 dns-sidecar 是如何做检测的sidecar 的探测周期，默认为 5s 一次。–probe 指定了探测参数，比如： 1- --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,A 表示：探测的服务“标签”为 kubedns，使用域名 kubernetes.default.svc.cluster.local ，将 A 记录类型的DNS解析请求，打到 127.0.0.1:10053 上。 注意：这里的“标签”，是为某个DNS服务目标，打的一个标识而已，后边，这个“标签”会用到。主要是为了方便 通过 dns-sidecar 的 metrics 接口，访问到具体的某个“标签”的DNS服务的健康指标。 指定多个 –probe 参数，则都会探测多个目标服务 123456789101112131415// dns/pkg/sidecar/server.go#43func (s *server) Run(options *Options) &#123; s.options = options glog.Infof(&quot;Starting server (options %+v)&quot;, *s.options) // 循环遍历 probes 目标，然后开启探测 for _, probeOption := range options.Probes &#123; probe := &amp;dnsProbe&#123;DNSProbeOption: probeOption&#125; s.probes = append(s.probes, probe) probe.Start(options) &#125; s.runMetrics(options)&#125; 追一下是如何进行具体的DNS探测的： 1234567891011121314151617181920// dns/pkg/sidecar/dnsprobe.go#74func (p *dnsProbe) Start(options *Options) &#123; glog.V(2).Infof(&quot;Starting dnsProbe %+v&quot;, p.DNSProbeOption) p.lastError = fmt.Errorf(&quot;waiting for first probe&quot;) // 为探测目标，定制抓们的 metrics URL，这样就可以通过 http://x.x.x.x:port/healthcheck/具体标签 的方式，来访问具体某个标签的DNS探测状态了。 // 这里的“标签”，其实就是之前我们提到的所用之处。 http.HandleFunc(&quot;/healthcheck/&quot;+p.Label, p.httpHandler) p.registerMetrics(options) if p.delayer == nil &#123; glog.V(4).Infof(&quot;Using defaultLoopDelayer&quot;) p.delayer = &amp;defaultLoopDelayer&#123;&#125; &#125; // 异步探测动作 go p.loop()&#125; 具体的 DNS 探测动作： 123456789101112131415161718192021222324// dns/okg/sidecar/dnsprobe.go#111func (p *dnsProbe) loop() &#123; glog.V(4).Infof(&quot;Starting loop&quot;) p.delayer.Start(p.Interval) // 初始化一个dns客户端 dnsClient := &amp;dns.Client&#123;&#125; // 循环检测，用不退出 for &#123; glog.V(4).Infof(&quot;Sending DNS request @%v %v&quot;, p.Server, p.Name) // 发送一个 DNS 请求 msg, latency, err := dnsClient.Exchange(p.msg(), p.Server) glog.V(4).Infof(&quot;Got response, err=%v after %v&quot;, err, latency) if err == nil &amp;&amp; len(msg.Answer) == 0 &#123; err = fmt.Errorf(&quot;no RRs for domain %q&quot;, p.Name) &#125; // 更新 DNS请求的metrics指标（延迟）数据 p.update(err, latency) p.delayer.Sleep(latency) &#125;&#125; KubeDNS 服务弹性水平伸缩官方推荐方案：cluster-proportional-autoscaler关于 KubeDNS 服务弹性伸缩，官方已经给出了一套比较简单的弹性水平扩缩容解决方案，下面是一套推荐的配置，用它来部署 KubeDNS 的 autoscaler： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374apiVersion: extensions/v1beta1kind: Deploymentmetadata: labels: addonmanager.kubernetes.io/mode: Reconcile k8s-app: kubedns-autoscaler kubernetes.io/cluster-service: &quot;true&quot; name: kubedns-autoscaler namespace: kube-systemspec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: kubedns-autoscaler strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 1 type: RollingUpdate template: metadata: creationTimestamp: null labels: k8s-app: kubedns-autoscaler spec: affinity: nodeAffinity: preferredDuringSchedulingIgnoredDuringExecution: - preference: matchExpressions: - key: node-role.kubernetes.io/master operator: In values: - &quot;true&quot; weight: 100 podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchLabels: k8s-app: kubedns-autoscaler topologyKey: kubernetes.io/hostname containers: - command: - /cluster-proportional-autoscaler - --namespace=kube-system - --configmap=kubedns-autoscaler - --target=Deployment/kube-dns - --default-params=&#123;&quot;linear&quot;:&#123;&quot;nodesPerReplica&quot;:10,&quot;min&quot;:2&#125;&#125; - --logtostderr=true - --v=2 image: gcr.io/google_containers/cluster-proportional-autoscaler-amd64:1.1.2 imagePullPolicy: IfNotPresent name: autoscaler resources: requests: cpu: 20m memory: 10Mi terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst nodeSelector: beta.kubernetes.io/os: linux restartPolicy: Always schedulerName: default-scheduler securityContext: &#123;&#125; serviceAccount: cluster-proportional-autoscaler serviceAccountName: cluster-proportional-autoscaler terminationGracePeriodSeconds: 30 tolerations: - effect: NoSchedule key: node-role.kubernetes.io/master operator: Equal 我们看几个关键参数：123456- --namespace=kube-system // autoscaler 配置所在的命名空间- --configmap=kubedns-autoscaler // autoscaler 的 configmap 配置名称- --target=Deployment/kube-dns // 表示要弹性伸缩的目标- --default-params=&#123;&quot;linear&quot;:&#123;&quot;nodesPerReplica&quot;:10,&quot;min&quot;:2&#125;&#125; // 默认配置，这个配置可以由 configmap 覆盖- --logtostderr=true- --v=2 我们可以定义 configmap 来配置 kubedns-autoscaler 的具体参数，configmap 中可以配置的内容如下： cluster-proportional-autoscaler 线性配置方式123456789data: linear: |- &#123; &quot;coresPerReplica&quot;: 2, &quot;nodesPerReplica&quot;: 1, &quot;min&quot;: 1, &quot;max&quot;: 100, &quot;preventSinglePointFailure&quot;: true &#125; 123min：表示目标最小实例数，也就是 KubeDNS 最少的实例数量。coresPerReplica：当集群中有很多核（不是可用，而是总量）时，它决定 KubeDNS 的实例数量。nodesPerReplica：当集群中核数（不是可用，而是总量）少时，nodesPerReplica 来控制实例数量。 整体来看，由下面这个公式，的出来最大的DNS实例数： 123replicas = max( ceil( cores * 1/coresPerReplica ) , ceil( nodes * 1/nodesPerReplica ) )replicas = min(replicas, max)replicas = max(replicas, min) cluster-proportional-autoscaler 梯度配置方式省略，也可以查看官方文档。 cluster-proportional-autoscaler 水平扩容是如何实现的 独立部署 cluster-proportional-autoscaler autoscaler 从 APIServer（也就是K8S Master）拉取集群的核数和节点数，并根据这2者，确定一个 POD 最大实例数。 可以通过 configmap 配置 autoscaler 的参数，而不需要 重启 autoscaler 实例。 autoscaler 提供了【线性】及【梯度】2种扩容方式。 这里边有一个小问题：通常，我们使用 ConfigMap 都是讲 ConfigMap 作为 volume ，Mount 到容器上，而 autoscaler 并没有使用这种方式，而是监听了K8S资源（通过 –configmap 及 –namespace 配置）从中获取配置，这种方式非常快而且高效，且不需要重启 autoscaler 实例。 扩容项目为：https://github.com/kubernetes-incubator/cluster-proportional-autoscaler cluster-proportional-autoscaler 不足之处基本上从上面可以知道，这个 扩容器，比较简单，最多就是根据集群的核数，以及节点数来配置最大实例数，其实这个扩缩容，并没有完整的实现出来，官方的意思是，Kubernetes 设想的 Horizontal Pod Autoscaler（https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/） 是一个顶级的资源，需要根据集群中容器的CPU指标来衡量一个合理的值，但是当前这个 cluster-proportional-autoscaler ，仅仅是一个 DIY 实现，所谓的 DIY 实现，意思就是，你得自己手动来控制（比如通过更改 configmap 的方式），它并没有收集集群中容器的CPU利用率等数据，仅仅是一个比较粗略的水平调度实现而已。 Questions到底dnsmasq和kube-dns，谁提供的DNS上游能力？DNS 上游查询能力，也即是我们访问 非 K8S 内的域名，比如 youku.com，是 dnsmasq 向上游 DNS 服务器查询的，还是 kube-dns 来做的？ 首先，dnsmasq 是 K8S 内 DNS 服务的入口，它决定内部的域名往 kube-dns 转发，其他的域名，往上游 DNS 服务器转发，然后将结果根据域名的TTL做缓存，加速DNS查询。 从上面文章的分析看，应该就是 dnsmasq 来上游查询处理的，但是，从 kube-dns 源码上看，其实 kube-dns 本身，也是具备提供 DNS 上游查询能力的。我们需要具体来看，这个上游查询，是谁来做的。 在 K8S 中，我们这样设置 DNS 上游，需要在 名为 kube-dns 的 ConfigMap 中来做。比如： 12345678910apiVersion: v1kind: ConfigMapmetadata: name: kube-dns namespace: kube-systemdata: stubDomains: | &#123;&quot;acme.local&quot;: [&quot;1.2.3.4&quot;]&#125; upstreamNameservers: | [&quot;8.8.8.8&quot;, &quot;8.8.4.4&quot;] 上游DNS服务器字段：upstreamNameservers。 既然是需要配置 ConfigMap 来使配置生效，那么就看一下，是谁 使用了这个 ConfigMap 就行了。所以，我们回顾文章最开始部分的 deployment，看看谁将 ConfigMap 作为 Voloume 挂载到自己容器里使用就行了。 我们能够看到，使用 ConfigMap 的，有2个容器，分别是： ①：dnsmasq 容器②：kube-dns 容器 kube-dns 容器使用这个 ConfigMap，肯定是可以根据其配置，使用配置中的 upstreamNameservers 中的地址来决定域名解析请求往何处转发的。但问题在于，它有这个能力，但不代表上游解析，真的由它来做。 这里边需要注意的是，dnsmasq 容器也使用了这个配置，但并不是 dnsmasq 容器的 dnsmasq 进程直接使用的，而是 dnsmasq 容器中的 dnsmasq-nanny 进程来使用的，这个进程本身就是保姆进程，当 ConfigMap 变化后，dnsmasq-nanny 进程，便会解析配置，将配置中的 upstreamNameservers、stubDomains 内容，转换为 dnsmasq 进程能够识别的参数，然后杀死 原来的 dnsmasq 进程，启动一个新的。 所以，在 KubeDNS 服务中，提供上游DNS解析能力的，是 dnsmasq 容器，而不是 kube-dns 容器。 附录1：kube-dns使用何种数据结构实现DNS检索 后续有时间再补充。]]></content>
      <categories>
        <category>kubernetes</category>
        <category>kubedns</category>
        <category>coredns</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>coredns</tag>
        <tag>kubedns</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CoreDNS系列1：Kubernetes内部域名解析原理、弊端及优化方式]]></title>
    <url>%2F2018%2F11%2F20%2F9%2F</url>
    <content type="text"><![CDATA[Kubernetes 中的 DNS本篇，是 CoreDNS 的前篇之一，后续会着重介绍 CoreDNS，但是步步深入讲 CoreDNS，讲一下 Kubernetes，以及 kubedns 有一定的必要，所以，就有了 CoreDNS 系列，本篇主要尽可能详尽的说明 Kubernetes 的DNS解析原理，以及 Kubernetes 集群中 DNS 解析目前存在的弊端和优化方式。 在 Kubernetes 中，服务发现有几种方式：①：基于环境变量的方式②：基于内部域名的方式 基本上，使用环境变量的方式很少，主要还是使用内部域名这种服务发现的方式。 其中，基于内部域名的方式，涉及到 Kubernetes 内部域名的解析，而 kubedns，是 Kubernetes 官方的 DNS 解析组件。从 1.11 版本开始，kubeadm 已经使用第三方的 CoreDNS 替换官方的 kubedns 作为 Kubernetes 集群的内部域名解析组件，我们的重点，是 CoreDNS，但是在开始 CoreDNS 之前，需要先了解下 kubedns，后续，会对这2个 DNS 组件做对比，分析它们的优劣势。 Kubernetes 中的域名是如何解析的在 Kubernetes 中，比如服务 a 访问服务 b，对于同一个 Namespace下，可以直接在 pod 中，通过 curl b 来访问。对于跨 Namespace 的情况，服务名后边对应 Namespace即可。比如 curl b.default。那么，使用者这里边会有几个问题： ①：服务名是什么？②：为什么同一个 Namespace 下，直接访问服务名即可？不同 Namespace 下，需要带上 Namespace 才行？③：为什么内部的域名可以做解析，原理是什么？ DNS 如何解析，依赖容器内 resolv 文件的配置 1234cat /etc/resolv.confnameserver 10.233.0.3search default.svc.cluster.local svc.cluster.local cluster.local 这个文件中，配置的 DNS Server，一般就是 K8S 中，kubedns 的 Service 的 ClusterIP，这个IP是虚拟IP，无法ping，但可以访问。 1234[root@node4 user1]# kubectl get svc -n kube-systemNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkube-dns ClusterIP 10.233.0.3 &lt;none&gt; 53/UDP,53/TCP 270dkubernetes-dashboard ClusterIP 10.233.22.223 &lt;none&gt; 443/TCP 124d 所以，所有域名的解析，其实都要经过 kubedns 的虚拟IP 10.233.0.3 进行解析，不论是 Kubernetes 内部域名还是外部的域名。 Kubernetes 中，域名的全称，必须是 service-name.namespace.svc.cluster.local 这种模式，服务名，就是Kubernetes中 Service 的名称，所以，当我们执行下面的命令时： 1curl b 必须得有一个 Service 名称为 b，这是前提。在容器内，会根据 /etc/resolve.conf 进行解析流程。选择 nameserver 10.233.0.3 进行解析，然后，用字符串 “b”，依次带入 /etc/resolve.conf 中的 search 域，进行DNS查找，分别是： 12// search 内容类似如下（不同的pod，第一个域会有所不同）search default.svc.cluster.local svc.cluster.local cluster.local b.default.svc.cluster.local -&gt; b.svc.cluster.local -&gt; b.cluster.local ，直到找到为止。所以，我们执行 curl b，或者执行 curl b.default，都可以完成DNS请求，这2个不同的操作，会分别进行不同的DNS查找步骤： 1234567// curl b，可以一次性找到（b +default.svc.cluster.local）b.default.svc.cluster.local// curl b.default，第一次找不到（ b.default + default.svc.cluster.local）b.default.default.svc.cluster.local// 第二次查找（ b.default + svc.cluster.local），可以找到b.default.svc.cluster.local So Questionscurl b，要比 curl b.default 效率高？答案是肯定的，因为 curl b.default，多经过了一次 DNS 查询。当执行 curl b.default，也就使用了带有命名空间的内部域名时，容器的第一个 DNS 请求是 12// b.default + default.svc.cluster.localb.default.default.svc.cluster.local 当请求不到 DNS 结果时，使用 12// b.default + svc.cluster.localb.default.svc.cluster.local 进行请求，此时才可以得到正确的DNS解析。 访问外部域名走 search 域吗这个答案，不能说肯定也不能说否定，看情况，可以说，大部分情况要走 search 域。 我们以请求 youku.com 为例，通过抓包的方式，看一看在某个容器中访问 youku.com，进行的DNS查找的过程，都产生了什么样的数据包。注意：我们要抓DNS容器的包，就得先进入到DNS容器的网络中（而不是发起DNS请求的那个容器）。 由于DNS容器往往不具备bash，所以无法通过 docker exec 的方式进入容器内抓包，我们采用其他的方式： 123456// 1、找到容器ID，并打印它的NS IDdocker inspect --format &quot;&#123;&#123;.State.Pid&#125;&#125;&quot; 16938de418ac// 2、进入此容器的网络Namespacensenter -n -t 54438// 3、抓DNS包tcpdump -i eth0 udp dst port 53|grep youku.com 在其他的容器中，进行 youku.com 域名查找 1nslookup youku.com 172.22.121.65 注意：nslookup命令的最后指定DNS服务容器的IP，是因为，如果不指定，且DNS服务的容器存在多个的话，那么DNS请求，可能会均分到所有DNS服务的容器上，我们如果只抓某单个DNS服务容器抓到的包，可能就不全了，指定IP后，DNS的请求，就必然只会打到单个的DNS容器。抓包的数据才完整。 可以看到类似如下结果： 1234517:01:28.732260 IP 172.20.92.100.36326 &gt; nodexxxx.domain: 4394+ A? youku.com.default.svc.cluster.local. (50)17:01:28.733158 IP 172.20.92.100.49846 &gt; nodexxxx.domain: 60286+ A? youku.com.svc.cluster.local. (45)17:01:28.733888 IP 172.20.92.100.51933 &gt; nodexxxx.domain: 63077+ A? youku.com.cluster.local. (41)17:01:28.734588 IP 172.20.92.100.33401 &gt; nodexxxx.domain: 27896+ A? youku.com. (27)17:01:28.734758 IP nodexxxx.34138 &gt; 192.168.x.x.domain: 27896+ A? youku.com. (27) 我们可以看到，在真正解析 youku.com 之前，经历了 youku.com.default.svc.cluster.local. -&gt; youku.com.svc.cluster.local. -&gt; youku.com.cluster.local. -&gt; youku.com. 这也就意味着有3次DNS请求，是浪费的无意义的请求。 为何会出现DNS请求浪费的情况这是因为，在 Kubernetes 中，其实 /etc/resolv.conf 这个文件，并不止包含 nameserver 和 search 域，还包含了非常重要的一项：ndots。我们之前没有提及这个项，也是希望再次能引起读者重视。 1234[root@xxxx-67f54c6dff-h4zxq /]# cat /etc/resolv.conf nameserver 10.233.0.3search cicd.svc.cluster.local svc.cluster.local cluster.localoptions ndots:5 ndots:5，表示：如果查询的域名包含的点“.”，不到5个，那么进行DNS查找，将使用非完全限定名称（或者叫绝对域名），如果你查询的域名包含点数大于等于5，那么DNS查询，默认会使用绝对域名进行查询。举例来说： 如果我们请求的域名是，a.b.c.d.e，这个域名中有4个点，那么容器中进行DNS请求时，会使用非绝对域名进行查找，使用非绝对域名，会按照 /etc/resolv.conf 中的 search 域，走一遍追加匹配： a.b.c.d.e.cicd.svc.cluster.local. -&gt;a.b.c.d.e.svc.cluster.local. -&gt;a.b.c.d.e.cluster.local. 直到找到为止。如果走完了search域还找不到，则使用 a.b.c.d.e. ，作为绝对域名进行DNS查找。 我们通过抓包分析一个具体案例： 域名中点数少于5个的情况： 12345678910111213141516// 对域名 a.b.c.d.ccccc 进行DNS解析请求 [root@xxxxx-67f54c6dff-h4zxq /]# nslookup a.b.c.d.ccccc 172.22.121.65 Server: 172.22.121.65Address: 172.22.121.65#53** server can&apos;t find a.b.c.d.ccccc: NXDOMAIN// 抓包数据如下：18:08:11.013497 IP 172.20.92.100.33387 &gt; node011094.domain: 28844+ A? a.b.c.d.ccccc.cicd.svc.cluster.local. (54)18:08:11.014337 IP 172.20.92.100.33952 &gt; node011094.domain: 57782+ A? a.b.c.d.ccccc.svc.cluster.local. (49)18:08:11.015079 IP 172.20.92.100.45984 &gt; node011094.domain: 55144+ A? a.b.c.d.ccccc.cluster.local. (45)18:08:11.015747 IP 172.20.92.100.54589 &gt; node011094.domain: 22860+ A? a.b.c.d.ccccc. (31)18:08:11.015970 IP node011094.36383 &gt; 192.168.x.x.domain: 22860+ A? a.b.c.d.ccccc. (31)// 结论：// 点数少于5个，先走search域，最后将其视为绝对域名进行查询 域名中点数&gt;=5个的情况： 12345678910111213141516// 对域名 a.b.c.d.e.ccccc 进行DNS解析请求[root@xxxxx-67f54c6dff-h4zxq /]# nslookup a.b.c.d.e.ccccc 172.22.121.65 Server: 172.22.121.65Address: 172.22.121.65#53** server can&apos;t find a.b.c.d.e.ccccc: NXDOMAIN// 抓包数据如下：18:10:14.514595 IP 172.20.92.100.34423 &gt; node011094.domain: 61170+ A? a.b.c.d.e.ccccc. (33)18:10:14.514856 IP node011094.58522 &gt; 192.168.x.x.domain: 61170+ A? a.b.c.d.e.ccccc. (33)18:10:14.515880 IP 172.20.92.100.49328 &gt; node011094.domain: 267+ A? a.b.c.d.e.ccccc.cicd.svc.cluster.local. (56)18:10:14.516678 IP 172.20.92.100.35651 &gt; node011094.domain: 54181+ A? a.b.c.d.e.ccccc.svc.cluster.local. (51)18:10:14.517356 IP 172.20.92.100.33259 &gt; node011094.domain: 53022+ A? a.b.c.d.e.ccccc.cluster.local. (47)// 结论：// 点数&gt;=5个，直接视为绝对域名进行查找，只有当查询不到的时候，才继续走 search 域。 如何优化 DNS 请求浪费的情况优化方式1：使用全限定域名其实最直接，最有效的优化方式，就是使用 “fully qualified name”，简单来说，使用“完全限定域名”（也叫绝对域名），你访问的域名，必须要以 “.” 为后缀，这样就会避免走 search 域进行匹配，我们抓包再试一次： 12// 注意：youku.com 后边有一个点 .nslookup youku.com. 172.22.121.65 在DNS服务容器上抓到的包如下： 1216:57:07.628112 IP 172.20.92.100.36772 &gt; nodexxxx.domain: 46851+ [1au] A? youku.com. (38)16:57:07.628339 IP nodexxxx.47350 &gt; 192.168.x.x.domain: 46851+ [1au] A? youku.com. (38) 并没有多余的DNS请求。 优化方式2：具体应用配置特定的 ndots其实，往往我们还真不太好用这种绝对域名的方式，有谁请求youku.com的时候，还写成 youku.com. 呢？ 在 Kubernetes 中，默认设置了 ndots 值为5，是因为，Kubernetes 认为，内部域名，最长为5，要保证内部域名的请求，优先走集群内部的DNS，而不是将内部域名的DNS解析请求，有打到外网的机会，Kubernetes 设置 ndots 为5是一个比较合理的行为。 如果你需要定制这个长度，最好是为自己的业务，单独配置 ndots 即可（Pod为例，其实配置deployment最好）。 12345678910111213apiVersion: v1kind: Podmetadata: namespace: default name: dns-examplespec: containers: - name: test image: nginx dnsConfig: options: - name: ndots value: &quot;1&quot; Kubernetes DNS 策略在Kubernetes 中，有4种 DNS 策略，从 Kubernetes 源码中看： 1234567891011121314151617181920const ( // DNSClusterFirstWithHostNet indicates that the pod should use cluster DNS // first, if it is available, then fall back on the default // (as determined by kubelet) DNS settings. DNSClusterFirstWithHostNet DNSPolicy = &quot;ClusterFirstWithHostNet&quot; // DNSClusterFirst indicates that the pod should use cluster DNS // first unless hostNetwork is true, if it is available, then // fall back on the default (as determined by kubelet) DNS settings. DNSClusterFirst DNSPolicy = &quot;ClusterFirst&quot; // DNSDefault indicates that the pod should use the default (as // determined by kubelet) DNS settings. DNSDefault DNSPolicy = &quot;Default&quot; // DNSNone indicates that the pod should use empty DNS settings. DNS // parameters such as nameservers and search paths should be defined via // DNSConfig. DNSNone DNSPolicy = &quot;None&quot;) 这几种DNS策略，需要在Pod，或者Deployment、RC等资源中，设置 dnsPolicy 即可，以 Pod 为例： 123456789101112131415161718192021222324252627apiVersion: v1kind: Podmetadata: labels: name: cadvisor-nodexxxx hostip: 192.168.x.x name: cadvisor-nodexxxx namespace: monitoringspec: containers: - args: - --profiling - --housekeeping_interval=10s - --storage_duration=1m0s image: google/cadvisor:latest name: cadvisor-nodexxxx ports: - containerPort: 8080 name: http protocol: TCP resources: &#123;&#125; securityContext: privileged: true terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst nodeName: nodexxxx 具体来说： None 表示空的DNS设置这种方式一般用于想要自定义 DNS 配置的场景，而且，往往需要和 dnsConfig 配合一起使用达到自定义 DNS 的目的。 Default 有人说 Default 的方式，是使用宿主机的方式，这种说法并不准确。这种方式，其实是，让 kubelet 来决定使用何种 DNS 策略。而 kubelet 默认的方式，就是使用宿主机的 /etc/resolv.conf（可能这就是有人说使用宿主机的DNS策略的方式吧），但是，kubelet 是可以灵活来配置使用什么文件来进行DNS策略的，我们完全可以使用 kubelet 的参数：–resolv-conf=/etc/resolv.conf 来决定你的DNS解析文件地址。 ClusterFirst 这种方式，表示 POD 内的 DNS 使用集群中配置的 DNS 服务，简单来说，就是使用 Kubernetes 中 kubedns 或 coredns 服务进行域名解析。如果解析不成功，才会使用宿主机的 DNS 配置进行解析。 ClusterFirstWithHostNet 在某些场景下，我们的 POD 是用 HOST 模式启动的（HOST模式，是共享宿主机网络的），一旦用 HOST 模式，表示这个 POD 中的所有容器，都要使用宿主机的 /etc/resolv.conf 配置进行DNS查询，但如果你想使用了 HOST 模式，还继续使用 Kubernetes 的DNS服务，那就将 dnsPolicy 设置为 ClusterFirstWithHostNet。 下一篇，我们将主要讲 kubedns 的架构组成。]]></content>
      <categories>
        <category>kubernetes</category>
        <category>kubedns</category>
        <category>coredns</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>coredns</tag>
        <tag>kubedns</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一次对进程大量积压 ESTABLISHED 链接的排查记录]]></title>
    <url>%2F2018%2F11%2F16%2F8%2F</url>
    <content type="text"><![CDATA[背景我们都知道，基于Kubernetes的微服务，大行其道，传统部署模式一直都在跟着变化，但其实，在原有业务向服务化方向过度过程中，有些场景可能会变得复杂。 比如说：将Kubernetes的模式应用到开发环节上，这个环节需要频繁的变更代码，微服务的方式，可能就需要不断的： 改代码-&gt;构建镜像-&gt;镜像推送-&gt;部署-&gt;拉去镜像-&gt;生成容器 尤其是PHP的业务，不需要构建二进制，仅需要发布代码，因此，如果按照上面的部署方式，就需要频繁改代码，走构建镜像这个流程，最后再做发布，这在开发环节就显得过于麻烦了，换而言之，有没有办法，能让开发直接将代码上传到容器中呢？ 其实是有的，就是设计一个FTP中间件代理，让用户本地改完代码，通过FTP客户端（很多IDE是支持FTP的）直接上传到容器内部，甚至于用户保存一下代码就上传到容器内。 因此，这就引出了今天的主角，是我基于FTP协议+gRPC协议自研的FTP代理工具。 这个工具上线后，服务全公司所有研发，经过一段时间运行和修补，相对稳定，也做了一些关于内存方面的优化，直到又一次，在维护这个FTP代理的时候，发现一个奇怪的问题： FTP代理进程，监听的是 192.168.88.32 的 21 端口，所以，这个端口对应了多少连接，就表示有多少个客户端存在，通过： 1netstat -apn |grep &quot;192.168.88.32:21&quot; 发现，有将近1000个链接，且都是 ESTABLISHED，ESTABLISHED 状态表示一个连接的状态是“已连接”，但我们研发团队，并没有那么多人，直觉上看，事出反常必有妖。 初步分析可能性感觉可能有一种情况，就是每个人开了多个FTP客户端，实际场景下，研发同学组可能会使用3种类型的FTP客户端 PHPStorm：这个客户端（SFTP插件）自己会维护一个FTP长连接。Sublime + VsCode，这2个客户端不会维护链接，数据交互完成（比如传输任务），就主动发送 QUIT 指令到FTP代理端，然后所有链接关闭。很干净。 另外，使用PHPStorm的话，也存在开多个IDE创建，就使用多个FTP客户端连接的情况。为了继续排查，我把所有对 192.168.88.32:21 的链接，做了分组统计，看看哪个IP的连接数最多 12# 注：61604 是 ftp代理的进程IDnetstat -apn|grep &quot;61604/server&quot;|grep &apos;192.168.88.32:21&apos;|awk -F &apos;:&apos; &apos;&#123;print$2&#125;&apos;|awk &apos;&#123;print$2&#125;&apos;|sort|uniq -c |sort 上面的统计，是看哪个IP，对 192.168.88.32:21 连接数最多（18个）。 统计发现，很多IP，都存在多个链接的情况，难道每个人都用了多个IDE且可能还多IDE窗口使用吗？于是，挑了一个最多的，找到公司中使用这个IP的人，沟通发现，他确实使用了IDE多窗口，但是远远没有使用18个客户端那么多，仅仅PHPStorm开了3个窗口而已。 初步排查结论：应该是FTP代理所在服务器的问题，和用户开多个客户端没有关系。 进一步排查这次排查，是怀疑，这将近1000个的 ESTABLISHED 客户端链接中，有大量假的 ESTABLISHED 链接存在，之前的统计发现，实际上，对 192.168.88.32:21 的客户端链接进行筛选，得到的IP，一共才200个客户端IP而已，平均下来，每个人都有5个FTP客户端链接FTP代理，想象觉得不太可能。那么，如何排查 ESTABLISHED 假链接呢？ 在 TCP 四次挥手过程中，首先需要有一端，发起 FIN 包，接收方接受到 FIN 包之后，便开启四次挥手的过程，这也是连接断开的过程。 从之前的排查看，有人的IP，发起了多达18个FTP连接，那么，要排查是不是在 FTP 代理服务器上，存在假的 ESTABLISHED 连接的话，就首先需要去 开发同学的机器上看，客户端连接的端口，是不是仍在使用。比如： 1tcp ESTAB 0 0 192.168.88.32:21 192.168.67.38:58038 这个表明，有一个研发的同学 IP是 192.168.67.38，使用了端口 58038，连接 192.168.88.32 上的 FTP 代理服务的 21 端口。所以，先要去看，到底研发同学的电脑上，这个端口存在不存在。 后来经过与研发同学沟通确认，研发电脑上并没有 58038 端口使用，这说明，对FTP代理服务的的客户端链接中显示的端口，也就是实际用户的客户端端口，存在大量不存在的情况。 结论：FTP代理服务器上，存在的近1000个客户端连接中（ESTABLISHED状态），有大量的假连接存在。也就是说，实际上这个连接早就断开不存在了，但服务端却还显示存在。 排查假 ESTABLISHED 连接首先，如果出现假的 ESTABLISHED 连接，表示连接的客户端已经不存在了，客户端一方，要么发起了 TCP FIN 请求服务端没有收到，比如因为网络的各种原因（比如断网了）之后，FTP客户端无法发送FIN到服务端。要么服务端服务器接受到了 FIN，但是在后续过程中，丢包了等等。 为了验证上面的问题，我本机进行了一次模拟，连接FTP服务端后，本机直接断网，断网后，杀死FTP客户端进程，等待5分钟（为什么等待5分钟后面说）后，重新联网。然后再 FTP 服务端，查看服务器上与 FTP代理进行连接的所有IP，然后发现我本机的IP和端口依然在列，然后再我本机，通过 1lsof -i :端口号 却没有任何记录，直接说明：服务端确实保持了假 ESTABLISHED 链接，一直不释放。 上面提到，我等待5分钟，是因为，服务端的 keepalive，是这样的配置： 1234[root@xx xx]# sysctl -a |grep keepalivenet.ipv4.tcp_keepalive_intvl = 75net.ipv4.tcp_keepalive_probes = 9net.ipv4.tcp_keepalive_time = 300 服务器默认设置的 tcp keepalive 检测是300秒后进行检测，也就是5分钟，当检测失败后，一共进行9次重试，每次时间间隔是75秒。那么，问题就来了，服务器设置了 keepalive，如果 300 + 9*75 秒后，依然连接不上，就应该主动关闭假 ESTABLISHED 连接才对。为何还会积压呢？ 猜想1：大量的积压的 ESTABLISHED 连接，实际上都还没有到释放时间为了验证这个问题，我们就需要具体的看某个连接，什么时候创建的。所以，我找到其中一个我确定是假的 ESTABLISHED的链接（那个IP的用户，把所有FTP客户端都关了，进程也杀死了），看此连接的创建时间，过程如下： 先确定 FTP 代理进程的ID，为 61604 然后，看看这个进程的所有连接，找到某个端口的（55360，就是一个客户端所使用的端口） 12[root@xxx xxx]# lsof -p 61604|grep 55360server 61604 root 6u IPv4 336087732 0t0 TCP node088032:ftp-&gt;192.168.70.16:55360 (ESTABLISHED) 我们看到一个 “6u”，这个就是进程使用的这个连接的socket文件，Linux中，一切皆文件。我们看看这个文件的创建时间，就是这个连接的创建时间了 123ll /proc/61604/fd/6//输出：lrwx------. 1 root root 64 Nov 1 14:03 /proc/61604/fd/6 -&gt; socket:[336087732] 这个连接是11月1号创建的，现在已经11月8号，这个时间，早已经超出了 keepalive 探测 TCP连接是否存活的时间。这说明2个点： 1、可能 Linux 的 KeepAlive 压根没生效。2、可能我的 FTP 代理进程，压根没有使用 TCP KeepAlive 猜想2： FTP 代理进程，压根没有使用 TCP KeepAlive要验证这个结论，就得先知道，怎么看一个连接，到底具不具备 KeepAlive 功效？ netstat 命令不好使（也可能我没找到方法），我们使用 ss 命令，查看 FTP进程下所有连接21端口的链接 1ss -aoen|grep 192.168.12.32:21|grep ESTAB 从众多结果中，随便筛选2个结果： 12tcp ESTAB 0 0 192.168.12.32:21 192.168.20.63:63677 ino:336879672 sk:65bb &lt;-&gt;tcp ESTAB 0 0 192.168.12.32:21 192.168.49.21:51896 ino:336960511 sk:67f7 &lt;-&gt; 我们再对比一下，所有连接服务器sshd进程的 1234tcp ESTAB 0 0 192.168.12.32:333 192.168.53.207:63269 timer:(keepalive,59sec,0) ino:336462258 sk:6435 &lt;-&gt;tcp ESTAB 0 0 192.168.12.32:333 192.168.55.185:64892 timer:(keepalive,3min59sec,0) ino:336461969 sk:62d1 &lt;-&gt;tcp ESTAB 0 0 192.168.12.32:333 192.168.53.207:63220 timer:(keepalive,28sec,0) ino:336486442 sk:6329 &lt;-&gt;tcp ESTAB 0 0 192.168.12.32:333 192.168.53.207:63771 timer:(keepalive,12sec,0) ino:336896561 sk:65de &lt;-&gt; 对比很容易发现，连接 21端口的所有连接，多没有 timer 项。这说明，FTP代理 进程监听 21 端口时，所有进来的链接，全都没有使用keepalive。 找了一些文章，大多只是说，怎么配置Linux 的 Keep Alive，以及不配置的，会造成 ESTABLISHED 不释放问题，没有说进程需要额外设置啊？难道 Linux KeepAlive 配置，不是对所有连接直接就生效的？ 所以，我们有必要验证 Linux keepalive，必须要进程自己额外开启才能生效 验证 Linux keepalive，必须要进程自己额外开启才能生效在开始这个验证之前，先摘取一段FTP中间件代理关于监听 21 端口的部分代码： 1234567891011121314151617181920212223242526272829func (ftpServer *FTPServer) ListenAndServe() error &#123; laddr, err := net.ResolveTCPAddr(&quot;tcp4&quot;, ftpServer.listenTo) if err != nil &#123; return err &#125; listener, err := net.ListenTCP(&quot;tcp4&quot;, laddr) if err != nil &#123; return err &#125; for &#123; clientConn, err := listener.AcceptTCP() if err != nil || clientConn == nil &#123; ftpServer.logger.Print(&quot;listening error&quot;) break &#125; //以闭包的方式整理处理driver和ftpBridge，协程结束整体由GC做资源释放 go func(c *net.TCPConn) &#123; driver, err := ftpServer.driverFactory.NewDriver(ftpServer.FTPDriverType) if err != nil &#123; ftpServer.logger.Print(&quot;Error creating driver, aborting client connection：&quot; + err.Error()) &#125; else &#123; ftpBridge := NewftpBridge(c, driver) ftpBridge.Serve() &#125; c = nil &#125;(clientConn) &#125; return nil&#125; 足够明显，整个函数，net.ListenTCP 附近都没有任何设置KeepAlive的相关操作。我们查看 相关函数，找到了设置 KeepAlive的地方，进行一下设置： 123456if err != nil || clientConn == nil &#123; ftpServer.logger.Print(&quot;listening error&quot;) break&#125;// 此处，设置 keepaliveclientConn.SetKeepAlive(true) 重新构建部署之后，可以看到，所有对21端口的连接，全部都带了 timer 1ss -aoen|grep 192.168.12.32:21|grep ESTAB 输出如下： 1234tcp ESTAB 0 0 192.168.12.32:21 192.168.70.76:54888 timer:(keepalive,1min19sec,0) ino:397279721 sk:6b49 &lt;-&gt;tcp ESTAB 0 0 192.168.12.32:21 192.168.37.125:49648 timer:(keepalive,1min11sec,0) ino:398533882 sk:6b4a &lt;-&gt;tcp ESTAB 0 0 192.168.12.32:21 192.168.33.196:64471 timer:(keepalive,7.957ms,0) ino:397757143 sk:6b4c &lt;-&gt;tcp ESTAB 0 0 192.168.12.32:21 192.168.21.159:56630 timer:(keepalive,36sec,0) ino:396741646 sk:6b4d &lt;-&gt; 可以很明显看到，所有的连接，全部具备了 timer 功效，说明：想要使用 Linux 的 KeepAlive，需要程序单独做设置进行开启才行。 最后：ss 命令结果中 keepalive 的说明首先，看一下 Linux 中的配置，我的机器如下： 1234[root@xx xx]# sysctl -a |grep keepalivenet.ipv4.tcp_keepalive_intvl = 75net.ipv4.tcp_keepalive_probes = 9net.ipv4.tcp_keepalive_time = 300 tcp_keepalive_time：表示多长时间后，开始检测TCP链接是否有效。tcp_keepalive_probes：表示如果检测失败，会一直探测 9 次。tcp_keepalive_intvl：承上，探测9次的时间间隔为 75 秒。 然后，我们看一下 ss 命令的结果： 1ss -aoen|grep 192.168.12.32:21|grep ESTAB 1tcp ESTAB 0 0 192.168.12.32:21 192.168.70.76:54888 timer:(keepalive,1min19sec,0) ino:397279721 sk:6b49 &lt;-&gt; 摘取这部分：timer:(keepalive,1min19sec,0) ，其中： keepalive：表示此链接具备 keepalive 功效。1min19sec：表示剩余探测时间，这个时间每次看都会边，是一个递减的值，第一次探测，需要 net.ipv4.tcp_keepalive_time 这个时间倒计时，如果探测失败继续探测，后边会按照 net.ipv4.tcp_keepalive_intvl 这个时间值进行探测。直到探测成功。0：这个值是探测时，检测到这是一个无效的TCP链接的话已经进行了的探测次数。]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes是如何对资源（CPU、内存等）做限制的]]></title>
    <url>%2F2018%2F10%2F31%2F7%2F</url>
    <content type="text"><![CDATA[Kubernetes对资源的限制在Kubernetes中，对资源（CPU、内存等）的限制，需要定义在yaml中，以Deployment举例： 123456789101112131415161718apiVersion: v1kind: Podmetadata: name: cpu-overload namespace: testspec: containers: - name: cpu-overload image: stress/stress:latest resources: limits: cpu: &quot;2&quot; memory: 1000Mi requests: cpu: &quot;1&quot; memory: 500Mi command: [&quot;stress&quot;] args: [&quot;-c&quot;, &quot;2&quot;] 其中，CPU 有2个限制： requests：相对限制，是容器的最低申请资源，这个限制是相对的，无法做到绝对严格。 limits：绝对限制，这个是限制的绝对的，不可能超越。 本例中，对容器 cpu-overload 的 CPU 的限制，是，申请1个核的运算资源，最多可以使用2个核。 这里需要特别说明一点，所谓的最多2个核，其实是均摊的，如果这个容器真的触发了计算瓶颈，在docker中看，CPU使用率是200%，但在宿主机去看，其实并非是将2个核占满了，而是将压力分摊到了多个CPU的核上。 对Kubernetes来说，只能做到限制容器资源，无法对pod资源做限制，Kubernetes官方认为，要计算一个pod的资源限制，将pod中各个容器的资源做加和就行了。这里不展示详细说，具体怎么为Kubernetes限制内存和CPU，可以直接参看官方文档：Managing Compute Resources for Containers ，这篇文章写的够详细了。 资源限制的传递Kubernetes其实可以认为是一系列组件包装起来的一个大型工具。关于资源限制，其实Kubernetes自己做不了这些，而是将对资源限制，通过yaml中的定义，传递到Docker容器中。比如，之前我们在Deployment中容器的CPU，限制为最多使用2个核，这个限制，Kubernetes会传递给Docker来做，所以本质上，Kubernetes资源的限制能力，来源于Docker，而Docker能做到什么程度的限制，又取决于Linux的cgroups，所以在很早之前的Docker是不支持在Windows平台运行的，归根结底，还是因为cgroups是Linux内核支持的产物。 说了这么多，我们可以通过一个实例来说明这个传递性。在开始前，简单说一下步骤: 在Kubernetes中启动一个单独的pod，资源限制为最多4个CPU核。 找到这个pod对应的容器，看一下容器的运行配置，是不是限制了4个核。 找到这个容器对应的Cgroups配置，看是否对容器限制了4个核。 实验首先，创建一个限制了1个核的pod12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576apiVersion: v1kind: Podmetadata: labels: testsss: cadvisor-test name: cadvisor-test namespace: testspec: containers: - args: - -allow_dynamic_housekeeping=true - -global_housekeeping_interval=1m0s - -housekeeping_interval=5s - -disable_metrics=udp,tcp,percpu,sched,disk,network - -storage_duration=15s - -profiling=true - -enable_load_reader=true - -port=30008 - -max_procs=1 image: google/cadvisor imagePullPolicy: IfNotPresent name: cadvisor-test ports: - containerPort: 30008 hostPort: 30008 name: http protocol: TCP resources: limits: cpu: &quot;1&quot; memory: 2000Mi requests: cpu: &quot;0.1&quot; memory: 100Mi terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /rootfs name: rootfs readOnly: true - mountPath: /var/run name: var-run readOnly: true - mountPath: /sys name: sys readOnly: true - mountPath: /var/lib/docker name: docker readOnly: true - mountPath: /dev/disk name: disk readOnly: true dnsPolicy: ClusterFirst hostNetwork: true nodeName: nodexxx restartPolicy: Always volumes: - hostPath: path: / type: &quot;&quot; name: rootfs - hostPath: path: /var/run type: &quot;&quot; name: var-run - hostPath: path: /sys type: &quot;&quot; name: sys - hostPath: path: /DATA/docker type: &quot;&quot; name: docker - hostPath: path: /dev/disk name: disk 在这个yaml中，我们队cAdvisor容器，限制为1核2G内存。我们通过 1kubectl apply -f cadvisor-pod.yaml 将pod运行起来。 查看容器的运行时限制运行为容器后，查看此pod所在节点，进入到节点，找到这个容器，通过下面指令查看此容器的运行时配置 1docker inspect 6f9ecad83132 然后，从一大堆输出中，找到下面的重点部分： 1234567891011121314151617181920212223242526272829&quot;Isolation&quot;: &quot;&quot;,&quot;CpuShares&quot;: 102,&quot;Memory&quot;: 2097152000,&quot;NanoCpus&quot;: 0,&quot;CgroupParent&quot;: &quot;/kubepods/burstable/pod1fb8b728-dbed-11e8-a89e-801844e1f8c8&quot;,&quot;BlkioWeight&quot;: 0,&quot;BlkioWeightDevice&quot;: null,&quot;BlkioDeviceReadBps&quot;: null,&quot;BlkioDeviceWriteBps&quot;: null,&quot;BlkioDeviceReadIOps&quot;: null,&quot;BlkioDeviceWriteIOps&quot;: null,&quot;CpuPeriod&quot;: 100000,&quot;CpuQuota&quot;: 100000,&quot;CpuRealtimePeriod&quot;: 0,&quot;CpuRealtimeRuntime&quot;: 0,&quot;CpusetCpus&quot;: &quot;&quot;,&quot;CpusetMems&quot;: &quot;&quot;,&quot;Devices&quot;: [],&quot;DeviceCgroupRules&quot;: null,&quot;DiskQuota&quot;: 0,&quot;KernelMemory&quot;: 0,&quot;MemoryReservation&quot;: 0,&quot;MemorySwap&quot;: 4194304000,&quot;MemorySwappiness&quot;: -1,&quot;OomKillDisable&quot;: false,&quot;PidsLimit&quot;: 0,&quot;Ulimits&quot;: null,&quot;CpuCount&quot;: 0,&quot;CpuPercent&quot;: 0, 其中： Memory：限制内存资源，单位为byte，2097152000 = 2G CpuShares：CPU使用的相对权重，一个核为1024，我们设置的request cpu为 0.1 ，所以就是 102 CpuPeriod：一个CPU为100000，也就是100个milicpu，这个一般不需要改。 CpuQuota：CPU资源的绝对限制，一般和CpuPeriod结合在一起，CpuQuota/CpuPeriod，就是能够使用的核数，100000/100000=1，表示我们能最多使用1个CPU核心。 CpusetCpus：这个值表示当前容器运行时，绑定到哪几个CPU编号上，注意：这个不是CPU个数，而是绑定到哪几个CPU上，多个CPU编号用逗号分割。 从上面的docker运行时限制看，和Kubernetes的Pod的定义完全吻合。下面再看Cgroups的限制，这才是核心。 根据容器，查Cgroups的限制内容首先，我们看一下pod的名称： 12[root@nodexx test]# kubectl get pod -n test|grep cadvisorcadvisor-test 1/1 Running 0 14h 然后，在pod所在的宿主机，找到这个pod对应的容器Id 123456docker ps|grep cadvisor//找到结果[root@nodexx docker]# docker ps|grep cadvisor 6f9ecad83132 google/cadvisor &quot;/usr/bin/cadvisor...&quot; 15 hours ago Up 15 hours k8s_cadvisor-test_cadvisor-test_test_1fb8b728-dbed-11e8-a89e-801844e1f8c8_0139259f9a21c gcr.io/google_containers/pause-amd64:3.0 &quot;/pause&quot; 15 hours ago Up 15 hours 我们可以注意到，一个匹配出来2个容器，一个是 cadvisor 容器，一个是pause容器，pause容器，是 Kubernetes pod 的基础容器。我们只需要 cadivsor容器的Id：6f9ecad83132，我们要通过它拿到这个容器的Cgroup信息 1234# docker inspect 6f9ecad83132|grep Cgroup&quot;Cgroup&quot;: &quot;&quot;,&quot;CgroupParent&quot;: &quot;/kubepods/burstable/pod1fb8b728-dbed-11e8-a89e-801844e1f8c8&quot;,&quot;DeviceCgroupRules&quot;: null, 好了，完事具备，我们直接进入Cgroup配置目录： 1cd /sys/fs/cgroup/cpu/kubepods/burstable/pod1fb8b728-dbed-11e8-a89e-801844e1f8c8 注意一下这个目录，是 /sys/fs/cgroup/cpu 与 /kubepods/burstable/pod1fb8b728-dbed-11e8-a89e-801844e1f8c8 拼接起来的一个整体路径。在这个目录下，有很多文件： 1234567891011121314151617181920212223ll //total 0drwxr-xr-x 2 root root 0 Oct 30 18:14 139259f9a21cc26fb8b40b85b345566489fc3bedc1b36766b032abc60b93e702drwxr-xr-x 2 root root 0 Oct 30 18:14 6f9ecad83132b77c7e0ce3bfe8e56b471c8c41b920970b3fdd6da4d158fb4b1e-rw-r--r-- 1 root root 0 Oct 30 18:14 cgroup.clone_children-rw-r--r-- 1 root root 0 Oct 30 18:14 cgroup.procs-r--r--r-- 1 root root 0 Oct 30 18:14 cpuacct.stat-rw-r--r-- 1 root root 0 Oct 30 18:14 cpuacct.usage-r--r--r-- 1 root root 0 Oct 30 18:14 cpuacct.usage_all-r--r--r-- 1 root root 0 Oct 30 18:14 cpuacct.usage_percpu-r--r--r-- 1 root root 0 Oct 30 18:14 cpuacct.usage_percpu_sys-r--r--r-- 1 root root 0 Oct 30 18:14 cpuacct.usage_percpu_user-r--r--r-- 1 root root 0 Oct 30 18:14 cpuacct.usage_sys-r--r--r-- 1 root root 0 Oct 30 18:14 cpuacct.usage_user-rw-r--r-- 1 root root 0 Oct 30 10:40 cpu.cfs_period_us-rw-r--r-- 1 root root 0 Oct 30 10:40 cpu.cfs_quota_us-rw-r--r-- 1 root root 0 Oct 30 18:14 cpu.rt_period_us-rw-r--r-- 1 root root 0 Oct 30 18:14 cpu.rt_runtime_us-rw-r--r-- 1 root root 0 Oct 30 10:40 cpu.shares-r--r--r-- 1 root root 0 Oct 30 18:14 cpu.stat-rw-r--r-- 1 root root 0 Oct 30 18:14 notify_on_release-rw-r--r-- 1 root root 0 Oct 30 18:14 tasks 其中，当前目录下有很多Cgroup内容，而有2个子目录： 12139259f9a21cc26fb8b40b85b345566489fc3bedc1b36766b032abc60b93e7026f9ecad83132b77c7e0ce3bfe8e56b471c8c41b920970b3fdd6da4d158fb4b1e 这2个目录，其实就是 pod 中的2个容器（pause容器，cadvisor-test容器），我们进入 cadvisor-test容器的目录下 1cd 6f9ecad83132b77c7e0ce3bfe8e56b471c8c41b920970b3fdd6da4d158fb4b1e 为什么可以判定2个目录中，6f9ecad83132b77c7e0ce3bfe8e56b471c8c41b920970b3fdd6da4d158fb4b1e就是cadvisor-test容器目录呢？因为容器的ID，就是6f9ecad83132呀！ 好了，查看下目录下的内容： 123456789101112131415161718-rw-r--r-- 1 root root 0 Oct 30 18:15 cgroup.clone_children-rw-r--r-- 1 root root 0 Oct 30 10:40 cgroup.procs-r--r--r-- 1 root root 0 Oct 30 18:15 cpuacct.stat-rw-r--r-- 1 root root 0 Oct 30 18:15 cpuacct.usage-r--r--r-- 1 root root 0 Oct 30 18:15 cpuacct.usage_all-r--r--r-- 1 root root 0 Oct 30 18:15 cpuacct.usage_percpu-r--r--r-- 1 root root 0 Oct 30 18:15 cpuacct.usage_percpu_sys-r--r--r-- 1 root root 0 Oct 30 18:15 cpuacct.usage_percpu_user-r--r--r-- 1 root root 0 Oct 30 18:15 cpuacct.usage_sys-r--r--r-- 1 root root 0 Oct 30 18:15 cpuacct.usage_user-rw-r--r-- 1 root root 0 Oct 30 10:40 cpu.cfs_period_us-rw-r--r-- 1 root root 0 Oct 30 10:40 cpu.cfs_quota_us-rw-r--r-- 1 root root 0 Oct 30 18:15 cpu.rt_period_us-rw-r--r-- 1 root root 0 Oct 30 18:15 cpu.rt_runtime_us-rw-r--r-- 1 root root 0 Oct 30 10:40 cpu.shares-r--r--r-- 1 root root 0 Oct 30 18:15 cpu.stat-rw-r--r-- 1 root root 0 Oct 30 18:15 notify_on_release-rw-r--r-- 1 root root 0 Oct 30 18:15 tasks 其中，能够看到好几个熟悉的词，比如 cpu.cfs_quota_us，这个正是对CPU资源做限制的。我们查看一下其内容： 12[root@nodexxx # cat cpu.cfs_quota_us100000 没错，这个值正是100000，也就是1个CPU。 Linux Cgroup 是如何与Dock而关联的？上面的方式，已经层层找到了对CPU、内存等限制，是如何通过Kubernets的Deployment，一步步追查到Cgroup的。那么，Linux Cgroup，怎么与容器关联起来的呢？ 我们看一个cgroup目录中的tasks 12[root@nodexxx]# cat tasks 21694 这个值，就是进程ID，所以，Cgroup对资源的限制，就是对进程ID来限制的。我们看一下这个进程ID 12[root@nodexxx]# ps -ef|grep 21694root 21694 21663 7 Oct30 ? 01:05:05 /usr/bin/cadvisor -logtostderr -allow_dynamic_housekeeping=true -global_housekeeping_interval=1m0s -housekeeping_interval=5s -disable_metrics=udp,tcp,percpu,sched,disk,network -storage_duration=15s -profiling=true -enable_load_reader=true -port=30008 -max_procs=1 此进程ID，正是cadvisor的进程ID，其实这个进程，是容器内的进程，换句话说，其父进程，肯定是一个容器进程： 123[root@nodexxx]# ps -ef|grep 21663root 21663 2567 0 Oct30 ? 00:00:00 docker-containerd-shim 6f9ecad83132b77c7e0ce3bfe8e56b471c8c41b920970b3fdd6da4d158fb4b1e /var/run/docker/libcontainerd/6f9ecad83132b77c7e0ce3bfe8e56b471c8c41b920970b3fdd6da4d158fb4b1e docker-runcroot 21694 21663 7 Oct30 ? 01:05:16 /usr/bin/cadvisor -logtostderr -allow_dynamic_housekeeping=true -global_housekeeping_interval=1m0s -housekeeping_interval=5s -disable_metrics=udp,tcp,percpu,sched,disk,network -storage_duration=15s -profiling=true -enable_load_reader=true -port=30008 -max_procs=1 总结 Kubernete对资源的限制，靠的是Docker，Docker对资源的限制，靠的是Linux Cgroup 。 Linux Cgroup 限制资源，是限制进程，只需要在Cgroup配置目录的tasks文件中，添加进程ID，限制立即生效。 Linux Cgroup 不仅仅可以限制CPU，内存，还可以限制磁盘IO等。]]></content>
      <categories>
        <category>docker</category>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker ENTRYPOINT/FROM 对 docker stop 的影响]]></title>
    <url>%2F2018%2F10%2F28%2F6%2F</url>
    <content type="text"><![CDATA[背景之前有同事和我提过，在 Kubernetes 中，删除一个“应用”，有些应用删除比较慢。我们的“应用”，可以理解为一个 Deployment，删除应用，就是删除 Deployment，然后等待 Pod 全部退出。 当时其实没有太在意这个事情，因为 Kubernetes 的删除，从最细的粒度来看，就是删除 Pod，而删除 Pod，其实本质上就是停止容器，停止容器，本身其实会执行的是 docker stop 的过程，超时后，执行 docker kill 逻辑。 docker stop 是 docker daemon 进程，向 docker 容器进程，发送 kill -USR2 信号，而 docker kill，其实是一个 Kill -9 信号。换句话说，先让容器自行退出，一定时间内没有成功，再强制杀死。 当时觉得，有些容器停止慢，应该是容器业务比较繁重造成的，进程自行退出花费时间比较多而已。但今天看偶尔看 ENTRYPOINT 的的东西的时候，发现，也许这个问题，并没有那么简单。 ENTRYPOINT 的用法说明我们知道，在构建镜像的时候，可以指定程序入口。可以设置 ENTRYPOINT 和 CMD。这2个可以配合使用，也可以完全独立使用。我们本次，不关心CMD，只讨论 ENTRYPOINT 的不同写法，对容器的影响。 Dockerfile 中 ENTRYPOINT 的2个用法12345// 用法1ENTRYPOINT [&quot;your executable program&quot;, &quot;param1&quot;, &quot;param2&quot; ...]// 用法2ENTRYPOINT command param1 param2 ... 第一种写法，你可以自行定义某个二进制程序以及参数，作为容器的1号进程的相关启动内容。这种写法，也就是数组写法。 第二种写法，会将所设定的程序，限定在 /bin/sh -c 下执行。换句话说，这种方式，容器内会有2个进程，一个是 /bin/sh 进程，一个是真正的你的二进制程序的进程，它的进程ID，不是1。 注意1：Docker 官方，推荐第一种写法。 注意2：第二种写法，限定你的进程在 /bin/sh 下，是很多文章提到的，但这个说法，其实并不准确，后边我会测试说明。现在，我们先默认这句话是正确的。 需要特别说明的是： 如果你的进程不是1号进程，/bin/sh 是1号进程的话，会存在一个问题：/bin/sh 进程，不会处理 Linux 信号。这就导致，用第二种 ENTRYPOINT 的写法，就可能出现 docker stop 无法正常停止容器（当时等待 docker stop 超时后，docker daemon还是会发送 kill 信号的，这个可以保证容器停止并退出）。 好了，我们要用第二种写法，测试 docker stop 无法正常停止容器这个过程。 在开始之前，我们明确几个事情 我们要测试 Dockerfile 中 ENTRYPOINT 写法不同，对容器中进程的影响 我们的可执行程序，就用 top 我们要看这个影响，是否会间接影响到 docker stop 最后，我们看一下，如果基础镜像不同，是否测试结果也会不同，我们先用 ubuntu:latest 做基础镜像测试。最后再用 centos:7.5.1804 作为基础镜像测试。 开始测试使用 ubuntu 作为基础镜像做测试Dockerfile 内容如下: 12FROM ubuntu:latestENTRYPOINT top -b 执行下面命令，创建测试镜像，并运行为容器： 123docker build -t test-centos2 -f Dockerfile .$ docker run -it test-centos2 可以直接看到如下结果: 123456789top - 11:40:05 up 1 day, 19:34, 0 users, load average: 0.08, 0.05, 0.01Tasks: 2 total, 1 running, 1 sleeping, 0 stopped, 0 zombie%Cpu(s): 0.2 us, 0.3 sy, 0.0 ni, 99.6 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 stKiB Mem : 2046932 total, 333164 free, 234772 used, 1478996 buff/cacheKiB Swap: 1048572 total, 1048080 free, 492 used. 1607492 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 4500 696 628 S 0.0 0.0 0:00.17 sh 6 root 20 0 36528 3004 2644 R 0.0 0.1 0:00.01 top 这个结果说明： 容器内确实起了2个进程 1号进程就是 /bin/sh 我们的可执行程序，不是1号进程，而是 /bin/sh 进程的子进程，进程ID为6 那么，我们停止此容器。停止之前，先说一下，docker stop 会给容器发送SIG信号，让进程自行退出，如果进程不处理此信号，docker stop 会超时，然后 发送 kill 信号。默认超时时间是 10s ，我们执行如下命令： 1234time docker stop -t 30 e58dc2887ef8//输出：e58dc2887ef8docker stop -t 30 e58dc2887ef8 0.39s user 0.09s system 1% cpu 31.515 total 这个输出表明，docker stop 真的等了 30s 后才执行成功，也就是，/bin/sh 确实没有处理 SIG 信号。最后被 kill 掉了。 初期结论和说明我们要尽量避免上面的现象，就要保证容器的1号进程，是应用的真正可执行程序，不能是 /bin/sh 进程，否则，Kubernetes 删除 pod，就会等待一段时间才能执行成功。另外，我们还是应该尽量使用官方推荐的 ENTRYPOINT 写法（数组写法） 使用centos作为基础镜像测试但是，问题到此并没有结束。我们之前的 Dockerfile 是使用 ubuntu 作为基础镜像的。我们尝试，换为 centos 作为基础镜像试一下 12FROM centos:7.5.1804ENTRYPOINT top -b 然后，我们生成新镜像，并运行为容器： 123456789$ docker run -it test-centos3top - 16:11:26 up 1 day, 20:29, 0 users, load average: 0.00, 0.00, 0.00Tasks: 1 total, 1 running, 0 sleeping, 0 stopped, 0 zombie%Cpu(s): 33.3 us, 33.3 sy, 0.0 ni, 33.3 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 stKiB Mem : 2046932 total, 328508 free, 237228 used, 1481196 buff/cacheKiB Swap: 1048572 total, 1048080 free, 492 used. 1604860 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 56032 3668 3236 R 0.0 0.2 0:00.05 top 我们能看到，只有1个进程，且进程ID为1。我们直接进入到容器内部看一下： docker exec -it e8fa215e3ad6link123456[root@e8fa215e3ad6 /]# ps -efUID PID PPID C STIME TTY TIME CMDroot 1 0 0 16:11 pts/0 00:00:00 top -broot 6 0 3 16:12 pts/1 00:00:00 /bin/bashroot 20 6 0 16:12 pts/1 00:00:00 ps -ef[root@e8fa215e3ad6 /]# 从上，我们确实只能看到 top 进程为1号进程，并没有 /bin/sh 进程。而这个 Dockerfile 和之前的 Dockerfile，唯一的区别就是基础镜像不同。 结论 /bin/sh 进程是无法处理 Linux 信号的。 不论使用哪种镜像做应用镜像的基础镜像，都要注意构建完应用镜像后测试一下，最好不要让 /bin/sh 成为 1 号进程。 尽量在 Dockerfile 中，为 ENTRYPOINT、CMD、RUN 使用数组方式写法。]]></content>
      <categories>
        <category>docker</category>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cAdvisor内存占用不断飙升导致其在k8s内不断crash问题排查]]></title>
    <url>%2F2018%2F10%2F24%2F5%2F</url>
    <content type="text"><![CDATA[背景我们的额监控方案为：Kubernetes（K8S）+cAdvisor+Prometheus+Grafana。然后，用cAdivor监控容器信息，其实，cAdivor其实到现在的主流K8S版本中，Kubelet进程已经将其内置了，但是我们没有这么用，因为没有必要因为让Prometheus定期去Kubelet上采集容器信息，平白增添对Kubelet的压力。相反，我觉得，还是应该还是应该单独部署cAdvisor，这样一来，不论是定制化cAdvisor，还是版本更新，都会更方面。所以，我使用DaemonSet部署了cAdvisor。 问题用DaemonSet的方式部署cAdvisor，本质上，就是每个K8S的宿主机都启动了一个pod，实际观测，发现这些Pod的状态，会随着时间的推移，开始频繁出现Crash。这个问题，势必会导致cAdvisor无法正常监控容器信息。下面是具体的排查过程。 排查初探首先，Pod Crash 必然有其原因，所以，一开始是通过下面的方式，看cAdvisor到底为何会Crash，通过1kubectl describe pod -n monitoring pod-xxxxx 找到Last State部分，发现其为：1State: OOMKilled 这说明，这个 Pod，是因为内存不够，cAdvisor在运行过程，超出了Pod的资源限制，被OOM杀掉了。既然资源不够，那么首先，就是调大其内存限制。 一开始为这个Pod设置的上限资源为1核CPU+1G内存，既然内存无法满足，那么调大为2G，继续观测，发现依然会OOM。然后又调整为3G、4G、10G、20G（机器内存大，土豪），发现虽然内存变大了会有一些缓解，但实际上，即使内存上限设置为20G，还是会有Crash的情况，那么，这时候就需要反思一下几个问题了： 是否是cAdvisor存在bug？ 哪个机器上的cAdvisor Pod总是重启？ 排查是否是cAdvisor版本问题针对第一点，我们升级了cAdivor镜像为最新版，问题依旧。 排查是否是cAdvisor参数配置问题google一些文章，有人提过类似的问题，官方issue的解释中，有人提到可能配置不对，可能采集的指标过多等，于是，我review了一下我的配置，调整后的完整配置如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697apiVersion: extensions/v1beta1kind: DaemonSetmetadata: labels: name: cadvisor name: cadvisor namespace: monitoringspec: revisionHistoryLimit: 10 selector: matchLabels: name: cadvisor template: metadata: annotations: prometheus.io/port: &quot;28762&quot; prometheus.io/scrape: &quot;true&quot; creationTimestamp: null labels: name: cadvisor spec: automountServiceAccountToken: false containers: - args: - -allow_dynamic_housekeeping=true - -global_housekeeping_interval=1m0s - -housekeeping_interval=3s - -disable_metrics=udp,tcp,percpu,sched - -storage_duration=15s - -profiling=true - -port=28762 - -max_procs=1 image: mine/cadvisor-test:v0.0.2 imagePullPolicy: IfNotPresent name: cadvisor ports: - containerPort: 28762 hostPort: 28762 name: http protocol: TCP resources: limits: cpu: &quot;1&quot; memory: 3000Mi requests: cpu: &quot;1&quot; memory: 500Mi terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /rootfs name: rootfs readOnly: true - mountPath: /var/run name: var-run readOnly: true - mountPath: /sys name: sys readOnly: true - mountPath: /var/lib/docker name: docker readOnly: true - mountPath: /dev/disk name: disk readOnly: true dnsPolicy: ClusterFirst hostNetwork: true restartPolicy: Always schedulerName: default-scheduler securityContext: &#123;&#125; terminationGracePeriodSeconds: 30 volumes: - hostPath: path: / type: &quot;&quot; name: rootfs - hostPath: path: /var/run type: &quot;&quot; name: var-run - hostPath: path: /sys type: &quot;&quot; name: sys - hostPath: path: /DATA/docker type: &quot;&quot; name: docker - hostPath: path: /dev/disk type: &quot;&quot; name: disk templateGeneration: 6 updateStrategy: rollingUpdate: maxUnavailable: 1 type: RollingUpdate 我调整的部分主要集中在： 12345678// 这个是禁用哪些指标，默认只有udp、tcp- -disable_metrics=udp,tcp,percpu,sched// 存储最近多久的数据，原来是1分多钟，调整为15s- -storage_duration=15s// 是否开启性能测试，默认为关闭，之所以开启，是要一会儿debug内存占用- -profiling=true// 使用多少CPU，默认不到1个- -max_procs=1 上面的方式，是减少了一些采集指标，以及采集数据的最多保留时长，稍微有些效果，但是发现效果不大，原来某些机器上频繁Crash的cAdvisor Pod，还是Crash，另外某些机器上从来不Crash的，也不会Crash。那么，说明参数配置没什么用，问题应该出现某些机器上。 排查为何cAdivosr Pod在某些机器上Crash我回顾了一下我们的K8S节点，发现cAdvisor Pod不OOM的机器上面，容器都比较少。越是容器多的机器，这机器上的cAdvisor Pod就越容易OOM Crash。 那么，我们看一下 cAdvisor 的 Pod 日志，发现其频繁报一个错误： 1fsHandler.go:135] du and find on following dirs took 57.562700809s: [/rootfs/DATA/docker/overlay2/d8c002c4dc33c22129124e70bf7ca15fd312cd8867c040708d11d7d462ee58df/diff /rootfs/DATA/docker/containers/16eb9120ce2da24d867ee287c093ce7221f1d3ed39e69c3a8d128313a5dc0d63]; will not log again for this container unless duration exceeds 4s 这说明，cAdvisor会统计每一个容器占用的磁盘使用大小，这个大小是通过du命令来处理的，而且，这个统计耗费的时间很长。我们可以实际去看一下，发现这个目录，确实比较大，有些在2-3G。这说明，这个机器上，必然存在一些容器，里边在搞事情，写了很多的文件，导致 du 命令在统计的时候，比较耗时。 问题初步总结K8S节点，有些容器存储或写入了比较多的文件，造成cAdvisor统计容器磁盘使用耗时，进而引发此cAdivosr内存占用升高。 排查深入探究既然上面已经初步定为问题，但是我们依然会疑惑，为什么cAdivosr统计容器磁盘耗时会引发内存飙升呢？ 我们需要借助一些工具来进一步排查 通过 go tool pprof 分析内存 通过查看 cAdvisor 源码分析流程 在源码中，打断点，验证猜想 通过 go tool pprof 分析内存首先，将 DaemonSet 启动的 cAdvisor，使用 Host 模式启动，这样我们就可以直接通过访问宿主机上，cAdvisor开放的端口，来做性能采样了。 1go tool pprof -cum -svg -alloc_space http://x.x.x.x:28762/debug/pprof/heap 上面的步骤，会生成内存性能采样图，类似如下： 详细采样图，可以通过此连接查看： 采样图全 从图中，先看红色部分，颜色越深，表示这部分资源消耗越严重，我们这个采样图是采集的内存，可以看到，有 2366.70M，是 Gather 函数的，但其实，这个函数本身，并没有多少内存消耗，它的内存占用这么大，是 collectContainersInfo 函数分配的。其实不论怎样，Gather函数都脱离不了干系。那么，我们从源码看一下 源码分析首先，入口函数main中，注册了/metrics对应的handler，因为cAdvisor要开发 /metirics路径，让 Prometheus 来定时采集 1234567891011// cadvisor.go#82func main() &#123; defer glog.Flush() flag.Parse() //注册HTTP路径 *prometheusEndpoint 值就是 /metirics cadvisorhttp.RegisterPrometheusHandler(mux, containerManager, *prometheusEndpoint, containerLabelFunc, includedMetrics) glog.V(1).Infof(&quot;Starting cAdvisor version: %s-%s on port %d&quot;, version.Info[&quot;version&quot;], version.Info[&quot;revision&quot;], *argPort) addr := fmt.Sprintf(&quot;%s:%d&quot;, *argIp, *argPort) glog.Fatal(http.ListenAndServe(addr, mux))&#125; 然后，看一下，是谁在处理 /metrics 路由对应的操作 123456789101112// 代码文件：http/handler.go#97func RegisterPrometheusHandler(mux httpmux.Mux, containerManager manager.Manager, prometheusEndpoint string, f metrics.ContainerLabelsFunc, includedMetrics container.MetricSet) &#123; r := prometheus.NewRegistry() r.MustRegister( metrics.NewPrometheusCollector(containerManager, f, includedMetrics), prometheus.NewGoCollector(), prometheus.NewProcessCollector(os.Getpid(), &quot;&quot;), ) //可以看到，真正执行 /metrics 的函数，是 promhttp.HandlerFor mux.Handle(prometheusEndpoint, promhttp.HandlerFor(r, promhttp.HandlerOpts&#123;ErrorHandling: promhttp.ContinueOnError&#125;))&#125; 可以看到，真正执行 /metrics 的函数，是promhttp.HandlerFor，具体深入HandlerFor看一下 12345678// 代码文件：vendor/github.com/prometheus/client_golang/prometheus/promhttp/http.go#82func HandlerFor(reg prometheus.Gatherer, opts HandlerOpts) http.Handler &#123; return http.HandlerFunc(func(w http.ResponseWriter, req *http.Request) &#123; //这里就是真正的 Gather 调用 mfs, err := reg.Gather() ... &#125;)&#125; 至此，可以说明，每一次HTTP调用（调用 x.x.x.x:8080/metrics），都会又一次Gather调用。 所以我们猜想，之所以Gather函数有这么大的内存占用，主要是因为Gather函数调用次数多，而每次Gather函数执行之间长，导致形成了并发调用，这种情况下，Gather函数从执行到结束期间，都不会释放内存，并发调用，就会导致内存积压。 修改源码，重新构建部署，验证猜想那么，我们在Gather调用处，打断点，看一下执行时间： 123456789101112// 代码文件：vendor/github.com/prometheus/client_golang/prometheus/promhttp/http.go#82func HandlerFor(reg prometheus.Gatherer, opts HandlerOpts) http.Handler &#123; return http.HandlerFunc(func(w http.ResponseWriter, req *http.Request) &#123; pp.Println(&quot;请求开始————————&quot;) start:=time.Now() //这里就是真正的 Gather 调用 mfs, err := reg.Gather() ... timeCost := time.Since(start) pp.Println(fmt.Sprintf(&quot;请求结束，耗时 %v&quot;, timeCost)) &#125;)&#125; 我们打印了Gather执行的耗时，然后重新构建 cAdvisor源码，打一个私有镜像出来，推送到私有镜像仓库。然后我们使用这个测试镜像，重新部署cAdvisor。 现在，我们挑一台之前cAdvisor频发OOM Crash的机器，看一下它的log 1kubectl logs -n monitoring cadvisor-k9kpt -f 日志输出大致如下： 123456&quot;请求开始————————&quot;I1023 14:21:19.126794 1 fsHandler.go:135] du and find on following dirs took 15.420205027s: [/rootfs/var/lib/docker/overlay2/67ec1868b2c0ed5ce5b22ee014eb4d08993accd68546a3de6aa2a6355bdc1a78/diff /rootfs/var/lib/docker/containers/cd910753386b3325af8bd5a69fc01b261ca14c1bfaf754677662e903b755d34f]; will not log again for this container unless duration exceeds 56sI1023 14:21:19.305938 1 fsHandler.go:135] du and find on following dirs took 15.278733582s: [/rootfs/var/lib/docker/overlay2/10621b60f26962cb1a90d7a7dc1ce4e3c8a15f6e4e30861b8433c5c37727bb9e/diff /rootfs/var/lib/docker/containers/b2a4d11c37aa9c63b4759c5728956253fad46fa174c7fe4d91336a4ac7532127]; will not log again for this container unless duration exceeds 1m34sI1023 14:21:19.827757 1 fsHandler.go:135] du and find on following dirs took 13.897447077s: [/rootfs/var/lib/docker/overlay2/29b3b0dfc22053937e9c40e004a6d31af489573ff3a385020feb22d88d1a3d0a/diff /rootfs/var/lib/docker/containers/af962971a0643418d28c03b374e31a0c58dd6302524ea06dc8a23c4eccf5d663]; will not log again for this container unless duration exceeds 1m20sI1023 14:21:20.042949 1 fsHandler.go:135] du and find on following dirs took 14.514122984s: [/rootfs/var/lib/docker/overlay2/27f1d3cb3d421567754cb7abb986c16c3f3bec0874e983a2604aa7eda8834d9a/diff /rootfs/var/lib/docker/containers/60cad8688e31b557e2e98c47beaa1f3af2ea2e6cbfab0c1f399887b3eecec86c]; will not log again for this container unless duration exceeds 1m56s&quot;请求结束，耗时 58.093771464s&quot; 日志其实我只是截图了一部分，基本上可以看出来，Gather请求十分耗时，这个耗时，就是由 du 操作耗时造成的，有时候，du 耗时非常严重，能将近2分钟。 这样，基本上，就印证了，Gather函数处理慢，而Prometheus每隔3s请求一次，造成同时有非常多的 Gather函数在并发处理，也就导致了内存积压的情况。 彻底解决综上，其实我们只需要让 du 磁盘统计快了就可以了，du 的快慢，是一个CPU密集和磁盘IO密集的操作，要加快 du 操作，就需要给到足够的运算能力。 回顾之前我们的 cAdvisor 的 DaemonSet 的yaml配置，我们在资源的 limit 部分，仅给到了一个 CPU，我们加 CPU 核数增加到6。如下： 1234567resources: limits: cpu: &quot;6&quot; memory: 3000Mi requests: cpu: &quot;2&quot; memory: 500Mi 然后，更新 DaemonSet 部署 1kubectl apply -f cadvisor.ds.yaml 再次去观察 cAdvisor 的pod日志，发现du耗时明显缩短到2秒钟以内，pod内存占用过高的情况，再也没有出现过。问题得解！]]></content>
      <categories>
        <category>kubernetes</category>
        <category>cAdvisor</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>kubernetes</tag>
        <tag>cAdvisor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何解除github repo的 froked from xxx/yyy 提示]]></title>
    <url>%2F2018%2F10%2F16%2F4%2F</url>
    <content type="text"><![CDATA[场景在某些情况下，我们有些需要克隆某个人的repo。 可能需要然后进行修改，提Pull Request进行代码合并，或者克隆repo后作为我们自己的项目做二次定制。 然而，一旦从其他人的repo上克隆为我们自己的repo，则项目名称下会一直提示 forked from xxx/yyy。有时候，出于一些原因，我们需要彻底解除我们的repo与原来repo之间的绑定关系。 我自己的场景，是博客repo从其他人的repo做了克隆，后期我完全重构了这个博客的主题选择等内容，基本和原来的那个远程repo完全不相干了，这时候，博客这个repo总是看到 forked from xxx/yyy 感觉有些扎眼。 解除方式方式1最笨拙的解除方式，就是将我们的远程repo删除，重新创建repo提交我们的代码文件。这种方式操作暴力，而且有些麻烦。我们之所以说它麻烦，是因为有时候，我们已经对这个repo做了一些自己的设置了，如果把repo删除重建并提交代码，还得再重新给这个repo做设置，比较繁琐。 方式2除了上面的方式，我们还可以把解除操作的步骤，交给github官方来做。具体来说，就是提交一个“工单”。地址：https://github.com/contact进入地址后，向下拉动，找到工单填写的地方。其中：Name：填写你的用户名Email：选择你的邮箱 Subject：这是工单标题，可以这样写：detach my repo aaa/bbb from the repo xxx/yyy How can we help?这部分需要你填写详细的求助说明，可以这样写：Please detach my repo aaa/bbb from the repo xxx/yyy, Thanks for helping! 填写完之后，直接点击 send request 按钮提交，等待github官方处理即可，一般情况下，半天时间内官方就会给予处理。]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s线上集群的机器，重启iptables后，造成CI系统（drone）构建偶发失败，docker run容器无法访问外网问题排查记录]]></title>
    <url>%2F2018%2F10%2F16%2F3%2F</url>
    <content type="text"><![CDATA[引子线上的k8s集群内部，每一台机器，都有2块网卡，一块儿em1（10网段），一个 em2（192网段）。然后，线上的机器，通过iptables限制为：访问192段的服务任意端口都是畅通的，而访问10段的服务端口，只有333端口可以连通。333端口，是线上机器的ssh端口。 线上集群的k8s，使用的calico网络。 线上有一个问题，就是容器访问当前宿主机192段IP的999端口不通，访问其他机器192段IP的9999端口畅通。 我们知道，当从容器访问宿主机的应用时，会重新进入iptables的INPUT链，而线上的iptables INPUT链如下： 1234567891011Chain INPUT (policy ACCEPT 0 packets, 0 bytes)num pkts bytes target prot opt in out source destination 1 1477K 2519M cali-INPUT all -- * * 0.0.0.0/0 0.0.0.0/0 /* cali:Cz_u1IQiXIMmKD4c */2 272K 20M KUBE-SERVICES all -- * * 0.0.0.0/0 0.0.0.0/0 /* kubernetes service portals */3 273K 20M KUBE-FIREWALL all -- * * 0.0.0.0/0 0.0.0.0/0 4 263K 20M ACCEPT all -- * * 0.0.0.0/0 0.0.0.0/0 state RELATED,ESTABLISHED5 0 0 ACCEPT icmp -- * * 0.0.0.0/0 0.0.0.0/0 6 393 17292 ACCEPT all -- lo * 0.0.0.0/0 0.0.0.0/0 7 11880 1019K ACCEPT all -- em2 * 0.0.0.0/0 0.0.0.0/0 8 7 308 ACCEPT tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:3339 228 62052 REJECT all -- * * 0.0.0.0/0 0.0.0.0/0 reject-with icmp-host-prohibited 意思是： ①：如果报文是途径em2网卡，访问任意其他网卡。则全部放行。（这就是为什么192段的访问全部放行） ②：如果报文是tcp，不管报文从哪个网卡出来，去往哪个网卡，只要目的端口是333，就放行。（这就是为什么10段的访问，只能访问333） 我们容器发出的报文，网卡是 cal 网卡（calico的网卡名称），所以，容器只能访问333。 错误操作要把9999端口放开，意味着，我要添加开放9999端口到INPUT链路中，为了让iptables再重启后依然生效，我将规则，添加到 1/etc/sysconfig/iptables 在添加完成后其实并未生效，然后，有2种方式，用于让iptables生效： ①：手动添加规则，比如 iptables -t filter -I INPUT 9 -s 172.20.0.0/14 -p tcp -m tcp –dport 9999 -j ACCEPT ②：重启iptables（会自动加载 /etc/sysconfig/iptables内容 ） 我选择了后者，之所以选择后者，其实是因为，即便重启了iptables，加载了刚刚的规则，原来的k8s和calico的iptables会丢失，但是，k8s会重建自己的整套iptables规则的。 事实上，k8s确实也在我重启iptables后，重建了iptables，但问题来了。 引发的2个网络问题1、我们线上的CI系统，是基于drone，做的二次开发，改动了很多的东西，添加了很多特性，结果上面一系列操作之后，我发现，我们的CI系统中，个别镜像的构建（Dockerfile中的RUN操作，有网络相关操作时，比如yum）会失败。 2、线上的机器，执行 docker run，起来的容器，无法访问外网了。然而，k8s起来的pod里的容器，却可以访问外网。 镜像的构建，其实本质上，也是起容器来做事情，而且，你无法定制docker build操作时，使用的网络模式，只能是bridge模式。而 docker run 起来的容器，默认也是 bridge 模式。结合上面2个内容，基本上可以确定，此问题，和k8s无关，而是纯粹的docker的问题了。 排查①：一开始以为是k8s的iptables重建不完整，所以，多次用线上的机器的机器 nodeA（无人访问的一台），不断尝试清空iptables，等待k8s重建iptables，多次尝试无效。 ②：stop docker -> stop kubelet -> clean iptables -> start docker -> start kubelet。这个操作的意义是，以为重启docker，重启k8s，docker 和 k8s 都会重建自己的 iptables。多次尝试无效。 ③：使用现在的k8s安装工具：kubespray，重新安装 nodeA，并添加到k8s集群内。寄希望于安装工具重装docker和k8s，结果依然无效。 ④：回归问题本身，问题一定出在iptables上。也是因为一开始重启了iptables导致此一系列问题。 解决过程既然问题出在iptables上，而我们要访问外网，那么，就得重iptables 报文转出入手。好在线上的机器，并没有都为了适配9999端口重启iptables，有3台 k8s 的 master 节点，并没有重启 iptables。 然后，对比有问题机器、没有问题机器的 FORWARD、POSTROUTING 的规则。发现，缺少DOCKER相关的规则，这些规则，本来是安装docker之后，由docker创建的。那么，解决此问题，就需要重建docker的iptables。 ①：找一台干净的机器（只安装了docker，未启动容器，为安装k8s），然后从中提取docker相关的规则 ②：将提取的规则，与出问题的机器的iptables默认规则融合。 ③：执行 iptables-restore 恢复规则。 后续验证在问题机器上，docker run centos 容器，然后ping qq.com，链路畅通。有一个小问题，就是如果你先启动的容器执行 ping 外网操作。然后才再目的机器上执行 iptables-restore 的话，会发现，容器的 ping，有一个短暂的耗时，然后才会畅通，这是因为 iptables-restore 恢复规则后，有一个过程。 最后我整理一个相对标准的docker iptables规则备份，如果你也遇到了启动的docker容器，无法访问外网的问题，可以通过下面的内容，恢复规则 将下面的规则，保存到一个文件，比如 docker-iptables-backup 123456789101112131415161718192021222324252627# Generated by iptables-save v1.4.21 on Thu Apr 30 20:48:42 2015*nat:PREROUTING ACCEPT [18:1080]:INPUT ACCEPT [18:1080]:OUTPUT ACCEPT [22:1550]:POSTROUTING ACCEPT [22:1550]:DOCKER - [0:0]-A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER-A OUTPUT ! -d 127.0.0.0/8 -m addrtype --dst-type LOCAL -j DOCKER-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE-A POSTROUTING -s 172.17.0.1/32 -d 172.17.0.1/32 -p tcp -m tcp --dport 80 -j MASQUERADE-A DOCKER ! -i docker0 -p tcp -m tcp --dport 3001 -j DNAT --to-destination 172.17.0.1:80COMMIT# Completed on Thu Apr 30 20:48:42 2015# Generated by iptables-save v1.4.21 on Thu Apr 30 20:48:42 2015*filter:INPUT ACCEPT [495:53218]:FORWARD ACCEPT [0:0]:OUTPUT ACCEPT [480:89217]:DOCKER - [0:0]-A FORWARD -o docker0 -j DOCKER-A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT-A FORWARD -i docker0 ! -o docker0 -j ACCEPT-A FORWARD -i docker0 -o docker0 -j ACCEPT-A DOCKER -d 172.17.0.1/32 ! -i docker0 -o docker0 -p tcp -m tcp --dport 80 -j ACCEPTCOMMIT# Completed on Thu Apr 30 20:48:42 2015 然后，执行恢复 iptables-restore docker-iptables-backup 注意： ①：如果你的docker在k8s集群内部，理论上来说，上面的操作会有清空iptables的能力，但是你不需要担心，因为k8s会自行修复本身的iptables，也就是说，k8s会重建自己的iptables。但是假如你的k8s没能完整重建，就需要手动恢复k8s的iptables。如果是calico网络，只需要删除当前节点的calico pod即可，删除之后，k8s会重启这个pod，自然也就会重建iptables了。同理，flannel网络也是一样的。 ②：上面的规则，有一个关键，是 172.17.0.0 和 172.17.0.1，这是docker默认安装后的网段和docker0网卡的IP。如果你是默认安装的docker，上面的规则不需要改动，但如果你定制了docker0网卡的IP，记得修改一下上面的规则，适配你的环境。]]></content>
      <categories>
        <category>docker</category>
        <category>drone</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>drone</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于XFS文件系统的overlayfs下使用docker，为何要使用d_type=1]]></title>
    <url>%2F2018%2F10%2F16%2F2%2F</url>
    <content type="text"><![CDATA[什么是overlayfs首先，overlayfs是一种文件系统，也是目前dokcer在使用的最新的文件系统，其他的文件系统还有：aufs、device mapper等。而 overlayfs 其实和 aufs 是类似的。更准确的说，overlayfs，其实是 Linux 文件系统的一种上层文件系统。下面的底层的文件系统格式，是支持overlayfs的： ①：ext4 ②：xfs（必须在格式化为xfs的是，指定ftype=1，如果在 未使用ftype=1的方式格式化的xfs文件系统上使用，否则docker可能出现未知问题） 如何查看当前操作是否支持overlay1lsmod |grep over 如果没有输出，表示不支持，可以通过下面的命令开启overlay 1modprobe overlay 需要注意的是： ①：docker官方，建议使用 overlay2，而不是 overlay，因为 overlay2 更高效。要使用 overlay2的话，需要 Linux 内核在版本4以上。 ②：docker 官方建议，在 docker-ee 17.06.02及以上的版本使用 overlay2，以及，在docker-ce的版本，也使用 overlay2。而 overlay 虽然在 docker-ce 版本中是支持的，但是并不推荐。 ③：只要当前操作系统支持overlay，那docker就可以使用overlay或者overlay2了。 ④：指定docker的overlay2驱动，需要在启动docker的时候，指定 –storage-driver 参数，或者，在配置文件 /etc/docker/daemon.json 中 ，指定驱动配置 123456&#123; "storage-driver": "overlay2", "storage-opts": [ "overlay2.override_kernel_check=true" ]&#125; xfs文件系统的 d_type是什么d_type 是 Linux 内核的一个术语，表示 “目录条目类型”，而目录条目，其实是文件系统上目录信息的一个数据结构。d_type，就是这个数据结构的一个字段，这个字段用来表示文件的类型，是文件，还是管道，还是目录还是套接字等。 d_type 从 Linux 2.6 内核开始就已经支持了，只不过虽然 Linux 内核虽然支持，但有些文件系统实现了 d_type，而有些，没有实现，有些是选择性的实现，也就是需要用户自己用额外的参数来决定是否开启d_type的支持。 为什么docker在overlay2（xfs文件系统）需要d_type不论是 overlay，还是 overlay2，它们的底层文件系统都是 overlayfs 文件系统。而 overlayfs 文件系统，就会用到 d_type 这个东西用来文件的操作是被正确的处理了。换句话说，docker只要使用 overlay 或者 overlay2，就等于在用 overlayfs，也就一定会用到 d_type。所以，docker 提供了 1docker info 此命令，用来检测你docker服务，是否在使用overlay的时候正确的使用 d_type。如果用了 overlay/overlay2，但 d_type 没有开，就报警告。 如果在不支持 d_typ 的 overlay/overlay 驱动下使用docker，也就意味着 docker 在操作文件的时候，可能会遇到一些错误，比如 无法删除某些目录或文件，设置文件或目录的权限或用户失败等等。这些都是不可预料的错误。举个具体的场景，就是，docker构建的时候，可能在构建过程中，删除文件等操作失败，导致构建停止。 如何检测当前的文件系统，是否支持 d_type ？ 1xfs_info / 它用于检测指定挂载点的文件xfs文件系统的信息。如果你的文件系统是 xfs，则会提示类似如下信息 12345678910$ xfs_info /meta-data=/dev/sda1 isize=256 agcount=4, agsize=3276736 blks = sectsz=512 attr=2, projid32bit=1 = crc=0 finobt=0 spinodes=0data = bsize=4096 blocks=13106944, imaxpct=25 = sunit=0 swidth=0 blksnaming =version 2 bsize=4096 ascii-ci=0 ftype=1log =internal bsize=4096 blocks=6399, version=2 = sectsz=512 sunit=0 blks, lazy-count=1realtime =none extsz=4096 blocks=0, rtextents=0 注意其中的 ftype，1表示支持 d_type，0表示不支持。 参考： https://linuxer.pro/2017/03/fix-chown-error-discourse-bootstrap/ https://linuxer.pro/2017/03/what-is-d_type-and-why-docker-overlayfs-need-it/ https://blog.csdn.net/zxf_668899/article/details/54667521 https://docs.docker.com/storage/storagedriver/overlayfs-driver/]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>ovlerlayfs</tag>
      </tags>
  </entry>
</search>
