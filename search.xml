<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Docker ENTRYPOINT/FROM 对 docker stop 的影响]]></title>
    <url>%2F2018%2F10%2F28%2F6%2F</url>
    <content type="text"><![CDATA[背景之前有同事和我提过，在 Kubernetes 中，删除一个“应用”，有些应用删除比较慢。我们的“应用”，可以理解为一个 Deployment，删除应用，就是删除 Deployment，然后等待 Pod 全部退出。 当时其实没有太在意这个事情，因为 Kubernetes 的删除，从最细的粒度来看，就是删除 Pod，而删除 Pod，其实本质上就是停止容器，停止容器，本身其实会执行的是 docker stop 的过程，超时后，执行 docker kill 逻辑。 docker stop 是 docker daemon 进程，向 docker 容器进程，发送 kill -USR2 信号，而 docker kill，其实是一个 Kill -9 信号。换句话说，先让容器自行退出，一定时间内没有成功，再强制杀死。 当时觉得，有些容器停止慢，应该是容器业务比较繁重造成的，进程自行退出花费时间比较多而已。但今天看偶尔看 ENTRYPOINT 的的东西的时候，发现，也许这个问题，并没有那么简单。 ENTRYPOINT 的用法说明我们知道，在构建镜像的时候，可以指定程序入口。可以设置 ENTRYPOINT 和 CMD。这2个可以配合使用，也可以完全独立使用。我们本次，不关心CMD，只讨论 ENTRYPOINT 的不同写法，对容器的影响。 Dockerfile 中 ENTRYPOINT 的2个用法12345// 用法1ENTRYPOINT [&quot;your executable program&quot;, &quot;param1&quot;, &quot;param2&quot; ...]// 用法2ENTRYPOINT command param1 param2 ... 第一种写法，你可以自行定义某个二进制程序以及参数，作为容器的1号进程的相关启动内容。这种写法，也就是数组写法。 第二种写法，会将所设定的程序，限定在 /bin/sh -c 下执行。换句话说，这种方式，容器内会有2个进程，一个是 /bin/sh 进程，一个是真正的你的二进制程序的进程，它的进程ID，不是1。 注意1：Docker 官方，推荐第一种写法。 注意2：第二种写法，限定你的进程在 /bin/sh 下，是很多文章提到的，但这个说法，其实并不准确，后边我会测试说明。现在，我们先默认这句话是正确的。 需要特别说明的是： 如果你的进程不是1号进程，/bin/sh 是1号进程的话，会存在一个问题：/bin/sh 进程，不会处理 Linux 信号。这就导致，用第二种 ENTRYPOINT 的写法，就可能出现 docker stop 无法正常停止容器（当时等待 docker stop 超时后，docker daemon还是会发送 kill 信号的，这个可以保证容器停止并退出）。 好了，我们要用第二种写法，测试 docker stop 无法正常停止容器这个过程。 在开始之前，我们明确几个事情 我们要测试 Dockerfile 中 ENTRYPOINT 写法不同，对容器中进程的影响 我们的可执行程序，就用 top 我们要看这个影响，是否会间接影响到 docker stop 最后，我们看一下，如果基础镜像不同，是否测试结果也会不同，我们先用 ubuntu:latest 做基础镜像测试。最后再用 centos:7.5.1804 作为基础镜像测试。 开始测试使用 ubuntu 作为基础镜像做测试Dockerfile 内容如下: 12FROM ubuntu:latestENTRYPOINT top -b 执行下面命令，创建测试镜像，并运行为容器： 123docker build -t test-centos2 -f Dockerfile .$ docker run -it test-centos2 可以直接看到如下结果: 123456789top - 11:40:05 up 1 day, 19:34, 0 users, load average: 0.08, 0.05, 0.01Tasks: 2 total, 1 running, 1 sleeping, 0 stopped, 0 zombie%Cpu(s): 0.2 us, 0.3 sy, 0.0 ni, 99.6 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 stKiB Mem : 2046932 total, 333164 free, 234772 used, 1478996 buff/cacheKiB Swap: 1048572 total, 1048080 free, 492 used. 1607492 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 4500 696 628 S 0.0 0.0 0:00.17 sh 6 root 20 0 36528 3004 2644 R 0.0 0.1 0:00.01 top 这个结果说明： 容器内确实起了2个进程 1号进程就是 /bin/sh 我们的可执行程序，不是1号进程，而是 /bin/sh 进程的子进程，进程ID为6 那么，我们停止此容器。停止之前，先说一下，docker stop 会给容器发送SIG信号，让进程自行退出，如果进程不处理此信号，docker stop 会超时，然后 发送 kill 信号。默认超时时间是 10s ，我们执行如下命令： 1234time docker stop -t 30 e58dc2887ef8//输出：e58dc2887ef8docker stop -t 30 e58dc2887ef8 0.39s user 0.09s system 1% cpu 31.515 total 这个输出表明，docker stop 真的等了 30s 后才执行成功，也就是，/bin/sh 确实没有处理 SIG 信号。最后被 kill 掉了。 初期结论和说明我们要尽量避免上面的现象，就要保证容器的1号进程，是应用的真正可执行程序，不能是 /bin/sh 进程，否则，Kubernetes 删除 pod，就会等待一段时间才能执行成功。另外，我们还是应该尽量使用官方推荐的 ENTRYPOINT 写法（数组写法） 使用centos作为基础镜像测试但是，问题到此并没有结束。我们之前的 Dockerfile 是使用 ubuntu 作为基础镜像的。我们尝试，换为 centos 作为基础镜像试一下 12FROM centos:7.5.1804ENTRYPOINT top -b 然后，我们生成新镜像，并运行为容器： 123456789$ docker run -it test-centos3top - 16:11:26 up 1 day, 20:29, 0 users, load average: 0.00, 0.00, 0.00Tasks: 1 total, 1 running, 0 sleeping, 0 stopped, 0 zombie%Cpu(s): 33.3 us, 33.3 sy, 0.0 ni, 33.3 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 stKiB Mem : 2046932 total, 328508 free, 237228 used, 1481196 buff/cacheKiB Swap: 1048572 total, 1048080 free, 492 used. 1604860 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 56032 3668 3236 R 0.0 0.2 0:00.05 top 我们能看到，只有1个进程，且进程ID为1。我们直接进入到容器内部看一下： docker exec -it e8fa215e3ad6link123456[root@e8fa215e3ad6 /]# ps -efUID PID PPID C STIME TTY TIME CMDroot 1 0 0 16:11 pts/0 00:00:00 top -broot 6 0 3 16:12 pts/1 00:00:00 /bin/bashroot 20 6 0 16:12 pts/1 00:00:00 ps -ef[root@e8fa215e3ad6 /]# 从上，我们确实只能看到 top 进程为1号进程，并没有 /bin/sh 进程。而这个 Dockerfile 和之前的 Dockerfile，唯一的区别就是基础镜像不同。 结论 /bin/sh 进程是无法处理 Linux 信号的。 不论使用哪种镜像做应用镜像的基础镜像，都要注意构建完应用镜像后测试一下，最好不要让 /bin/sh 成为 1 号进程。 尽量在 Dockerfile 中，为 ENTRYPOINT、CMD、RUN 使用数组方式写法。]]></content>
      <categories>
        <category>docker</category>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cAdvisor内存占用不断飙升导致其在k8s内不断crash问题排查]]></title>
    <url>%2F2018%2F10%2F24%2F5%2F</url>
    <content type="text"><![CDATA[背景我们的额监控方案为：Kubernetes（K8S）+cAdvisor+Prometheus+Grafana。然后，用cAdivor监控容器信息，其实，cAdivor其实到现在的主流K8S版本中，Kubelet进程已经将其内置了，但是我们没有这么用，因为没有必要因为让Prometheus定期去Kubelet上采集容器信息，平白增添对Kubelet的压力。相反，我觉得，还是应该还是应该单独部署cAdvisor，这样一来，不论是定制化cAdvisor，还是版本更新，都会更方面。所以，我使用DaemonSet部署了cAdvisor。 问题用DaemonSet的方式部署cAdvisor，本质上，就是每个K8S的宿主机都启动了一个pod，实际观测，发现这些Pod的状态，会随着时间的推移，开始频繁出现Crash。这个问题，势必会导致cAdvisor无法正常监控容器信息。下面是具体的排查过程。 排查初探首先，Pod Crash 必然有其原因，所以，一开始是通过下面的方式，看cAdvisor到底为何会Crash，通过1kubectl describe pod -n monitoring pod-xxxxx 找到Last State部分，发现其为：1State: OOMKilled 这说明，这个 Pod，是因为内存不够，cAdvisor在运行过程，超出了Pod的资源限制，被OOM杀掉了。既然资源不够，那么首先，就是调大其内存限制。 一开始为这个Pod设置的上限资源为1核CPU+1G内存，既然内存无法满足，那么调大为2G，继续观测，发现依然会OOM。然后又调整为3G、4G、10G、20G（机器内存大，土豪），发现虽然内存变大了会有一些缓解，但实际上，即使内存上限设置为20G，还是会有Crash的情况，那么，这时候就需要反思一下几个问题了： 是否是cAdvisor存在bug？ 哪个机器上的cAdvisor Pod总是重启？ 排查是否是cAdvisor版本问题针对第一点，我们升级了cAdivor镜像为最新版，问题依旧。 排查是否是cAdvisor参数配置问题google一些文章，有人提过类似的问题，官方issue的解释中，有人提到可能配置不对，可能采集的指标过多等，于是，我review了一下我的配置，调整后的完整配置如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697apiVersion: extensions/v1beta1kind: DaemonSetmetadata: labels: name: cadvisor name: cadvisor namespace: monitoringspec: revisionHistoryLimit: 10 selector: matchLabels: name: cadvisor template: metadata: annotations: prometheus.io/port: &quot;28762&quot; prometheus.io/scrape: &quot;true&quot; creationTimestamp: null labels: name: cadvisor spec: automountServiceAccountToken: false containers: - args: - -allow_dynamic_housekeeping=true - -global_housekeeping_interval=1m0s - -housekeeping_interval=3s - -disable_metrics=udp,tcp,percpu,sched - -storage_duration=15s - -profiling=true - -port=28762 - -max_procs=1 image: mine/cadvisor-test:v0.0.2 imagePullPolicy: IfNotPresent name: cadvisor ports: - containerPort: 28762 hostPort: 28762 name: http protocol: TCP resources: limits: cpu: &quot;1&quot; memory: 3000Mi requests: cpu: &quot;1&quot; memory: 500Mi terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /rootfs name: rootfs readOnly: true - mountPath: /var/run name: var-run readOnly: true - mountPath: /sys name: sys readOnly: true - mountPath: /var/lib/docker name: docker readOnly: true - mountPath: /dev/disk name: disk readOnly: true dnsPolicy: ClusterFirst hostNetwork: true restartPolicy: Always schedulerName: default-scheduler securityContext: &#123;&#125; terminationGracePeriodSeconds: 30 volumes: - hostPath: path: / type: &quot;&quot; name: rootfs - hostPath: path: /var/run type: &quot;&quot; name: var-run - hostPath: path: /sys type: &quot;&quot; name: sys - hostPath: path: /DATA/docker type: &quot;&quot; name: docker - hostPath: path: /dev/disk type: &quot;&quot; name: disk templateGeneration: 6 updateStrategy: rollingUpdate: maxUnavailable: 1 type: RollingUpdate 我调整的部分主要集中在： 12345678// 这个是禁用哪些指标，默认只有udp、tcp- -disable_metrics=udp,tcp,percpu,sched// 存储最近多久的数据，原来是1分多钟，调整为15s- -storage_duration=15s// 是否开启性能测试，默认为关闭，之所以开启，是要一会儿debug内存占用- -profiling=true// 使用多少CPU，默认不到1个- -max_procs=1 上面的方式，是减少了一些采集指标，以及采集数据的最多保留时长，稍微有些效果，但是发现效果不大，原来某些机器上频繁Crash的cAdvisor Pod，还是Crash，另外某些机器上从来不Crash的，也不会Crash。那么，说明参数配置没什么用，问题应该出现某些机器上。 排查为何cAdivosr Pod在某些机器上Crash我回顾了一下我们的K8S节点，发现cAdvisor Pod不OOM的机器上面，容器都比较少。越是容器多的机器，这机器上的cAdvisor Pod就越容易OOM Crash。 那么，我们看一下 cAdvisor 的 Pod 日志，发现其频繁报一个错误： 1fsHandler.go:135] du and find on following dirs took 57.562700809s: [/rootfs/DATA/docker/overlay2/d8c002c4dc33c22129124e70bf7ca15fd312cd8867c040708d11d7d462ee58df/diff /rootfs/DATA/docker/containers/16eb9120ce2da24d867ee287c093ce7221f1d3ed39e69c3a8d128313a5dc0d63]; will not log again for this container unless duration exceeds 4s 这说明，cAdvisor会统计每一个容器占用的磁盘使用大小，这个大小是通过du命令来处理的，而且，这个统计耗费的时间很长。我们可以实际去看一下，发现这个目录，确实比较大，有些在2-3G。这说明，这个机器上，必然存在一些容器，里边在搞事情，写了很多的文件，导致 du 命令在统计的时候，比较耗时。 问题初步总结K8S节点，有些容器存储或写入了比较多的文件，造成cAdvisor统计容器磁盘使用耗时，进而引发此cAdivosr内存占用升高。 排查深入探究既然上面已经初步定为问题，但是我们依然会疑惑，为什么cAdivosr统计容器磁盘耗时会引发内存飙升呢？ 我们需要借助一些工具来进一步排查 通过 go tool pprof 分析内存 通过查看 cAdvisor 源码分析流程 在源码中，打断点，验证猜想 通过 go tool pprof 分析内存首先，将 DaemonSet 启动的 cAdvisor，使用 Host 模式启动，这样我们就可以直接通过访问宿主机上，cAdvisor开放的端口，来做性能采样了。 1go tool pprof -cum -svg -alloc_space http://x.x.x.x:28762/debug/pprof/heap 上面的步骤，会生成内存性能采样图，类似如下： 详细采样图，可以通过此连接查看： 采样图全 从图中，先看红色部分，颜色越深，表示这部分资源消耗越严重，我们这个采样图是采集的内存，可以看到，有 2366.70M，是 Gather 函数的，但其实，这个函数本身，并没有多少内存消耗，它的内存占用这么大，是 collectContainersInfo 函数分配的。其实不论怎样，Gather函数都脱离不了干系。那么，我们从源码看一下 源码分析首先，入口函数main中，注册了/metrics对应的handler，因为cAdvisor要开发 /metirics路径，让 Prometheus 来定时采集 1234567891011// cadvisor.go#82func main() &#123; defer glog.Flush() flag.Parse() //注册HTTP路径 *prometheusEndpoint 值就是 /metirics cadvisorhttp.RegisterPrometheusHandler(mux, containerManager, *prometheusEndpoint, containerLabelFunc, includedMetrics) glog.V(1).Infof(&quot;Starting cAdvisor version: %s-%s on port %d&quot;, version.Info[&quot;version&quot;], version.Info[&quot;revision&quot;], *argPort) addr := fmt.Sprintf(&quot;%s:%d&quot;, *argIp, *argPort) glog.Fatal(http.ListenAndServe(addr, mux))&#125; 然后，看一下，是谁在处理 /metrics 路由对应的操作 123456789101112// 代码文件：http/handler.go#97func RegisterPrometheusHandler(mux httpmux.Mux, containerManager manager.Manager, prometheusEndpoint string, f metrics.ContainerLabelsFunc, includedMetrics container.MetricSet) &#123; r := prometheus.NewRegistry() r.MustRegister( metrics.NewPrometheusCollector(containerManager, f, includedMetrics), prometheus.NewGoCollector(), prometheus.NewProcessCollector(os.Getpid(), &quot;&quot;), ) //可以看到，真正执行 /metrics 的函数，是 promhttp.HandlerFor mux.Handle(prometheusEndpoint, promhttp.HandlerFor(r, promhttp.HandlerOpts&#123;ErrorHandling: promhttp.ContinueOnError&#125;))&#125; 可以看到，真正执行 /metrics 的函数，是promhttp.HandlerFor，具体深入HandlerFor看一下 12345678// 代码文件：vendor/github.com/prometheus/client_golang/prometheus/promhttp/http.go#82func HandlerFor(reg prometheus.Gatherer, opts HandlerOpts) http.Handler &#123; return http.HandlerFunc(func(w http.ResponseWriter, req *http.Request) &#123; //这里就是真正的 Gather 调用 mfs, err := reg.Gather() ... &#125;)&#125; 至此，可以说明，每一次HTTP调用（调用 x.x.x.x:8080/metrics），都会又一次Gather调用。 所以我们猜想，之所以Gather函数有这么大的内存占用，主要是因为Gather函数调用次数多，而每次Gather函数执行之间长，导致形成了并发调用，这种情况下，Gather函数从执行到结束期间，都不会释放内存，并发调用，就会导致内存积压。 修改源码，重新构建部署，验证猜想那么，我们在Gather调用处，打断点，看一下执行时间： 123456789101112// 代码文件：vendor/github.com/prometheus/client_golang/prometheus/promhttp/http.go#82func HandlerFor(reg prometheus.Gatherer, opts HandlerOpts) http.Handler &#123; return http.HandlerFunc(func(w http.ResponseWriter, req *http.Request) &#123; pp.Println(&quot;请求开始————————&quot;) start:=time.Now() //这里就是真正的 Gather 调用 mfs, err := reg.Gather() ... timeCost := time.Since(start) pp.Println(fmt.Sprintf(&quot;请求结束，耗时 %v&quot;, timeCost)) &#125;)&#125; 我们打印了Gather执行的耗时，然后重新构建 cAdvisor源码，打一个私有镜像出来，推送到私有镜像仓库。然后我们使用这个测试镜像，重新部署cAdvisor。 现在，我们挑一台之前cAdvisor频发OOM Crash的机器，看一下它的log 1kubectl logs -n monitoring cadvisor-k9kpt -f 日志输出大致如下： 123456&quot;请求开始————————&quot;I1023 14:21:19.126794 1 fsHandler.go:135] du and find on following dirs took 15.420205027s: [/rootfs/var/lib/docker/overlay2/67ec1868b2c0ed5ce5b22ee014eb4d08993accd68546a3de6aa2a6355bdc1a78/diff /rootfs/var/lib/docker/containers/cd910753386b3325af8bd5a69fc01b261ca14c1bfaf754677662e903b755d34f]; will not log again for this container unless duration exceeds 56sI1023 14:21:19.305938 1 fsHandler.go:135] du and find on following dirs took 15.278733582s: [/rootfs/var/lib/docker/overlay2/10621b60f26962cb1a90d7a7dc1ce4e3c8a15f6e4e30861b8433c5c37727bb9e/diff /rootfs/var/lib/docker/containers/b2a4d11c37aa9c63b4759c5728956253fad46fa174c7fe4d91336a4ac7532127]; will not log again for this container unless duration exceeds 1m34sI1023 14:21:19.827757 1 fsHandler.go:135] du and find on following dirs took 13.897447077s: [/rootfs/var/lib/docker/overlay2/29b3b0dfc22053937e9c40e004a6d31af489573ff3a385020feb22d88d1a3d0a/diff /rootfs/var/lib/docker/containers/af962971a0643418d28c03b374e31a0c58dd6302524ea06dc8a23c4eccf5d663]; will not log again for this container unless duration exceeds 1m20sI1023 14:21:20.042949 1 fsHandler.go:135] du and find on following dirs took 14.514122984s: [/rootfs/var/lib/docker/overlay2/27f1d3cb3d421567754cb7abb986c16c3f3bec0874e983a2604aa7eda8834d9a/diff /rootfs/var/lib/docker/containers/60cad8688e31b557e2e98c47beaa1f3af2ea2e6cbfab0c1f399887b3eecec86c]; will not log again for this container unless duration exceeds 1m56s&quot;请求结束，耗时 58.093771464s&quot; 日志其实我只是截图了一部分，基本上可以看出来，Gather请求十分耗时，这个耗时，就是由 du 操作耗时造成的，有时候，du 耗时非常严重，能将近2分钟。 这样，基本上，就印证了，Gather函数处理慢，而Prometheus每隔3s请求一次，造成同时有非常多的 Gather函数在并发处理，也就导致了内存积压的情况。 彻底解决综上，其实我们只需要让 du 磁盘统计快了就可以了，du 的快慢，是一个CPU密集和磁盘IO密集的操作，要加快 du 操作，就需要给到足够的运算能力。 回顾之前我们的 cAdvisor 的 DaemonSet 的yaml配置，我们在资源的 limit 部分，仅给到了一个 CPU，我们加 CPU 核数增加到6。如下： 1234567resources: limits: cpu: &quot;6&quot; memory: 3000Mi requests: cpu: &quot;2&quot; memory: 500Mi 然后，更新 DaemonSet 部署 1kubectl apply -f cadvisor.ds.yaml 再次去观察 cAdvisor 的pod日志，发现du耗时明显缩短到2秒钟以内，pod内存占用过高的情况，再也没有出现过。问题得解！]]></content>
      <categories>
        <category>kubernetes</category>
        <category>cAdvisor</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>kubernetes</tag>
        <tag>cAdvisor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何解除github repo的 froked from xxx/yyy 提示]]></title>
    <url>%2F2018%2F10%2F16%2F4%2F</url>
    <content type="text"><![CDATA[场景在某些情况下，我们有些需要克隆某个人的repo。 可能需要然后进行修改，提Pull Request进行代码合并，或者克隆repo后作为我们自己的项目做二次定制。 然而，一旦从其他人的repo上克隆为我们自己的repo，则项目名称下会一直提示 forked from xxx/yyy。有时候，出于一些原因，我们需要彻底解除我们的repo与原来repo之间的绑定关系。 我自己的场景，是博客repo从其他人的repo做了克隆，后期我完全重构了这个博客的主题选择等内容，基本和原来的那个远程repo完全不相干了，这时候，博客这个repo总是看到 forked from xxx/yyy 感觉有些扎眼。 解除方式方式1最笨拙的解除方式，就是将我们的远程repo删除，重新创建repo提交我们的代码文件。这种方式操作暴力，而且有些麻烦。我们之所以说它麻烦，是因为有时候，我们已经对这个repo做了一些自己的设置了，如果把repo删除重建并提交代码，还得再重新给这个repo做设置，比较繁琐。 方式2除了上面的方式，我们还可以把解除操作的步骤，交给github官方来做。具体来说，就是提交一个“工单”。地址：https://github.com/contact进入地址后，向下拉动，找到工单填写的地方。其中：Name：填写你的用户名Email：选择你的邮箱 Subject：这是工单标题，可以这样写：detach my repo aaa/bbb from the repo xxx/yyy How can we help?这部分需要你填写详细的求助说明，可以这样写：Please detach my repo aaa/bbb from the repo xxx/yyy, Thanks for helping! 填写完之后，直接点击 send request 按钮提交，等待github官方处理即可，一般情况下，半天时间内官方就会给予处理。]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s线上集群的机器，重启iptables后，造成CI系统（drone）构建偶发失败，docker run容器无法访问外网问题排查记录]]></title>
    <url>%2F2018%2F10%2F16%2F3%2F</url>
    <content type="text"><![CDATA[引子线上的k8s集群内部，每一台机器，都有2块网卡，一块儿em1（10网段），一个 em2（192网段）。然后，线上的机器，通过iptables限制为：访问192段的服务任意端口都是畅通的，而访问10段的服务端口，只有333端口可以连通。333端口，是线上机器的ssh端口。 线上集群的k8s，使用的calico网络。 线上有一个问题，就是容器访问当前宿主机192段IP的999端口不通，访问其他机器192段IP的9999端口畅通。 我们知道，当从容器访问宿主机的应用时，会重新进入iptables的INPUT链，而线上的iptables INPUT链如下： 1234567891011Chain INPUT (policy ACCEPT 0 packets, 0 bytes)num pkts bytes target prot opt in out source destination 1 1477K 2519M cali-INPUT all -- * * 0.0.0.0/0 0.0.0.0/0 /* cali:Cz_u1IQiXIMmKD4c */2 272K 20M KUBE-SERVICES all -- * * 0.0.0.0/0 0.0.0.0/0 /* kubernetes service portals */3 273K 20M KUBE-FIREWALL all -- * * 0.0.0.0/0 0.0.0.0/0 4 263K 20M ACCEPT all -- * * 0.0.0.0/0 0.0.0.0/0 state RELATED,ESTABLISHED5 0 0 ACCEPT icmp -- * * 0.0.0.0/0 0.0.0.0/0 6 393 17292 ACCEPT all -- lo * 0.0.0.0/0 0.0.0.0/0 7 11880 1019K ACCEPT all -- em2 * 0.0.0.0/0 0.0.0.0/0 8 7 308 ACCEPT tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:3339 228 62052 REJECT all -- * * 0.0.0.0/0 0.0.0.0/0 reject-with icmp-host-prohibited 意思是： ①：如果报文是途径em2网卡，访问任意其他网卡。则全部放行。（这就是为什么192段的访问全部放行） ②：如果报文是tcp，不管报文从哪个网卡出来，去往哪个网卡，只要目的端口是333，就放行。（这就是为什么10段的访问，只能访问333） 我们容器发出的报文，网卡是 cal 网卡（calico的网卡名称），所以，容器只能访问333。 错误操作要把9999端口放开，意味着，我要添加开放9999端口到INPUT链路中，为了让iptables再重启后依然生效，我将规则，添加到 1/etc/sysconfig/iptables 在添加完成后其实并未生效，然后，有2种方式，用于让iptables生效： ①：手动添加规则，比如 iptables -t filter -I INPUT 9 -s 172.20.0.0/14 -p tcp -m tcp –dport 9999 -j ACCEPT ②：重启iptables（会自动加载 /etc/sysconfig/iptables内容 ） 我选择了后者，之所以选择后者，其实是因为，即便重启了iptables，加载了刚刚的规则，原来的k8s和calico的iptables会丢失，但是，k8s会重建自己的整套iptables规则的。 事实上，k8s确实也在我重启iptables后，重建了iptables，但问题来了。 引发的2个网络问题1、我们线上的CI系统，是基于drone，做的二次开发，改动了很多的东西，添加了很多特性，结果上面一系列操作之后，我发现，我们的CI系统中，个别镜像的构建（Dockerfile中的RUN操作，有网络相关操作时，比如yum）会失败。 2、线上的机器，执行 docker run，起来的容器，无法访问外网了。然而，k8s起来的pod里的容器，却可以访问外网。 镜像的构建，其实本质上，也是起容器来做事情，而且，你无法定制docker build操作时，使用的网络模式，只能是bridge模式。而 docker run 起来的容器，默认也是 bridge 模式。结合上面2个内容，基本上可以确定，此问题，和k8s无关，而是纯粹的docker的问题了。 排查①：一开始以为是k8s的iptables重建不完整，所以，多次用线上的机器的机器 nodeA（无人访问的一台），不断尝试清空iptables，等待k8s重建iptables，多次尝试无效。 ②：stop docker -> stop kubelet -> clean iptables -> start docker -> start kubelet。这个操作的意义是，以为重启docker，重启k8s，docker 和 k8s 都会重建自己的 iptables。多次尝试无效。 ③：使用现在的k8s安装工具：kubespray，重新安装 nodeA，并添加到k8s集群内。寄希望于安装工具重装docker和k8s，结果依然无效。 ④：回归问题本身，问题一定出在iptables上。也是因为一开始重启了iptables导致此一系列问题。 解决过程既然问题出在iptables上，而我们要访问外网，那么，就得重iptables 报文转出入手。好在线上的机器，并没有都为了适配9999端口重启iptables，有3台 k8s 的 master 节点，并没有重启 iptables。 然后，对比有问题机器、没有问题机器的 FORWARD、POSTROUTING 的规则。发现，缺少DOCKER相关的规则，这些规则，本来是安装docker之后，由docker创建的。那么，解决此问题，就需要重建docker的iptables。 ①：找一台干净的机器（只安装了docker，未启动容器，为安装k8s），然后从中提取docker相关的规则 ②：将提取的规则，与出问题的机器的iptables默认规则融合。 ③：执行 iptables-restore 恢复规则。 后续验证在问题机器上，docker run centos 容器，然后ping qq.com，链路畅通。有一个小问题，就是如果你先启动的容器执行 ping 外网操作。然后才再目的机器上执行 iptables-restore 的话，会发现，容器的 ping，有一个短暂的耗时，然后才会畅通，这是因为 iptables-restore 恢复规则后，有一个过程。 最后我整理一个相对标准的docker iptables规则备份，如果你也遇到了启动的docker容器，无法访问外网的问题，可以通过下面的内容，恢复规则 将下面的规则，保存到一个文件，比如 docker-iptables-backup 123456789101112131415161718192021222324252627# Generated by iptables-save v1.4.21 on Thu Apr 30 20:48:42 2015*nat:PREROUTING ACCEPT [18:1080]:INPUT ACCEPT [18:1080]:OUTPUT ACCEPT [22:1550]:POSTROUTING ACCEPT [22:1550]:DOCKER - [0:0]-A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER-A OUTPUT ! -d 127.0.0.0/8 -m addrtype --dst-type LOCAL -j DOCKER-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE-A POSTROUTING -s 172.17.0.1/32 -d 172.17.0.1/32 -p tcp -m tcp --dport 80 -j MASQUERADE-A DOCKER ! -i docker0 -p tcp -m tcp --dport 3001 -j DNAT --to-destination 172.17.0.1:80COMMIT# Completed on Thu Apr 30 20:48:42 2015# Generated by iptables-save v1.4.21 on Thu Apr 30 20:48:42 2015*filter:INPUT ACCEPT [495:53218]:FORWARD ACCEPT [0:0]:OUTPUT ACCEPT [480:89217]:DOCKER - [0:0]-A FORWARD -o docker0 -j DOCKER-A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT-A FORWARD -i docker0 ! -o docker0 -j ACCEPT-A FORWARD -i docker0 -o docker0 -j ACCEPT-A DOCKER -d 172.17.0.1/32 ! -i docker0 -o docker0 -p tcp -m tcp --dport 80 -j ACCEPTCOMMIT# Completed on Thu Apr 30 20:48:42 2015 然后，执行恢复 iptables-restore docker-iptables-backup 注意： ①：如果你的docker在k8s集群内部，理论上来说，上面的操作会有清空iptables的能力，但是你不需要担心，因为k8s会自行修复本身的iptables，也就是说，k8s会重建自己的iptables。但是假如你的k8s没能完整重建，就需要手动恢复k8s的iptables。如果是calico网络，只需要删除当前节点的calico pod即可，删除之后，k8s会重启这个pod，自然也就会重建iptables了。同理，flannel网络也是一样的。 ②：上面的规则，有一个关键，是 172.17.0.0 和 172.17.0.1，这是docker默认安装后的网段和docker0网卡的IP。如果你是默认安装的docker，上面的规则不需要改动，但如果你定制了docker0网卡的IP，记得修改一下上面的规则，适配你的环境。]]></content>
      <categories>
        <category>docker</category>
        <category>drone</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>drone</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于XFS文件系统的overlayfs下使用docker，为何要使用d_type=1]]></title>
    <url>%2F2018%2F10%2F16%2F2%2F</url>
    <content type="text"><![CDATA[什么是overlayfs首先，overlayfs是一种文件系统，也是目前dokcer在使用的最新的文件系统，其他的文件系统还有：aufs、device mapper等。而 overlayfs 其实和 aufs 是类似的。更准确的说，overlayfs，其实是 Linux 文件系统的一种上层文件系统。下面的底层的文件系统格式，是支持overlayfs的： ①：ext4 ②：xfs（必须在格式化为xfs的是，指定ftype=1，如果在 未使用ftype=1的方式格式化的xfs文件系统上使用，否则docker可能出现未知问题） 如何查看当前操作是否支持overlay1lsmod |grep over 如果没有输出，表示不支持，可以通过下面的命令开启overlay 1modprobe overlay 需要注意的是： ①：docker官方，建议使用 overlay2，而不是 overlay，因为 overlay2 更高效。要使用 overlay2的话，需要 Linux 内核在版本4以上。 ②：docker 官方建议，在 docker-ee 17.06.02及以上的版本使用 overlay2，以及，在docker-ce的版本，也使用 overlay2。而 overlay 虽然在 docker-ce 版本中是支持的，但是并不推荐。 ③：只要当前操作系统支持overlay，那docker就可以使用overlay或者overlay2了。 ④：指定docker的overlay2驱动，需要在启动docker的时候，指定 –storage-driver 参数，或者，在配置文件 /etc/docker/daemon.json 中 ，指定驱动配置 123456&#123; "storage-driver": "overlay2", "storage-opts": [ "overlay2.override_kernel_check=true" ]&#125; xfs文件系统的 d_type是什么d_type 是 Linux 内核的一个术语，表示 “目录条目类型”，而目录条目，其实是文件系统上目录信息的一个数据结构。d_type，就是这个数据结构的一个字段，这个字段用来表示文件的类型，是文件，还是管道，还是目录还是套接字等。 d_type 从 Linux 2.6 内核开始就已经支持了，只不过虽然 Linux 内核虽然支持，但有些文件系统实现了 d_type，而有些，没有实现，有些是选择性的实现，也就是需要用户自己用额外的参数来决定是否开启d_type的支持。 为什么docker在overlay2（xfs文件系统）需要d_type不论是 overlay，还是 overlay2，它们的底层文件系统都是 overlayfs 文件系统。而 overlayfs 文件系统，就会用到 d_type 这个东西用来文件的操作是被正确的处理了。换句话说，docker只要使用 overlay 或者 overlay2，就等于在用 overlayfs，也就一定会用到 d_type。所以，docker 提供了 1docker info 此命令，用来检测你docker服务，是否在使用overlay的时候正确的使用 d_type。如果用了 overlay/overlay2，但 d_type 没有开，就报警告。 如果在不支持 d_typ 的 overlay/overlay 驱动下使用docker，也就意味着 docker 在操作文件的时候，可能会遇到一些错误，比如 无法删除某些目录或文件，设置文件或目录的权限或用户失败等等。这些都是不可预料的错误。举个具体的场景，就是，docker构建的时候，可能在构建过程中，删除文件等操作失败，导致构建停止。 如何检测当前的文件系统，是否支持 d_type ？ 1xfs_info / 它用于检测指定挂载点的文件xfs文件系统的信息。如果你的文件系统是 xfs，则会提示类似如下信息 12345678910$ xfs_info /meta-data=/dev/sda1 isize=256 agcount=4, agsize=3276736 blks = sectsz=512 attr=2, projid32bit=1 = crc=0 finobt=0 spinodes=0data = bsize=4096 blocks=13106944, imaxpct=25 = sunit=0 swidth=0 blksnaming =version 2 bsize=4096 ascii-ci=0 ftype=1log =internal bsize=4096 blocks=6399, version=2 = sectsz=512 sunit=0 blks, lazy-count=1realtime =none extsz=4096 blocks=0, rtextents=0 注意其中的 ftype，1表示支持 d_type，0表示不支持。 参考： https://linuxer.pro/2017/03/fix-chown-error-discourse-bootstrap/ https://linuxer.pro/2017/03/what-is-d_type-and-why-docker-overlayfs-need-it/ https://blog.csdn.net/zxf_668899/article/details/54667521 https://docs.docker.com/storage/storagedriver/overlayfs-driver/]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>ovlerlayfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F10%2F16%2F1%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
